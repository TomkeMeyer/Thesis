{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1bcfa9-6360-4e7d-9a95-ddd188011c84",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import os\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6986b7f6-f470-48a2-b51e-a96708c58240",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 13:35:45.036955: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734266145.103446   71006 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734266145.123878   71006 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 13:35:45.189681: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import copy\n",
    "import torch\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as python_random\n",
    "import tensorflow as tf\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as Xet\n",
    "from argparse import ArgumentParser\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATSx\n",
    "from neuralforecast.losses.pytorch import MQLoss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nbeats_pytorch.model import NBeatsNet\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import statsmodels.api as sm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow.keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4fd5bd7-a819-4640-84b7-8408a49c8100",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Load data into desired formats for training/validation/testing, including preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, horizon, back_horizon):\n",
    "        self.horizon = horizon\n",
    "        self.back_horizon = back_horizon\n",
    "        self.scaler = list()\n",
    "        self.historical_values = list()  # first by patient idx, then by col_idx\n",
    "\n",
    "    def preprocessing(\n",
    "        self,\n",
    "        lst_train_arrays,\n",
    "        lst_test_arrays,\n",
    "        # train_mode=True, # flag for train_mode (split into train/val), test_mode (no split)\n",
    "        train_size=0.8,\n",
    "        normalize=False,\n",
    "        sequence_stride=6,\n",
    "        target_col=0,\n",
    "        horizon=12\n",
    "    ):\n",
    "        self.normalize = normalize\n",
    "        self.sequence_stride = sequence_stride\n",
    "        self.target_col = target_col\n",
    "        train_arrays = copy.deepcopy(lst_train_arrays)\n",
    "        test_arrays = copy.deepcopy(lst_test_arrays)\n",
    "        # count valid timesteps for each individual series\n",
    "        # train_array.shape = n_timesteps x n_features\n",
    "        self.valid_steps_train = [train_array.shape[0] for train_array in train_arrays]\n",
    "        train_lst, val_lst, test_lst = list(), list(), list()\n",
    "        for idx in range(len(train_arrays)):\n",
    "            print(idx, \"index\")\n",
    "            bg_sample_train = train_arrays[idx]\n",
    "            #bg_sample_train_exog = np.delete(train_arrays[idx], 0, 1)\n",
    "            bg_sample_test = test_arrays[idx]#[:, target_col]\n",
    "            #bg_sample_test_exog = np.delete(test_arrays[idx], 0, 1)\n",
    "            valid_steps_sample = self.valid_steps_train[idx]\n",
    "            #train_target = bg_sample_train_target[: int(train_size * valid_steps_sample)].copy()\n",
    "            train = bg_sample_train[: int(train_size * valid_steps_sample), :].copy()\n",
    "            #val_target = bg_sample_train_target[int(train_size * valid_steps_sample) :].copy()\n",
    "            val = bg_sample_train[int(train_size * valid_steps_sample) :, :].copy()\n",
    "            #test_target = bg_sample_test_target[:].copy()\n",
    "            test = bg_sample_test[:, :].copy()\n",
    "            if self.normalize:\n",
    "                scaler_cols = list()\n",
    "                # train.shape = n_train_timesteps x n_features\n",
    "                for col_idx in range(train.shape[1]):\n",
    "                    scaler = MinMaxScaler(feature_range=(0, 1), clip=False)\n",
    "                    train[:, col_idx] = remove_extra_dim(\n",
    "                        scaler.fit_transform((add_extra_dim(train[:, col_idx])))\n",
    "                    )\n",
    "                    val[:, col_idx] = remove_extra_dim(\n",
    "                        scaler.transform(add_extra_dim(val[:, col_idx]))\n",
    "                    )\n",
    "                    test[:, col_idx] = remove_extra_dim(\n",
    "                        scaler.transform(add_extra_dim(test[:, col_idx]))\n",
    "                    )\n",
    "                    scaler_cols.append(scaler)  # by col_idx, each feature\n",
    "                self.scaler.append(scaler_cols)  # by pat_idx, each patient\n",
    "                \n",
    "            lst_hist_values = list()\n",
    "            for col_idx in range(train.shape[1]):\n",
    "                all_train_col = np.concatenate((train[:, col_idx], val[:, col_idx]))\n",
    "                # decimals = 1, 2 OR 3?\n",
    "                unique_values = np.unique(np.round(all_train_col, decimals=2))\n",
    "                lst_hist_values.append(unique_values)\n",
    "            self.historical_values.append(lst_hist_values)\n",
    "\n",
    "            train_lst.append(train)\n",
    "            #train_lst_exog.append(train_exog)\n",
    "            val_lst.append(val)\n",
    "            #val_lst_exog.append(val_exog)\n",
    "            test_lst.append(test)\n",
    "            #test_lst_exog.append(test_exog)\n",
    "        \n",
    "\n",
    "        (\n",
    "            self.X_train_exog,\n",
    "            self.X_train_target,\n",
    "            self.Y_train,\n",
    "            self.train_idxs,\n",
    "        ) = self.create_sequences(\n",
    "            train_lst,\n",
    "            self.horizon,\n",
    "            self.back_horizon,\n",
    "            self.sequence_stride,\n",
    "            self.target_col,\n",
    "        )\n",
    "        (\n",
    "            self.X_val_exog,\n",
    "            self.X_val_target,\n",
    "            self.Y_val,\n",
    "            self.val_idxs,\n",
    "        ) = self.create_sequences(\n",
    "            val_lst,\n",
    "            self.horizon,\n",
    "            self.back_horizon,\n",
    "            self.sequence_stride,\n",
    "            self.target_col,\n",
    "        )\n",
    "        (\n",
    "            self.X_test_exog,\n",
    "            self.X_test_target,\n",
    "            self.Y_test,\n",
    "            self.test_idxs,\n",
    "        ) = self.create_sequences(\n",
    "            test_lst,\n",
    "            self.horizon,\n",
    "            self.back_horizon,\n",
    "            self.sequence_stride,\n",
    "            self.target_col,\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_sequences(\n",
    "        series_lst, horizon, back_horizon, sequence_stride, target_col=0, exog=False\n",
    "    ):\n",
    "        Xs_exog, Xs_target, Ys, sample_idxs = list(), list(), list(), list()\n",
    "        \n",
    "        cnt_nans = 0\n",
    "        for idx, series in enumerate(series_lst):\n",
    "            len_series = series.shape[0]\n",
    "            if len_series < (horizon + back_horizon):\n",
    "                print(\n",
    "                    f\"Warning: not enough timesteps to split for sample {idx}, len: {len_series}, horizon: {horizon}, back: {back_horizon}.\"\n",
    "                )\n",
    "            for i in range(0, len_series - back_horizon - horizon, sequence_stride):\n",
    "                input_series_exog = series[i : (i + back_horizon)]\n",
    "                input_series_exog = np.delete(input_series_exog, [target_col], axis=1)\n",
    "                input_series_target = series[i : (i + back_horizon), [target_col]]\n",
    "                output_series = series[\n",
    "                    (i + back_horizon) : (i + back_horizon + horizon), [target_col]\n",
    "                ]\n",
    "                # TODO: add future plans as additional variables (?)\n",
    "                if np.isfinite(input_series_exog).all() and np.isfinite(input_series_target).all() and np.isfinite(output_series).all():\n",
    "                    Xs_exog.append(input_series_exog)\n",
    "                    Xs_target.append(input_series_target)\n",
    "                    Ys.append(output_series)\n",
    "                    # record the sample index when splitting\n",
    "                    sample_idxs.append(idx)\n",
    "                else:\n",
    "                    cnt_nans += 1\n",
    "                    if cnt_nans % 100 == 0:\n",
    "                        print(f\"{cnt_nans} strides skipped due to NaN values.\")\n",
    "        #print(\"train\", np.array(Xs), \"test\", np.array(Ys), \"val\", np.array(sample_idxs))\n",
    "        return np.array(Xs_exog), np.array(Xs_target), np.array(Ys), np.array(sample_idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6fdba9-a79d-4357-8557-3223aee456bb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove an extra dimension\n",
    "def remove_extra_dim(input_array):\n",
    "    # 2d to 1d\n",
    "    if len(input_array.shape) == 2:\n",
    "        return np.reshape(input_array, (-1))\n",
    "    # 3d to 2d (remove the last empty dim)\n",
    "    elif len(input_array.shape) == 3:\n",
    "        return np.squeeze(np.asarray(input_array), axis=-1)\n",
    "    else:\n",
    "        print(\"Not implemented.\")\n",
    "        #print(input_array, \"JLNA;iknb\")\n",
    "\n",
    "# add an extra dimension\n",
    "def add_extra_dim(input_array):\n",
    "    # 1d to 2d\n",
    "    if len(input_array.shape) == 1:\n",
    "        return np.reshape(input_array, (-1, 1))\n",
    "    # 2d to 3d\n",
    "    elif len(input_array.shape) == 2:\n",
    "        return np.asarray(input_array)[:, :, np.newaxis]\n",
    "    else:\n",
    "        print(\"Not implemented.\")\n",
    "        #print(input_array, \"ALVNAPNV\")\n",
    "\n",
    "# Method: Fix the random seeds to get consistent models\n",
    "def reset_seeds(seed_value=39):\n",
    "    # ref: https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n",
    "    # necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "    np.random.seed(seed_value)\n",
    "    # necessary for starting core Python generated random numbers in a well-defined state.\n",
    "    python_random.seed(seed_value)\n",
    "    # set_seed() will make random number generation\n",
    "    tf.random.set_seed(seed_value)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83c0945e-74f5-410f-9b29-acfe552ffba4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(dataset, data_path):\n",
    "    df = []\n",
    "    df = pd.DataFrame(df)\n",
    "    if dataset == \"simulated\":\n",
    "        for i,j in zip([\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\"],[1,2,3,4,5,6,7,8,9,10]):\n",
    "            a = pd.read_csv(f\"../results/simulation_4/adult#0{i}.csv\")\n",
    "            a[\"Time\"] = a[[\"Time\"]].apply(\n",
    "                lambda x: pd.to_datetime(x, errors=\"coerce\", format=\"%Y-%m-%d %H:%M:%S\")\n",
    "            )\n",
    "            #a['Time'] = pd.to_datetime(a['Time'])\n",
    "            #a.rename(columns={\"Time\":\"ds\", \"BG\":\"y\"}, inplace=True)\n",
    "            a = a.dropna()\n",
    "            #date_index = pd.date_range(a.Time[0], periods=len(a),freq='3min')\n",
    "            #a.index = date_index\n",
    "            a['patient_id'] = pd.Series([f\"{j}\" for x in range(len(a.index))])\n",
    "            df = pd.concat([df,a], ignore_index=True)\n",
    "        \n",
    "        #df.drop(['Time','BG','LBGI','HBGI','Risk'], axis=1, inplace=True)\n",
    "        print(\"aldingvapnb[\", df)\n",
    "        idx = int( df.shape[0] * 0.8)#TEST_SIZE)\n",
    "        cut = int((df.shape[0]-idx)/10)\n",
    "        Y_train_df = df[df.CGM<df['CGM'].values[-cut]] # 132 train\n",
    "        Y_test_df = df[df.CGM>=df['CGM'].values[-cut]].reset_index(drop=True) # 12 test  \n",
    "        Y_train_df.to_csv(\"data/data_simulation/all_train.csv\")\n",
    "        Y_test_df.to_csv(\"data/data_simulation/all_test.csv\")\n",
    "        df.drop(['Time','BG','LBGI','HBGI','Risk'], axis=1, inplace=True)\n",
    "        #return df\n",
    "        \n",
    "    elif dataset == \"ohiot1dm\":\n",
    "        train = []\n",
    "        test = []\n",
    "        train = pd.DataFrame(train)\n",
    "        test = pd.DataFrame(test)\n",
    "        for i in [540, 544, 552, 567, 584, 596, 559, 563, 570, 575, 588, 591]:\n",
    "            file_train = pd.read_csv(data_path + \"data_OhioT1DM/\" + f\"{i}_train.csv\")\n",
    "            file_test = pd.read_csv(data_path + \"data_OhioT1DM/\" + f\"{i}_test.csv\")\n",
    "            \n",
    "            file_train['patient_id'] = pd.Series([f\"{i}\" for x in range(len(file_train.index))])\n",
    "            file_test['patient_id'] = pd.Series([f\"{i}\" for x in range(len(file_test.index))])\n",
    "            \n",
    "            train = pd.concat([train, file_train], ignore_index=True)\n",
    "            test = pd.concat([train, file_test], ignore_index=True)\n",
    "            \n",
    "        train.to_csv(data_path + \"data_OhioT1DM/all_train.csv\")\n",
    "        test.to_csv(data_path + \"data_OhioT1DM/all_test.csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def1e674-4b21-49dc-b23f-00d8621b8753",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_data(dataset, data_path):\n",
    "    prepare_data(dataset, data_path)\n",
    "    if dataset == \"ohiot1dm\":\n",
    "        train, orig_train = load_ohio_data(data_path, \"all_train.csv\")\n",
    "        test, orig_test = load_ohio_data(data_path, \"all_test.csv\")\n",
    "    elif dataset == \"simulated\":\n",
    "        #idx = int( df.shape[0] * 1-TEST_SIZE )\n",
    "        #cut = int((df.shape[0]-idx)/10)\n",
    "        #train = df[df.CGM<df['CGM'].values[-cut]] # 132 train\n",
    "        #test = df[df.CGM>=df['CGM'].values[-cut]].reset_index(drop=True) # 12 test  \n",
    "        train, orig_train = load_sim_data(data_path, \"all_train.csv\")\n",
    "        test, orig_test = load_sim_data(data_path, \"all_test.csv\")\n",
    "    else:\n",
    "        print(\"No dataset chosen\")\n",
    "    return train, test, orig_train, orig_test\n",
    "\n",
    "def load_ohio_data(data_path, file_name=\"all_train.csv\"):\n",
    "    # load all the patients, combined\n",
    "    data = pd.read_csv(data_path + \"data_OhioT1DM/\" + file_name)\n",
    "\n",
    "    from functools import reduce\n",
    "    from operator import or_ as union\n",
    "\n",
    "    def idx_union(mylist):\n",
    "        idx = reduce(union, (index for index in mylist))\n",
    "        return idx\n",
    "\n",
    "    idx_missing = data.loc[data[\"missing\"] != -1].index\n",
    "    idx_missing_union = idx_union([idx_missing - 1, idx_missing])\n",
    "\n",
    "    data = data.drop(idx_missing_union)\n",
    "    data_bg = data[\n",
    "        [\n",
    "            \"index_new\",\n",
    "            \"patient_id\",\n",
    "            \"glucose\",\n",
    "            \"basal\",\n",
    "            \"bolus\",\n",
    "            \"carbs\",\n",
    "            \"exercise_intensity\",\n",
    "        ]\n",
    "    ]\n",
    "    data_bg[\"time\"] = data_bg[[\"index_new\"]].apply(\n",
    "        lambda x: pd.to_datetime(x, errors=\"coerce\", format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    )\n",
    "    data_bg = data_bg.drop(\"index_new\", axis=1)\n",
    "\n",
    "    data_bg[\"bolus\"][data_bg[\"bolus\"] == -1] = 0\n",
    "    data_bg[\"carbs\"][data_bg[\"carbs\"] == -1] = 0\n",
    "    data_bg[\"exercise_intensity\"][data_bg[\"exercise_intensity\"] == -1] = 0\n",
    "    data_bg[\"glucose\"][data_bg[\"glucose\"] == -1] = np.NaN\n",
    "\n",
    "    lst_patient_id = [\n",
    "        540,\n",
    "        544,\n",
    "        552,\n",
    "        567,\n",
    "        584,\n",
    "        596,\n",
    "        559,\n",
    "        563,\n",
    "        570,\n",
    "        575,\n",
    "        588,\n",
    "        591,\n",
    "    ]\n",
    "    lst_arrays = list()\n",
    "    for pat_id in lst_patient_id:\n",
    "        lst_arrays.append(\n",
    "            np.asarray(\n",
    "                data_bg[data_bg[\"patient_id\"] == pat_id][\n",
    "                    [\n",
    "                        \"glucose\",\n",
    "                        \"basal\",\n",
    "                        \"bolus\",\n",
    "                        \"carbs\",\n",
    "                        \"exercise_intensity\",\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    return lst_arrays, data_bg\n",
    "\n",
    "\n",
    "def load_sim_data(data_path, file_name=\"all_train.csv\"):\n",
    "    data = pd.read_csv(data_path + \"data_simulation/\" + file_name)\n",
    "    data_bg = data[[\"patient_id\", \"Time\", \"CGM\", \"CHO\", \"insulin\"]]\n",
    "    #print(data_bg)\n",
    "    data_bg[\"time\"] = data_bg[[\"Time\"]].apply(\n",
    "        lambda x: pd.to_datetime(x, errors=\"coerce\", format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    )\n",
    "    data_bg = data_bg.drop(\"Time\", axis=1)\n",
    "    lst_patient_id = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    lst_arrays = list()\n",
    "    for pat_id in lst_patient_id:\n",
    "        lst_arrays.append(\n",
    "            np.asarray(\n",
    "                data_bg[data_bg[\"patient_id\"] == pat_id][[\"CGM\", \"CHO\", \"insulin\"]]\n",
    "            )\n",
    "        )\n",
    "    return lst_arrays, data_bg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1b1a137-f7a4-4042-b85c-0235d9ec6570",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def forecast_metrics(dataset, Y_pred, inverse_transform=True):\n",
    "    Y_test_original, Y_pred_original = list(), list()\n",
    "    #Y_pred = np.squeeze(Y_pred, axis=-1)\n",
    "    if inverse_transform:\n",
    "        for i in range(dataset.X_test_exog.shape[0]):\n",
    "            #print(\"Y_test\", dataset.Y_test[i], \"Y_pred\", Y_pred[i])\n",
    "            idx = dataset.test_idxs[i]\n",
    "            scaler = dataset.scaler[idx]\n",
    "\n",
    "            Y_test_original.append(\n",
    "                scaler[dataset.target_col].inverse_transform(dataset.X_test_target[i])\n",
    "            )\n",
    "            Y_pred_original.append(\n",
    "                scaler[dataset.target_col].inverse_transform(Y_pred[i])\n",
    "            )\n",
    "\n",
    "        Y_test_original = np.array(Y_test_original)\n",
    "        Y_pred_original = np.array(Y_pred_original)\n",
    "    else:\n",
    "        Y_test_original = dataset.X_test_target\n",
    "        Y_pred_original = Y_pred\n",
    "\n",
    "    def smape(Y_test, Y_pred):\n",
    "        # src: https://github.com/ServiceNow/N-BEATS/blob/c746a4f13ffc957487e0c3279b182c3030836053/common/metrics.py\n",
    "        def smape_sample(actual, forecast):\n",
    "            return 200 * np.mean(\n",
    "                np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast))\n",
    "            )\n",
    "\n",
    "        return np.mean([smape_sample(Y_test[i], Y_pred[i]) for i in range(len(Y_pred))])\n",
    "\n",
    "    def rmse(Y_test, Y_pred):\n",
    "        return np.sqrt(np.mean((Y_pred - Y_test) ** 2))\n",
    "    print(\"test\", Y_test_original, \"pred\", Y_pred_original)\n",
    "    mean_smape = smape(Y_test_original, Y_pred_original)\n",
    "    mean_rmse = rmse(Y_test_original, Y_pred_original)\n",
    "\n",
    "    return mean_smape, mean_rmse\n",
    "\n",
    "def forecast_metrics_single(Y_orig, Y_pred, inverse_transform=True):\n",
    "    Y_test_original, Y_pred_original = list(), list()\n",
    "    #Y_pred = np.squeeze(Y_pred, axis=-1)\n",
    "    if inverse_transform:\n",
    "        #for i in range(dataset.X_test_exog.shape[0]):\n",
    "            #print(\"Y_test\", dataset.Y_test[i], \"Y_pred\", Y_pred[i])\n",
    "        #    idx = dataset.test_idxs[i]\n",
    "        scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "        \n",
    "        Y_test_original = scaler.inverse_transform(Y_orig)  # Ensure correct shape\n",
    "        Y_pred_original = scaler.inverse_transform(Y_pred.numpy().reshape(-1, 1))  # Ensure correct shape\n",
    "\n",
    "\n",
    "        Y_test_original = np.array(Y_test_original)\n",
    "        Y_pred_original = np.array(Y_pred_original)\n",
    "    else:\n",
    "        Y_test_original = dataset.X_test_target\n",
    "        Y_pred_original = Y_pred\n",
    "\n",
    "    def smape(Y_test, Y_pred):\n",
    "        # src: https://github.com/ServiceNow/N-BEATS/blob/c746a4f13ffc957487e0c3279b182c3030836053/common/metrics.py\n",
    "        def smape_sample(actual, forecast):\n",
    "            return 200 * np.mean(\n",
    "                np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast))\n",
    "            )\n",
    "\n",
    "        return np.mean([smape_sample(Y_test[i], Y_pred[i]) for i in range(len(Y_pred))])\n",
    "\n",
    "    def rmse(Y_test, Y_pred):\n",
    "        return np.sqrt(np.mean((Y_pred - Y_test) ** 2))\n",
    "    print(\"test\", Y_test_original, \"pred\", Y_pred_original)\n",
    "    mean_smape = smape(Y_test_original, Y_pred_original)\n",
    "    mean_rmse = rmse(Y_test_original, Y_pred_original)\n",
    "\n",
    "    return mean_smape, mean_rmse\n",
    "\n",
    "\n",
    "def polynomial_values(shift, change_percent, poly_order, horizon, desired_steps=None):\n",
    "    \"\"\"\n",
    "    shift: e.g., +0.1 (110% of the start value)\n",
    "    change_percent: e.g., 0.1 (10% increase)\n",
    "    poly_order: e.g., order 1, or 2, ...\n",
    "    horizon: the forecasting horizon\n",
    "    desired_steps: the desired timesteps for the change_percent to finally happen (can be larger than horizon)\n",
    "    \"\"\"\n",
    "    if horizon == 1:\n",
    "        return np.asarray([shift + change_percent])\n",
    "    desired_steps = desired_steps if desired_steps else horizon\n",
    "\n",
    "    p_orders = [shift]  # intercept\n",
    "    p_orders.extend([0 for i in range(poly_order)])\n",
    "    p_orders[-1] = change_percent / ((desired_steps - 1) ** poly_order)\n",
    "\n",
    "    p = np.polynomial.Polynomial(p_orders)\n",
    "    p_coefs = list(reversed(p.coef))\n",
    "    value_lst = np.asarray([np.polyval(p_coefs, i) for i in range(desired_steps)])\n",
    "\n",
    "    return value_lst[:horizon]\n",
    "\n",
    "\n",
    "def generate_bounds(\n",
    "    center,\n",
    "    shift,\n",
    "    desired_center,\n",
    "    poly_order,\n",
    "    horizon,\n",
    "    fraction_std,\n",
    "    input_series,\n",
    "    desired_steps,\n",
    "):\n",
    "    if center == \"last\":\n",
    "        start_value = input_series[-1]\n",
    "    elif center == \"median\":\n",
    "        start_value = np.median(input_series)\n",
    "    elif center == \"mean\":\n",
    "        start_value = np.mean(input_series)\n",
    "    elif center == \"min\":\n",
    "        start_value = np.min(input_series)\n",
    "    elif center == \"max\":\n",
    "        start_value = np.max(input_series)\n",
    "    else:\n",
    "        print(\"Center: not implemented.\")\n",
    "\n",
    "    std = np.std(input_series)\n",
    "    # Calculate the change_percent based on the desired center (in 2 hours)\n",
    "    change_percent = (desired_center - start_value) / start_value\n",
    "    # Create a default fluctuating range for the upper and lower bound if std is too small\n",
    "    fluct_range = fraction_std * std if fraction_std * std >= 0.025 else 0.025\n",
    "    upper = add_extra_dim(\n",
    "        start_value\n",
    "        * (\n",
    "            1\n",
    "            + polynomial_values(\n",
    "                shift, change_percent, poly_order, horizon, desired_steps\n",
    "            )\n",
    "            + fluct_range\n",
    "        )\n",
    "    )\n",
    "    lower = add_extra_dim(\n",
    "        start_value\n",
    "        * (\n",
    "            1\n",
    "            + polynomial_values(\n",
    "                shift, change_percent, poly_order, horizon, desired_steps\n",
    "            )\n",
    "            - fluct_range\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return upper, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a96c350c-50ef-452b-abee-19cde6412da4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71006/3259339262.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"time\"] = data_bg[[\"index_new\"]].apply(\n",
      "/tmp/ipykernel_71006/3259339262.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"bolus\"][data_bg[\"bolus\"] == -1] = 0\n",
      "/tmp/ipykernel_71006/3259339262.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"carbs\"][data_bg[\"carbs\"] == -1] = 0\n",
      "/tmp/ipykernel_71006/3259339262.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"exercise_intensity\"][data_bg[\"exercise_intensity\"] == -1] = 0\n",
      "/tmp/ipykernel_71006/3259339262.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"glucose\"][data_bg[\"glucose\"] == -1] = np.NaN\n",
      "/tmp/ipykernel_71006/3259339262.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"time\"] = data_bg[[\"index_new\"]].apply(\n",
      "/tmp/ipykernel_71006/3259339262.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"bolus\"][data_bg[\"bolus\"] == -1] = 0\n",
      "/tmp/ipykernel_71006/3259339262.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"carbs\"][data_bg[\"carbs\"] == -1] = 0\n",
      "/tmp/ipykernel_71006/3259339262.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"exercise_intensity\"][data_bg[\"exercise_intensity\"] == -1] = 0\n",
      "/tmp/ipykernel_71006/3259339262.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"glucose\"][data_bg[\"glucose\"] == -1] = np.NaN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of loaded train: 12*(12236, 5)\n",
      "The shape of test: 12*(12236, 5)\n",
      "===========Desired trend parameters=============\n",
      "center: last, desired_shift: 0;\n",
      "fraction_std:1;\n",
      "desired_change:'sample_based', poly_order:1.\n",
      "0 index\n",
      "1 index\n",
      "2 index\n",
      "3 index\n",
      "4 index\n",
      "5 index\n",
      "6 index\n",
      "7 index\n",
      "8 index\n",
      "9 index\n",
      "10 index\n",
      "11 index\n",
      "100 strides skipped due to NaN values.\n",
      "200 strides skipped due to NaN values.\n",
      "300 strides skipped due to NaN values.\n",
      "400 strides skipped due to NaN values.\n",
      "500 strides skipped due to NaN values.\n",
      "600 strides skipped due to NaN values.\n",
      "700 strides skipped due to NaN values.\n",
      "800 strides skipped due to NaN values.\n",
      "900 strides skipped due to NaN values.\n",
      "1000 strides skipped due to NaN values.\n",
      "100 strides skipped due to NaN values.\n",
      "200 strides skipped due to NaN values.\n",
      "100 strides skipped due to NaN values.\n",
      "200 strides skipped due to NaN values.\n",
      "300 strides skipped due to NaN values.\n",
      "400 strides skipped due to NaN values.\n",
      "500 strides skipped due to NaN values.\n",
      "600 strides skipped due to NaN values.\n",
      "700 strides skipped due to NaN values.\n",
      "800 strides skipped due to NaN values.\n",
      "900 strides skipped due to NaN values.\n",
      "1000 strides skipped due to NaN values.\n",
      "1100 strides skipped due to NaN values.\n",
      "1200 strides skipped due to NaN values.\n",
      "1300 strides skipped due to NaN values.\n",
      "[[[0.45238095 0.         0.         0.        ]\n",
      "  [0.45238095 0.         0.         0.        ]\n",
      "  [0.45238095 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.45238095 0.         0.         0.        ]\n",
      "  [0.45238095 0.         0.         0.        ]\n",
      "  [0.45238095 0.         0.         0.        ]]\n",
      "\n",
      " [[0.45238095 0.         0.         0.        ]\n",
      "  [0.45238095 0.         0.         0.        ]\n",
      "  [0.19047619 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.19047619 0.         0.         0.        ]\n",
      "  [0.19047619 0.         0.         0.        ]\n",
      "  [0.19047619 0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.19047619 0.         0.         0.        ]\n",
      "  [0.19047619 0.         0.         0.        ]\n",
      "  [0.19047619 0.         0.         0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.784      0.         0.         0.        ]\n",
      "  [0.784      0.         0.         0.        ]\n",
      "  [0.784      0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.784      0.         0.         0.        ]\n",
      "  [0.784      0.         0.         0.        ]\n",
      "  [0.784      0.         0.         0.        ]]\n",
      "\n",
      " [[0.784      0.         0.         0.        ]\n",
      "  [0.784      0.         0.         0.        ]\n",
      "  [0.784      0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.784      0.         0.         0.        ]\n",
      "  [0.784      0.         0.         0.        ]\n",
      "  [0.784      0.         0.         0.        ]]\n",
      "\n",
      " [[0.784      0.         0.         0.        ]\n",
      "  [0.784      0.         0.         0.        ]\n",
      "  [0.784      0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.8        0.         0.         0.        ]\n",
      "  [0.8        0.         0.         0.        ]\n",
      "  [0.8        0.         0.         0.        ]]] [[[0.07598784]\n",
      "  [0.06990881]\n",
      "  [0.07902736]\n",
      "  ...\n",
      "  [0.27051672]\n",
      "  [0.26443769]\n",
      "  [0.25531915]]\n",
      "\n",
      " [[0.25227964]\n",
      "  [0.25531915]\n",
      "  [0.26139818]\n",
      "  ...\n",
      "  [0.11246201]\n",
      "  [0.10030395]\n",
      "  [0.09726444]]\n",
      "\n",
      " [[0.09422492]\n",
      "  [0.07902736]\n",
      "  [0.06079027]\n",
      "  ...\n",
      "  [0.26139818]\n",
      "  [0.23708207]\n",
      "  [0.20972644]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.3452381 ]\n",
      "  [0.33630952]\n",
      "  [0.31547619]\n",
      "  ...\n",
      "  [0.34821429]\n",
      "  [0.35119048]\n",
      "  [0.34821429]]\n",
      "\n",
      " [[0.3452381 ]\n",
      "  [0.33630952]\n",
      "  [0.32440476]\n",
      "  ...\n",
      "  [0.34821429]\n",
      "  [0.3452381 ]\n",
      "  [0.3422619 ]]\n",
      "\n",
      " [[0.35714286]\n",
      "  [0.37797619]\n",
      "  [0.37797619]\n",
      "  ...\n",
      "  [0.23809524]\n",
      "  [0.23214286]\n",
      "  [0.22916667]]] (3870, 24, 4) (3870, 24, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 13:57:19.051082: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test [[[ 65.]\n",
      "  [ 63.]\n",
      "  [ 66.]\n",
      "  ...\n",
      "  [129.]\n",
      "  [127.]\n",
      "  [124.]]\n",
      "\n",
      " [[123.]\n",
      "  [124.]\n",
      "  [126.]\n",
      "  ...\n",
      "  [ 77.]\n",
      "  [ 73.]\n",
      "  [ 72.]]\n",
      "\n",
      " [[ 71.]\n",
      "  [ 66.]\n",
      "  [ 60.]\n",
      "  ...\n",
      "  [126.]\n",
      "  [118.]\n",
      "  [109.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 66.]\n",
      "  [ 80.]\n",
      "  [ 95.]\n",
      "  ...\n",
      "  [147.]\n",
      "  [150.]\n",
      "  [152.]]\n",
      "\n",
      " [[154.]\n",
      "  [157.]\n",
      "  [160.]\n",
      "  ...\n",
      "  [193.]\n",
      "  [193.]\n",
      "  [192.]]\n",
      "\n",
      " [[191.]\n",
      "  [192.]\n",
      "  [192.]\n",
      "  ...\n",
      "  [208.]\n",
      "  [200.]\n",
      "  [197.]]] pred [[[195.84070677]\n",
      "  [202.49780673]\n",
      "  [202.29171619]\n",
      "  ...\n",
      "  [235.02836353]\n",
      "  [208.27226382]\n",
      "  [208.69542646]]\n",
      "\n",
      " [[115.2167424 ]\n",
      "  [115.59441981]\n",
      "  [116.49907482]\n",
      "  ...\n",
      "  [107.03743647]\n",
      "  [108.21565978]\n",
      "  [109.1570736 ]]\n",
      "\n",
      " [[121.58137989]\n",
      "  [129.8837136 ]\n",
      "  [118.43667299]\n",
      "  ...\n",
      "  [148.7524845 ]\n",
      "  [153.26172864]\n",
      "  [152.59424591]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[153.12500906]\n",
      "  [152.67253542]\n",
      "  [155.06580114]\n",
      "  ...\n",
      "  [141.07402563]\n",
      "  [141.64114475]\n",
      "  [140.49186611]]\n",
      "\n",
      " [[153.12500906]\n",
      "  [152.67253542]\n",
      "  [155.06580114]\n",
      "  ...\n",
      "  [141.07402563]\n",
      "  [141.64114475]\n",
      "  [140.49186611]]\n",
      "\n",
      " [[167.55765247]\n",
      "  [165.67250586]\n",
      "  [171.64136076]\n",
      "  ...\n",
      "  [169.80701303]\n",
      "  [169.02258921]\n",
      "  [165.42774391]]]\n",
      "model trained, with test sMAPE score 28.2867; test RMSE score: 55.9121.\n",
      "tf.Tensor(\n",
      "[[[0.47367996]\n",
      "  [0.4939143 ]\n",
      "  [0.4932879 ]\n",
      "  ...\n",
      "  [0.5927914 ]\n",
      "  [0.51146585]\n",
      "  [0.51275206]]\n",
      "\n",
      " [[0.22862232]\n",
      "  [0.22977027]\n",
      "  [0.23251998]\n",
      "  ...\n",
      "  [0.2037612 ]\n",
      "  [0.20734243]\n",
      "  [0.21020387]]\n",
      "\n",
      " [[0.24796772]\n",
      "  [0.27320278]\n",
      "  [0.23840934]\n",
      "  ...\n",
      "  [0.33055466]\n",
      "  [0.34426057]\n",
      "  [0.34223175]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.33668157]\n",
      "  [0.33533493]\n",
      "  [0.34245774]\n",
      "  ...\n",
      "  [0.30081555]\n",
      "  [0.3025034 ]\n",
      "  [0.29908293]]\n",
      "\n",
      " [[0.33668157]\n",
      "  [0.33533493]\n",
      "  [0.34245774]\n",
      "  ...\n",
      "  [0.30081555]\n",
      "  [0.3025034 ]\n",
      "  [0.29908293]]\n",
      "\n",
      " [[0.37963587]\n",
      "  [0.37402532]\n",
      "  [0.39178976]\n",
      "  ...\n",
      "  [0.3863304 ]\n",
      "  [0.3839958 ]\n",
      "  [0.37329686]]], shape=(4977, 24, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument( \"--dataset\", type=str, help=\"Choose dataset.\")\n",
    "parser.add_argument( \"--horizon\", type=int, help=\"Horizon of forecasting task.\")\n",
    "parser.add_argument( \"--back-horizon\", type=int, help=\"Back horizon of forecasting task.\")\n",
    "parser.add_argument( \"--random-seed\", type=int, default=39, help=\"Random seed parameter, default 39.\")\n",
    "parser.add_argument( \"--train-size\", type=float, default=0.8, help=\"Proportional size of the training set.\")\n",
    "parser.add_argument( \"--test-group\", type=str, default=None, help=\"Extract random 100 samples from test group, i.e., 'hyper'/'hypo'; default None.\")\n",
    "# Parse the arguments from a string\n",
    "args = parser.parse_args(\"--dataset ohiot1dm --horizon 24 --back-horizon 24 --random-seed 35 --train-size 0.8\".split())\n",
    "#args = parser.parse_args()\n",
    "data_path = \"./data/\"\n",
    "lst_arrays, lst_arrays_test, orig_train, orig_test = load_data(args.dataset, data_path) #misschien toch load_data gebruiken?\n",
    "print(f\"The shape of loaded train: {len(lst_arrays)}*{lst_arrays[0].shape}\")\n",
    "print(f\"The shape of test: {len(lst_arrays_test)}*{lst_arrays_test[0].shape}\")\n",
    "\n",
    "print(f\"===========Desired trend parameters=============\")\n",
    "center = \"last\"\n",
    "desired_shift, poly_order = 0, 1\n",
    "fraction_std = 1#args.fraction_std\n",
    "print(f\"center: {center}, desired_shift: {desired_shift};\")\n",
    "print(f\"fraction_std:{fraction_std};\")\n",
    "print(f\"desired_change:'sample_based', poly_order:{poly_order}.\")\n",
    "\n",
    "TARGET_COL = 0\n",
    "if args.dataset == \"ohiot1dm\":\n",
    "    CHANGE_COLS = [1, 2, 3, 4]\n",
    "elif args.dataset == \"simulated\":\n",
    "    CHANGE_COLS = [1, 2]\n",
    "else:\n",
    "    CHANGE_COLS = None\n",
    "\n",
    "RANDOM_STATE = args.random_seed\n",
    "TRAIN_SIZE = args.train_size\n",
    "horizon, back_horizon = args.horizon, args.back_horizon\n",
    "dataset = DataLoader(horizon, back_horizon)\n",
    "dataset.preprocessing(#???\n",
    "    lst_train_arrays=lst_arrays,\n",
    "    lst_test_arrays=lst_arrays_test,\n",
    "    train_size=TRAIN_SIZE,\n",
    "    normalize=True,\n",
    "    sequence_stride= horizon,\n",
    "    target_col=TARGET_COL,\n",
    "    horizon = horizon\n",
    ")\n",
    "\n",
    "#print(dataset.X_train.shape, dataset.Y_train.shape)\n",
    "#print(dataset.X_val.shape, dataset.Y_val.shape)\n",
    "#print(dataset.X_test.shape, dataset.Y_test.shape)\n",
    "\n",
    "\n",
    "print(dataset.X_train_exog, dataset.X_train_target, dataset.X_train_exog.shape, dataset.X_train_target.shape)\n",
    "X = dataset.X_train_exog\n",
    "y = dataset.X_train_target\n",
    "tf.random.set_seed(args.random_seed)\n",
    "#tf_model = tf.keras.Sequential([\n",
    "#    tf.keras.layers.Dense(64, activation='relu'),\n",
    "#    tf.keras.layers.Dense(1)  # Assuming regression\n",
    "#])\n",
    "\n",
    "n_in_features = dataset.X_train_exog.shape[2]\n",
    "n_out_features = 1\n",
    "tf_model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(back_horizon, n_in_features)),\n",
    "        # Shape [batch, time, features] => [batch, time, gru_units]\n",
    "        tf.keras.layers.GRU(100, activation=\"tanh\", return_sequences=True),\n",
    "        tf.keras.layers.GRU(100, activation=\"tanh\", return_sequences=False),\n",
    "        # Shape => [batch, time, features]\n",
    "        tf.keras.layers.Dense(horizon, activation=\"linear\"),\n",
    "        tf.keras.layers.Reshape((horizon, n_out_features)),\n",
    "    ]\n",
    ")\n",
    "#orig_test_metric = np.asarray(orig_test.drop(['time'], axis=1))#[orig_test.patient_id==544], 'patient_id'\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "tf_model.compile(optimizer=optimizer, loss=\"mae\")\n",
    "#tf_model.compile(optimizer='adam', loss='mse')\n",
    "tf_model.fit(X, y, epochs=100, verbose=0)\n",
    "pred_tf = tf_model(dataset.X_test_exog)#[-horizon:])\n",
    "mean_smape, mean_rmse = forecast_metrics(dataset, pred_tf)\n",
    "print(\n",
    "    f\"model trained, with test sMAPE score {mean_smape:0.4f}; test RMSE score: {mean_rmse:0.4f}.\"\n",
    ")\n",
    "print(pred_tf)\n",
    "#Welke vorm moet pred hebben???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9df2cf7f-e23f-426d-92e7-30e3ee3ed2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 14:08:50.201798: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2024-12-16 14:15:45.997789: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 4s 18ms/step\n",
      "[[[0.10829307]\n",
      "  [0.09837344]\n",
      "  [0.1328608 ]\n",
      "  ...\n",
      "  [0.29508132]\n",
      "  [0.26594257]\n",
      "  [0.22990073]]\n",
      "\n",
      " [[0.26646215]\n",
      "  [0.26404414]\n",
      "  [0.2761548 ]\n",
      "  ...\n",
      "  [0.17744146]\n",
      "  [0.19072016]\n",
      "  [0.17071423]]\n",
      "\n",
      " [[0.11153011]\n",
      "  [0.10324298]\n",
      "  [0.0784109 ]\n",
      "  ...\n",
      "  [0.33868796]\n",
      "  [0.37203145]\n",
      "  [0.3535967 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.35647693]\n",
      "  [0.366659  ]\n",
      "  [0.35426667]\n",
      "  ...\n",
      "  [0.33167237]\n",
      "  [0.34150228]\n",
      "  [0.33777672]]\n",
      "\n",
      " [[0.35647693]\n",
      "  [0.366659  ]\n",
      "  [0.35426667]\n",
      "  ...\n",
      "  [0.33167237]\n",
      "  [0.34150228]\n",
      "  [0.33777672]]\n",
      "\n",
      " [[0.35647693]\n",
      "  [0.366659  ]\n",
      "  [0.35426667]\n",
      "  ...\n",
      "  [0.33167237]\n",
      "  [0.34150228]\n",
      "  [0.33777672]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from nbeats_keras.model import NBeatsNet\n",
    "\n",
    "# Prepare the data\n",
    "X = dataset.X_train_exog  # Shape: (num_samples, back_horizon, num_features)\n",
    "y = dataset.X_train_target  # Shape: (num_samples, horizon)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(args.random_seed)\n",
    "\n",
    "# Define model hyperparameters\n",
    "horizon, back_horizon = args.horizon, args.back_horizon\n",
    "\n",
    "# Create the N-BEATSx model\n",
    "nbeatsx_model = NBeatsNet(\n",
    "    stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n",
    "    forecast_length=horizon,\n",
    "    backcast_length=back_horizon,\n",
    "    hidden_layer_units=256,\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "nbeatsx_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=\"mae\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "nbeatsx_model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "pred_nbeatsx = nbeatsx_model.predict(dataset.X_test_exog)  # Predictions on the test set\n",
    "print(pred_nbeatsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c53a9935-b403-4958-9b29-8b663aa11701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 samples been transformed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 11:35:00.600249: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "No gradient defined for operation'IteratorGetNext' (op type: IteratorGetNext). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m tape\u001b[38;5;241m.\u001b[39mwatch(grad_X_e)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Calculate the value of the function and record the gradient\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m pred_nbeats \u001b[38;5;241m=\u001b[39m \u001b[43mnbeatsx_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_X_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m pred_nbeats \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(pred_nbeats)\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(max_bound, min_bound, pred_nbeats)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/nbeats_keras/model.py:203\u001b[0m, in \u001b[0;36mNBeatsNet.__getattr__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn_backcast\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    202\u001b[0m     cast_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_BACKCAST\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcast_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/ops/gradients_util.py:674\u001b[0m, in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    672\u001b[0m       grad_fn \u001b[38;5;241m=\u001b[39m func_call\u001b[38;5;241m.\u001b[39mpython_grad_func\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 674\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(\n\u001b[1;32m    675\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradient defined for operation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    676\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (op type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    677\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn general every operation must have an associated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    678\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`@tf.RegisterGradient` for correct autodiff, which this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    679\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mop is lacking. If you want to pretend this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    680\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperation is a constant in your program, you may insert \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    681\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.stop_gradient`. This can be useful to silence the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in cases where you know gradients are not needed, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me.g. the forward pass of tf.custom_gradient. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see more details in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    685\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/api_docs/python/tf/custom_gradient.\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop_state:\n\u001b[1;32m    687\u001b[0m   loop_state\u001b[38;5;241m.\u001b[39mEnterGradWhileContext(op, before\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mLookupError\u001b[0m: No gradient defined for operation'IteratorGetNext' (op type: IteratorGetNext). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient."
     ]
    }
   ],
   "source": [
    "#nbeats\n",
    "def compute_loss(max_bound, min_bound, pred):\n",
    "    mse_loss_ = tf.keras.losses.MeanSquaredError(\n",
    "        reduction=tf.keras.losses.Reduction.SUM\n",
    "    )\n",
    "    dist_max = mse_loss_(max_bound, pred)\n",
    "    dist_min = mse_loss_(min_bound, pred)\n",
    "    loss = dist_max + dist_min\n",
    "    return loss\n",
    "\n",
    "\n",
    "targets_nbeats = np.empty(X_test.shape)\n",
    "exogs_nbeats = np.empty(X_test.shape)\n",
    "losses_nbeats = np.empty(X_test.shape[0])\n",
    "max_bound_nbeats = np.empty(X_test.shape)\n",
    "min_bound_nbeats = np.empty(X_test.shape)\n",
    "\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    print(f\"{i} samples been transformed.\")\n",
    "    max_bound = (\n",
    "        desired_max_lst[i] if desired_max_lst != None else self.MISSING_MAX_BOUND\n",
    "    )\n",
    "    min_bound = (\n",
    "        desired_min_lst[i] if desired_min_lst != None else self.MISSING_MIN_BOUND\n",
    "    )\n",
    "    min_bound_nbeats[i] = min_bound\n",
    "    max_bound_nbeats[i] = max_bound\n",
    "    X = X_test[i]\n",
    "    y = Y_test[i]\n",
    "    X_test_exog = dataset.X_test_exog[i]\n",
    "#X = dataset.X_train_exog\n",
    "#X = random.choice(X)\n",
    "#X = tf.Variable(tf.convert_to_tensor(X, dtype=tf.float32), dtype=tf.float32)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.05,  epsilon=1e-07,)\n",
    "    \n",
    "    #nbeatsx_model.compile(optimizer=optimizer, loss=\"mae\")\n",
    "    #nbeatsx_model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "    grad_X_e = tf.convert_to_tensor(X_test_exog, dtype=tf.float32)\n",
    "    grad_X_e = tf.Variable(grad_X_e, dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(grad_X_e)\n",
    "        # Calculate the value of the function and record the gradient\n",
    "        pred_nbeats = nbeatsx_model.predict(tf.expand_dims(grad_X_e, axis=0))\n",
    "        pred_nbeats = tf.squeeze(pred_nbeats)\n",
    "        loss = compute_loss(max_bound, min_bound, pred_nbeats)\n",
    "        print(loss)\n",
    "#pred = nbeatsx_model(grad_X_e)\n",
    "\n",
    "    #if i == 0:\n",
    "    #    mean_smape, mean_rmse = forecast_metrics(dataset, pred_nbeats)\n",
    "    #    print(\n",
    "    #        f\"model trained, with test sMAPE score {mean_smape:0.4f}; test RMSE score: {mean_rmse:0.4f}.\"\n",
    "    #    )\n",
    "        \n",
    "    max_iter = 100\n",
    "    it = 0\n",
    "    while (tf.reduce_any(pred_nbeats>max_bound) or tf.reduce_any(pred_nbeats<min_bound)) and (it<max_iter):\n",
    "        #change (X_e)\n",
    "        gradient = tape.gradient(loss, grad_X_e)\n",
    "        #print(gradient)\n",
    "        if gradient is None:\n",
    "            print(\"no gradient\")\n",
    "            #break\n",
    "    \n",
    "        # Use the Adam optimizer to update the value of x\n",
    "        optimizer.apply_gradients([(gradient, grad_X_e)])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(grad_X_e)\n",
    "            # Calculate the value of the function and record the gradient\n",
    "            pred_nbeats = nbeatsx_model.predict(tf.expand_dims(grad_X_e, axis=0))\n",
    "            pred_nbeats = tf.squeeze(pred_nbeats)\n",
    "            loss = compute_loss(max_bound, min_bound, pred_nbeats)\n",
    "        # Record the current value of x\n",
    "        #print(f\"Iteration {it}, Loss: {loss.numpy()}, Grad_X_e: {grad_X_e.numpy()}\")\n",
    "        it += 1\n",
    "        \n",
    "    print(\"Optimized value of x:\", grad_X_e.numpy())\n",
    "    final_pred = nbeatsx_model.predict(tf.expand_dims(grad_X_e, axis=0))\n",
    "    print(\"Value of the function at the optimized point:\", final_pred.numpy())\n",
    "\n",
    "\n",
    "    targets_nbeats[i] = final_pred\n",
    "    exogs_nbeats[i] = grad_X_e.numpy()\n",
    "    losses_nbeats[i] = loss\n",
    "\n",
    "\n",
    "print(\"results\", targets_nbeats, exogs_nbeats, losses_nbeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5789e29d-2942-45ab-97ed-f5333c3d6a22",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========CF generation setup=============\n",
      "hyper bound value: 180, hypo bound: 70.\n",
      "hyper_indices shape: (942,)\n",
      "hypo_indices shape: (52,)\n",
      "LSASLSLKDGNS [[166.63123]\n",
      " [168.45897]\n",
      " [170.2148 ]\n",
      " [168.3485 ]\n",
      " [171.7133 ]\n",
      " [171.25824]\n",
      " [172.22322]\n",
      " [172.79024]\n",
      " [176.06343]\n",
      " [172.47719]\n",
      " [177.32266]\n",
      " [176.6106 ]\n",
      " [178.45052]\n",
      " [179.68536]\n",
      " [177.3807 ]\n",
      " [175.39018]\n",
      " [175.7042 ]\n",
      " [176.10582]\n",
      " [174.79025]\n",
      " [174.2059 ]\n",
      " [171.37386]\n",
      " [170.83212]\n",
      " [169.08313]\n",
      " [166.77776]]\n",
      "None\n",
      "Generating CFs for 4977 samples in total, for None test group...\n",
      "Group not identified: None, use a default center\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:88: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  change_percent = (desired_center - start_value) / start_value\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/numpy/lib/polynomial.py:780: RuntimeWarning: invalid value encountered in multiply\n",
      "  y = y * x + pv\n",
      "/tmp/ipykernel_71006/600091445.py:92: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n",
      "/tmp/ipykernel_71006/600091445.py:102: RuntimeWarning: invalid value encountered in multiply\n",
      "  start_value\n"
     ]
    }
   ],
   "source": [
    "hyper_bound, hypo_bound = 180, 70\n",
    "printst(f\"===========CF generation setup=============\")\n",
    "print(f\"hyper bound value: {hyper_bound}, hypo bound: {hypo_bound}.\")\n",
    "\n",
    "event_labels = list()\n",
    "for i in range(len(pred_tf)):\n",
    "    scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "    Y_preds_original = scaler.inverse_transform(pred_tf[i].numpy())\n",
    "    if np.any(Y_preds_original >= hyper_bound):\n",
    "        event_labels.append(\"hyper\")\n",
    "    elif np.any(Y_preds_original <= hypo_bound):\n",
    "        event_labels.append(\"hypo\")\n",
    "    else:\n",
    "        event_labels.append(\"normal\")\n",
    "hyper_indices = np.argwhere(np.array(event_labels) == \"hyper\").reshape(-1)\n",
    "hypo_indices = np.argwhere(np.array(event_labels) == \"hypo\").reshape(-1)\n",
    "\n",
    "print(f\"hyper_indices shape: {hyper_indices.shape}\")\n",
    "print(f\"hypo_indices shape: {hypo_indices.shape}\")\n",
    "\n",
    "print(\"LSASLSLKDGNS\", Y_preds_original)\n",
    "#plot(orig_train, orig_test, Y_preds_original)\n",
    "\n",
    "# use a subset of the test\n",
    "rand_test_size = 100\n",
    "print(args.test_group)\n",
    "if args.test_group == \"hyper\":\n",
    "    if len(hyper_indices) >= rand_test_size:\n",
    "        print(\"if\", hyper_indices)\n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        rand_test_idx = np.random.choice(\n",
    "            hyper_indices, rand_test_size, replace=False\n",
    "        )\n",
    "    else:\n",
    "        print(\"else\", hyper_indices)\n",
    "        rand_test_idx = hyper_indices\n",
    "elif args.test_group == \"hypo\":\n",
    "    if len(hypo_indices) >= rand_test_size:\n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        rand_test_idx = np.random.choice(\n",
    "            hypo_indices, rand_test_size, replace=False\n",
    "        )\n",
    "    else:\n",
    "        rand_test_idx = hypo_indices\n",
    "else:\n",
    "    rand_test_idx = np.arange(dataset.X_test_exog.shape[0])\n",
    "\n",
    "X_test = dataset.X_test_exog[rand_test_idx]\n",
    "Y_test = dataset.X_test_target[rand_test_idx]\n",
    "\n",
    "print(\n",
    "    f\"Generating CFs for {len(rand_test_idx)} samples in total, for {args.test_group} test group...\"\n",
    ")\n",
    "\n",
    "# loss calculation ==> min/max bounds\n",
    "desired_max_lst, desired_min_lst = list(), list()\n",
    "hist_inputs = list()\n",
    "\n",
    "# define the desired center to reach in two hours (24 timesteps for OhioT1DM)\n",
    "# then we need to cut the first 6 steps to generate the desired bounds\n",
    "desired_steps = 24 if args.dataset == \"ohiot1dm\" else 20\n",
    "if args.test_group == \"hyper\":\n",
    "    desired_center_2h = hyper_bound - 10  # -10 for a fluctuating bound\n",
    "elif args.test_group == \"hypo\":\n",
    "    desired_center_2h = hypo_bound + 10  # +10 for a fluctuating bound\n",
    "else:\n",
    "    print(\n",
    "        f\"Group not identified: {args.test_group}, use a default center\"\n",
    "    )\n",
    "    desired_center_2h = (hyper_bound + hypo_bound) / 2\n",
    "#print(f\"desired center {desired_center_2h} in {desired_steps} timesteps.\")\n",
    "\n",
    "for i in range(len(X_test)): #???Maybe join exog, target\n",
    "    idx = dataset.test_idxs[rand_test_idx[i]]\n",
    "    scaler = dataset.scaler[idx]\n",
    "\n",
    "    desired_center_scaled = scaler[TARGET_COL].transform(\n",
    "        np.array(desired_center_2h).reshape(-1, 1)\n",
    "    )[0][0]\n",
    "    #print(\n",
    "    #    f\"desired_center: {desired_center_2h}; after scaling: {desired_center_scaled:0.4f}\"\n",
    "    #)\n",
    "\n",
    "    # desired trend bounds: use the `center` parameter from the input sequence as the starting point\n",
    "    desired_max_scaled, desired_min_scaled = generate_bounds(\n",
    "        center=center,  # Use the parameters defined at the beginning of the script\n",
    "        shift=desired_shift,\n",
    "        desired_center=desired_center_scaled,\n",
    "        poly_order=poly_order,\n",
    "        horizon=horizon,\n",
    "        fraction_std=fraction_std,\n",
    "        input_series=X_test[i, :, TARGET_COL],\n",
    "        desired_steps=desired_steps,\n",
    "    )\n",
    "    # TODO: remove the ones that already satisfy the bounds here, OR afterwards?\n",
    "    desired_max_lst.append(desired_max_scaled)\n",
    "    desired_min_lst.append(desired_min_scaled)\n",
    "    hist_inputs.append(dataset.historical_values[idx])\n",
    "    \n",
    "for i in range(X_test.shape[0]):\n",
    "    max_bound = (\n",
    "        desired_max_lst[i] if desired_max_lst != None else self.MISSING_MAX_BOUND\n",
    "    )\n",
    "    min_bound = (\n",
    "        desired_min_lst[i] if desired_min_lst != None else self.MISSING_MIN_BOUND\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f236e-95b7-4f45-99a2-fe305c721507",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST (4977, 24, 4)\n",
      "0 samples been transformed.\n",
      "Optimized value of X: [[3.9066668 3.4542856 3.4542856 3.4542856]\n",
      " [3.9308414 3.4784603 3.4784603 3.4784603]\n",
      " [3.9518344 3.4994533 3.4994533 3.4994533]\n",
      " [4.012013  4.042088  3.559632  3.559632 ]\n",
      " [3.9913402 3.538959  3.538959  3.538959 ]\n",
      " [4.0107493 3.5583684 3.5583684 3.5583684]\n",
      " [4.0292034 3.5768216 3.5768216 3.5768216]\n",
      " [4.029744  3.5773628 3.5773628 3.5773628]\n",
      " [4.2663016 3.3615403 3.3615403 3.3615403]\n",
      " [4.2875557 3.3827949 3.3827949 3.3827949]\n",
      " [4.381385  3.9678512 3.4766228 3.4766228]\n",
      " [4.38771   3.9741764 3.4829483 3.4829483]\n",
      " [4.41088   3.9973474 3.5061183 3.5061183]\n",
      " [4.423017  4.009483  3.5182548 3.5182548]\n",
      " [4.454678  4.0411453 3.5499163 3.5499163]\n",
      " [4.4513454 4.0378113 3.5465834 3.5465834]\n",
      " [4.468054  4.054521  3.5632935 3.5632935]\n",
      " [4.208569  3.756189  3.756189  3.756189 ]\n",
      " [4.230275  3.777894  3.777894  3.777894 ]\n",
      " [4.234452  3.7820706 3.7820706 3.7820706]\n",
      " [4.244545  3.7921636 3.7921636 3.7921636]\n",
      " [4.250658  3.7982771 3.7982771 3.7982771]\n",
      " [4.27311   3.8207283 3.8207283 3.8207283]\n",
      " [4.2736573 3.8212767 3.8212767 3.8212767]] Best prediction: [0.34443384 0.34381697 0.34312335 0.3406397  0.34169063 0.34097493\n",
      " 0.34024063 0.339044   0.3466741  0.346032   0.34442186 0.34340164\n",
      " 0.34282827 0.34197244 0.3416335  0.34038514 0.33966714 0.33195665\n",
      " 0.33138144 0.3303536  0.32948643 0.32852107 0.327989   0.32688972]\n",
      "Original Prediction after scaling inversion: [[153.31873]\n",
      " [153.11578]\n",
      " [152.88757]\n",
      " [152.07047]\n",
      " [152.41621]\n",
      " [152.18076]\n",
      " [151.93916]\n",
      " [151.54547]\n",
      " [154.05579]\n",
      " [153.84453]\n",
      " [153.31479]\n",
      " [152.97914]\n",
      " [152.7905 ]\n",
      " [152.50893]\n",
      " [152.39742]\n",
      " [151.98671]\n",
      " [151.75049]\n",
      " [149.21373]\n",
      " [149.02449]\n",
      " [148.68633]\n",
      " [148.40103]\n",
      " [148.08344]\n",
      " [147.90839]\n",
      " [147.54672]]\n",
      "1 samples been transformed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/statsmodels/tsa/statespace/representation.py:374: FutureWarning: Unknown keyword arguments: dict_keys(['start_params']).Passing unknown keyword arguments will raise a TypeError beginning in version 0.15.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized value of X: [[-3.778317  -4.230698  -4.230698  -4.230698 ]\n",
      " [-3.7722085 -4.2245893 -4.2245893 -4.2245893]\n",
      " [-3.9983697 -4.188846  -4.188846  -4.188846 ]\n",
      " [-4.001818  -4.192295  -4.192295  -4.192295 ]\n",
      " [-3.925522  -4.2112355 -4.2112355 -4.2112355]\n",
      " [-3.9192479 -4.2049613 -4.2049613 -4.2049613]\n",
      " [-3.9269724 -4.2126865 -4.2126865 -4.2126865]\n",
      " [-3.920852  -4.2065673 -4.2065673 -4.2065673]\n",
      " [-3.9255726 -4.2112865 -4.2112865 -4.2112865]\n",
      " [-3.9289455 -4.214659  -4.214659  -4.214659 ]\n",
      " [-3.925297  -4.2110114 -4.2110114 -4.2110114]\n",
      " [-4.0107775 -4.201254  -4.201254  -4.201254 ]\n",
      " [-4.016782  -4.207258  -4.207258  -4.207258 ]\n",
      " [-4.014168  -4.204644  -4.204644  -4.204644 ]\n",
      " [-4.0156937 -4.20617   -4.20617   -4.20617  ]\n",
      " [-4.012097  -4.2025723 -4.2025723 -4.2025723]\n",
      " [-4.02025   -4.2107253 -4.2107253 -4.2107253]\n",
      " [-4.02444   -4.2149158 -4.2149158 -4.2149158]\n",
      " [-4.016452  -4.2069287 -4.2069287 -4.2069287]\n",
      " [-4.0153685 -4.2058454 -4.2058454 -4.2058454]\n",
      " [-4.030753  -4.22123   -4.22123   -4.22123  ]\n",
      " [-4.017947  -4.208424  -4.208424  -4.208424 ]\n",
      " [-4.0205007 -4.2109766 -4.2109766 -4.2109766]\n",
      " [-4.02392   -4.214396  -4.214396  -4.214396 ]] Best prediction: [0.17369024 0.17342143 0.17763443 0.17755046 0.17592807 0.17565696\n",
      " 0.17565618 0.17538849 0.17533019 0.1752461  0.17502679 0.1765274\n",
      " 0.17649476 0.17629607 0.17617747 0.17596024 0.17596996 0.17590342\n",
      " 0.17560214 0.1754343  0.17558444 0.17519085 0.17509383 0.17501375]\n",
      "Original Prediction after scaling inversion: [[97.1441  ]\n",
      " [97.05565 ]\n",
      " [98.441734]\n",
      " [98.4141  ]\n",
      " [97.88034 ]\n",
      " [97.791145]\n",
      " [97.790886]\n",
      " [97.70281 ]\n",
      " [97.68363 ]\n",
      " [97.65597 ]\n",
      " [97.58382 ]\n",
      " [98.077515]\n",
      " [98.06678 ]\n",
      " [98.00141 ]\n",
      " [97.96239 ]\n",
      " [97.890915]\n",
      " [97.89411 ]\n",
      " [97.87223 ]\n",
      " [97.7731  ]\n",
      " [97.71789 ]\n",
      " [97.76728 ]\n",
      " [97.637794]\n",
      " [97.605865]\n",
      " [97.57952 ]]\n",
      "2 samples been transformed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/statsmodels/tsa/statespace/representation.py:374: FutureWarning: Unknown keyword arguments: dict_keys(['start_params']).Passing unknown keyword arguments will raise a TypeError beginning in version 0.15.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized value of X: [[0.68287355 0.68287355 0.68287355 0.68287355]\n",
      " [0.7344569  0.7344569  0.7344569  0.7344569 ]\n",
      " [0.78815734 0.78815734 0.78815734 0.78815734]\n",
      " [0.84162414 0.84162414 0.84162414 0.84162414]\n",
      " [0.8985726  0.8985726  0.8985726  0.8985726 ]\n",
      " [0.9528092  0.9528092  0.9528092  0.9528092 ]\n",
      " [1.0057334  1.0057334  1.0057334  1.0057334 ]\n",
      " [1.060337   1.060337   1.060337   1.060337  ]\n",
      " [1.1092885  1.1092885  1.1092885  1.1092885 ]\n",
      " [1.1591986  1.1591986  1.1591986  1.1591986 ]\n",
      " [1.205463   1.205463   1.205463   1.205463  ]\n",
      " [1.2519381  1.2519381  1.2519381  1.2519381 ]\n",
      " [1.2122837  1.0218072  1.0218072  1.0218072 ]\n",
      " [1.5277799  1.9162512  1.3373038  1.3373038 ]\n",
      " [1.3073659  1.1168897  1.1168897  1.1168897 ]\n",
      " [1.3554529  1.1649768  1.1649768  1.1649768 ]\n",
      " [1.4417766  1.3477917  1.2513002  1.2513002 ]\n",
      " [1.4415125  1.2510363  1.2510363  1.2510363 ]\n",
      " [1.4837466  1.2932705  1.2932705  1.2932705 ]\n",
      " [1.5254439  1.3349679  1.3349679  1.3349679 ]\n",
      " [1.566947   1.3764708  1.3764708  1.3764708 ]\n",
      " [1.6080505  1.4175743  1.4175743  1.4175743 ]\n",
      " [1.6492385  1.4587622  1.4587622  1.4587622 ]\n",
      " [1.690596   1.5001196  1.5001196  1.5001196 ]] Best prediction: [0.2243129  0.22418568 0.22417451 0.22418022 0.2243588  0.2244493\n",
      " 0.22451034 0.22466749 0.22461197 0.22462162 0.22450243 0.22441639\n",
      " 0.22453828 0.22424515 0.2245747  0.22465076 0.22442949 0.22444563\n",
      " 0.22434323 0.22424032 0.22415106 0.22406657 0.22400694 0.22397551]\n",
      "Original Prediction after scaling inversion: [[113.79894 ]\n",
      " [113.75709 ]\n",
      " [113.75342 ]\n",
      " [113.75529 ]\n",
      " [113.81404 ]\n",
      " [113.84382 ]\n",
      " [113.8639  ]\n",
      " [113.9156  ]\n",
      " [113.89734 ]\n",
      " [113.90051 ]\n",
      " [113.861305]\n",
      " [113.83299 ]\n",
      " [113.8731  ]\n",
      " [113.77666 ]\n",
      " [113.88508 ]\n",
      " [113.9101  ]\n",
      " [113.8373  ]\n",
      " [113.84261 ]\n",
      " [113.80892 ]\n",
      " [113.77507 ]\n",
      " [113.745705]\n",
      " [113.7179  ]\n",
      " [113.69829 ]\n",
      " [113.68794 ]]\n",
      "3 samples been transformed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/statsmodels/tsa/statespace/representation.py:374: FutureWarning: Unknown keyword arguments: dict_keys(['start_params']).Passing unknown keyword arguments will raise a TypeError beginning in version 0.15.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized value of X: [[0.24915352 0.05867732 0.05867732 0.05867732]\n",
      " [0.24994421 0.05946805 0.05946805 0.05946805]\n",
      " [0.25055668 0.06008055 0.06008055 0.06008055]\n",
      " [0.2510299  0.06055363 0.06055363 0.06055363]\n",
      " [0.25140828 0.0609321  0.0609321  0.0609321 ]\n",
      " [0.2517077  0.06123152 0.06123152 0.06123152]\n",
      " [0.251949   0.06147268 0.06147268 0.06147268]\n",
      " [0.2521307  0.06165444 0.06165444 0.06165444]\n",
      " [0.25228754 0.06181137 0.06181137 0.06181137]\n",
      " [0.25239804 0.06192197 0.06192197 0.06192197]\n",
      " [0.25249478 0.06201848 0.06201848 0.06201848]\n",
      " [0.25256523 0.0620891  0.0620891  0.0620891 ]\n",
      " [0.25263676 0.06216059 0.06216059 0.06216059]\n",
      " [0.25267673 0.06220064 0.06220064 0.06220064]\n",
      " [0.2527193  0.06224309 0.06224309 0.06224309]\n",
      " [0.25275353 0.06227731 0.06227731 0.06227731]\n",
      " [0.2527817  0.06230554 0.06230554 0.06230554]\n",
      " [0.25279203 0.06231591 0.06231591 0.06231591]\n",
      " [0.25280675 0.06233042 0.06233042 0.06233042]\n",
      " [0.25281748 0.06234126 0.06234126 0.06234126]\n",
      " [0.25282833 0.06235212 0.06235212 0.06235212]\n",
      " [0.25284427 0.06236805 0.06236805 0.06236805]\n",
      " [0.25285152 0.06237535 0.06237535 0.06237535]\n",
      " [0.25285482 0.06237847 0.06237847 0.06237847]] Best prediction: [0.22455133 0.22460532 0.22462822 0.22462972 0.22462912 0.22462447\n",
      " 0.2246213  0.22460783 0.22460662 0.2245918  0.22458695 0.22457655\n",
      " 0.22458142 0.22456977 0.2245697  0.22456962 0.22457016 0.22455966\n",
      " 0.22455691 0.22455373 0.22455314 0.22455908 0.22455892 0.22455655]\n",
      "Original Prediction after scaling inversion: [[113.877396]\n",
      " [113.89515 ]\n",
      " [113.90269 ]\n",
      " [113.90318 ]\n",
      " [113.902985]\n",
      " [113.90145 ]\n",
      " [113.900406]\n",
      " [113.89597 ]\n",
      " [113.895584]\n",
      " [113.89071 ]\n",
      " [113.88911 ]\n",
      " [113.88569 ]\n",
      " [113.88728 ]\n",
      " [113.88345 ]\n",
      " [113.88343 ]\n",
      " [113.8834  ]\n",
      " [113.883575]\n",
      " [113.88013 ]\n",
      " [113.87923 ]\n",
      " [113.878174]\n",
      " [113.87798 ]\n",
      " [113.879944]\n",
      " [113.87988 ]\n",
      " [113.879105]]\n",
      "4 samples been transformed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/statsmodels/tsa/statespace/representation.py:374: FutureWarning: Unknown keyword arguments: dict_keys(['start_params']).Passing unknown keyword arguments will raise a TypeError beginning in version 0.15.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized value of X: [[-0.2799685  -0.47044486 -0.47044486 -0.47044486]\n",
      " [-0.2752821  -0.46575838 -0.46575838 -0.46575838]\n",
      " [-0.27064267 -0.46111906 -0.46111906 -0.46111906]\n",
      " [-0.2687215  -0.2687215  -0.2687215  -0.2687215 ]\n",
      " [-0.26433167 -0.26433167 -0.26433167 -0.26433167]\n",
      " [-0.2599982  -0.2599982  -0.2599982  -0.2599982 ]\n",
      " [-0.2553324  -0.2553324  -0.2553324  -0.2553324 ]\n",
      " [-0.2506841  -0.2506841  -0.2506841  -0.2506841 ]\n",
      " [-0.24613626 -0.24613626 -0.24613626 -0.24613626]\n",
      " [-0.24163109 -0.24163109 -0.24163109 -0.24163109]\n",
      " [-0.23661959 -0.23661959 -0.23661959 -0.23661959]\n",
      " [-0.23209542 -0.23209542 -0.23209542 -0.23209542]\n",
      " [-0.22681361 -0.22681361 -0.22681361 -0.22681361]\n",
      " [-0.22127855 -0.22127855 -0.22127855 -0.22127855]\n",
      " [-0.21663441 -0.21663441 -0.21663441 -0.21663441]\n",
      " [-0.21208958 -0.21208958 -0.21208958 -0.21208958]\n",
      " [-0.2096013  -0.4000774  -0.4000774  -0.4000774 ]\n",
      " [-0.20527092 -0.39574692 -0.39574692 -0.39574692]\n",
      " [-0.20118251 -0.39165863 -0.39165863 -0.39165863]\n",
      " [-0.19706558 -0.38754165 -0.38754165 -0.38754165]\n",
      " [-0.19259086 -0.38306707 -0.38306707 -0.38306707]\n",
      " [-0.18792635 -0.37840262 -0.37840262 -0.37840262]\n",
      " [-0.18375821 -0.3742344  -0.3742344  -0.3742344 ]\n",
      " [-0.17916551 -0.36964166 -0.36964166 -0.36964166]] Best prediction: [0.22470132 0.22470362 0.22470146 0.22436887 0.22433874 0.22430299\n",
      " 0.22430903 0.22431415 0.22430828 0.22429842 0.22435158 0.2243465\n",
      " 0.22443509 0.22455582 0.22456905 0.22457138 0.22432399 0.22430257\n",
      " 0.22425285 0.22420782 0.22420764 0.22423182 0.22419663 0.22421445]\n",
      "Original Prediction after scaling inversion: [[113.92673 ]\n",
      " [113.9275  ]\n",
      " [113.92678 ]\n",
      " [113.81736 ]\n",
      " [113.80744 ]\n",
      " [113.795685]\n",
      " [113.79767 ]\n",
      " [113.799355]\n",
      " [113.797424]\n",
      " [113.794174]\n",
      " [113.81167 ]\n",
      " [113.810005]\n",
      " [113.83914 ]\n",
      " [113.87886 ]\n",
      " [113.88322 ]\n",
      " [113.88398 ]\n",
      " [113.80259 ]\n",
      " [113.79555 ]\n",
      " [113.77918 ]\n",
      " [113.76437 ]\n",
      " [113.76431 ]\n",
      " [113.77227 ]\n",
      " [113.7607  ]\n",
      " [113.766556]]\n",
      "5 samples been transformed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/statsmodels/tsa/statespace/representation.py:374: FutureWarning: Unknown keyword arguments: dict_keys(['start_params']).Passing unknown keyword arguments will raise a TypeError beginning in version 0.15.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized value of X: [[-1.1113175 -1.3017937 -1.3017937 -1.3017937]\n",
      " [-1.109624  -1.3001001 -1.3001001 -1.3001001]\n",
      " [-1.1082654 -1.2987416 -1.2987416 -1.2987416]\n",
      " [-1.1066693 -1.2971454 -1.2971454 -1.2971454]\n",
      " [-1.1051402 -1.2956164 -1.2956164 -1.2956164]\n",
      " [-1.1030813 -1.2935574 -1.2935574 -1.2935574]\n",
      " [-1.1020406 -1.2925167 -1.2925167 -1.2925167]\n",
      " [-1.1005611 -1.2910373 -1.2910373 -1.2910373]\n",
      " [-1.0907    -1.4716525 -1.4716525 -1.4716525]\n",
      " [-1.0892365 -1.4701886 -1.4701886 -1.4701886]\n",
      " [-1.0874387 -1.4683912 -1.4683912 -1.4683912]\n",
      " [-1.0859991 -1.4669515 -1.4669515 -1.4669515]\n",
      " [-1.0841686 -1.4651212 -1.4651212 -1.4651212]\n",
      " [-1.0828687 -1.4638214 -1.4638214 -1.4638214]\n",
      " [-1.0810962 -1.4620486 -1.4620486 -1.4620486]\n",
      " [-1.0888017 -1.2792779 -1.2792779 -1.2792779]\n",
      " [-1.087623  -1.2780991 -1.2780991 -1.2780991]\n",
      " [-1.0857683 -1.2762445 -1.2762445 -1.2762445]\n",
      " [-1.0844079 -1.2748841 -1.2748841 -1.2748841]\n",
      " [-1.0827056 -1.2731818 -1.2731818 -1.2731818]\n",
      " [-1.0810301 -1.2715063 -1.2715063 -1.2715063]\n",
      " [-1.0794185 -1.269895  -1.269895  -1.269895 ]\n",
      " [-1.078181  -1.2686572 -1.2686572 -1.2686572]\n",
      " [-1.0762953 -1.2667714 -1.2667714 -1.2667714]] Best prediction: [0.22447897 0.22448419 0.22447056 0.22447035 0.22446637 0.22449227\n",
      " 0.22446086 0.22445418 0.2249195  0.22491199 0.22492331 0.2249145\n",
      " 0.22492775 0.22491112 0.22492115 0.2243975  0.22437415 0.22438888\n",
      " 0.22437581 0.22438201 0.22438675 0.2243879  0.22436804 0.22438468]\n",
      "Original Prediction after scaling inversion: [[113.853584]\n",
      " [113.8553  ]\n",
      " [113.85081 ]\n",
      " [113.85074 ]\n",
      " [113.84944 ]\n",
      " [113.857956]\n",
      " [113.847626]\n",
      " [113.84543 ]\n",
      " [113.99851 ]\n",
      " [113.99604 ]\n",
      " [113.99977 ]\n",
      " [113.99687 ]\n",
      " [114.00123 ]\n",
      " [113.99576 ]\n",
      " [113.99906 ]\n",
      " [113.82678 ]\n",
      " [113.81909 ]\n",
      " [113.823944]\n",
      " [113.81964 ]\n",
      " [113.82168 ]\n",
      " [113.82324 ]\n",
      " [113.82362 ]\n",
      " [113.817085]\n",
      " [113.82256 ]]\n",
      "6 samples been transformed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/statsmodels/tsa/statespace/representation.py:374: FutureWarning: Unknown keyword arguments: dict_keys(['start_params']).Passing unknown keyword arguments will raise a TypeError beginning in version 0.15.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Statistical new, TEST!!! CHECK!!!\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "def compute_loss(max_bound, min_bound, pred):\n",
    "    mse_loss_ = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "    dist_max = mse_loss_(max_bound, pred)\n",
    "    dist_min = mse_loss_(min_bound, pred)\n",
    "    loss = dist_max + dist_min\n",
    "    return loss\n",
    "\n",
    "def compute_gradient_finite_difference(model, X_e, max_bound, min_bound, epsilon=1e-4):\n",
    "    # Convert to numpy for SARIMAX\n",
    "    temp = X_e.numpy()\n",
    "    \n",
    "    # Get the original prediction\n",
    "    pred = model.forecast(horizon, start_params=start_params, exog=temp)\n",
    "    pred = tf.convert_to_tensor(pred, dtype=tf.float32)\n",
    "    \n",
    "    # Original loss\n",
    "    original_loss = compute_loss(max_bound, min_bound, pred)\n",
    "\n",
    "    # Approximate the gradient using finite differences\n",
    "    gradients = np.zeros_like(X_e.numpy())\n",
    "    for i in range(X_e.shape[0]):\n",
    "        X_e_perturbed = X_e.numpy()\n",
    "        X_e_perturbed[i] += epsilon\n",
    "        pred_perturbed = model.forecast(horizon, start_params=start_params, exog=X_e_perturbed)\n",
    "        pred_perturbed = tf.convert_to_tensor(pred_perturbed, dtype=tf.float32)\n",
    "        \n",
    "        # Compute the perturbed loss\n",
    "        perturbed_loss = compute_loss(max_bound, min_bound, pred_perturbed)\n",
    "        \n",
    "        # Finite difference (gradient approximation)\n",
    "        gradients[i] = (perturbed_loss.numpy() - original_loss.numpy()) / epsilon\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "\n",
    "#all samples\n",
    "print(\"TEST\", X_test.shape)\n",
    "targets_sarimax = np.empty(X_test.shape)\n",
    "exogs_sarimax = np.empty(X_test.shape)\n",
    "losses_sarimax = np.empty(X_test.shape[0])\n",
    "for i in range(X_test.shape[0]):\n",
    "    print(f\"{i} samples been transformed.\")\n",
    "    max_bound = (\n",
    "        desired_max_lst[i] if desired_max_lst != None else self.MISSING_MAX_BOUND\n",
    "    )\n",
    "    min_bound = (\n",
    "        desired_min_lst[i] if desired_min_lst != None else self.MISSING_MIN_BOUND\n",
    "    )\n",
    "    X = X_test[i]\n",
    "    y = Y_test[i]\n",
    "    X_test_exog = dataset.X_test_exog[i]\n",
    "#X = random.choice(dataset.X_train_exog)\n",
    "#y = random.choice(dataset.X_train_target)\n",
    "#X_test_exog = random.choice(dataset.X_test_exog)\n",
    "    \n",
    "    \n",
    "    mod = sm.tsa.SARIMAX(endog=np.asarray(y), exog=np.asarray(X), order=(1,0,0))\n",
    "    #res = mod.fit(disp=False)\n",
    "    mod = mod.fit(disp=False)\n",
    "    start_params = mod.params\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "    \n",
    "    \n",
    "    # Initialize the input tensor\n",
    "    grad_X_e = tf.convert_to_tensor(X_test_exog, dtype=tf.float32)\n",
    "    grad_X_e = tf.Variable(grad_X_e, dtype=tf.float32)\n",
    "    \n",
    "    # Initialize prediction and loss\n",
    "    \n",
    "    temp = grad_X_e.numpy()\n",
    "    pred = mod.forecast(horizon, start_params=start_params, exog=temp)\n",
    "    pred = tf.convert_to_tensor(pred, dtype=tf.float32)\n",
    "    \n",
    "    mean_smape, mean_rmse = forecast_metrics_single(dataset.X_test_target[i], pred_sarimax)\n",
    "    print(\n",
    "        f\"model trained, with test sMAPE score {mean_smape:0.4f}; test RMSE score: {mean_rmse:0.4f}.\"\n",
    "    )\n",
    "    # Perform optimization\n",
    "    max_iter = 100\n",
    "    it = 0\n",
    "    while (tf.reduce_any(pred > max_bound) or tf.reduce_any(pred < min_bound)) and (it < max_iter):\n",
    "        #print(\"Iteration:\", it)\n",
    "        \n",
    "        # Compute approximate gradients using finite differences\n",
    "        gradients = compute_gradient_finite_difference(mod, grad_X_e, max_bound, min_bound)\n",
    "        \n",
    "        # Apply gradients using optimizer\n",
    "        optimizer.apply_gradients([(tf.convert_to_tensor(gradients, dtype=tf.float32), grad_X_e)])\n",
    "    \n",
    "        # Update predictions and loss\n",
    "        temp = grad_X_e.numpy()\n",
    "        pred = mod.forecast(horizon, start_params=start_params, exog=temp)\n",
    "        pred = tf.convert_to_tensor(pred, dtype=tf.float32)\n",
    "        loss = compute_loss(max_bound, min_bound, pred)\n",
    "        \n",
    "        #print(f\"Iteration {it}, Loss: {loss.numpy()}, Grad_X_e: {grad_X_e.numpy()}\")\n",
    "        it += 1\n",
    "    \n",
    "    print(\"Optimized value of X:\", grad_X_e.numpy(), \"Best prediction:\", pred.numpy())\n",
    "    \n",
    "    # Final inversion step if needed\n",
    "    scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "    Y_preds_original_sarimax = scaler.inverse_transform(pred.numpy().reshape(-1, 1))  # Ensure correct shape\n",
    "    print(\"Original Prediction after scaling inversion:\", Y_preds_original_sarimax)\n",
    "\n",
    "    targets_sarimax[i] = Y_preds_original_sarimax\n",
    "    exogs_sarimax[i] = grad_X_e.numpy()\n",
    "    losses_sarimax[i] = loss\n",
    "\n",
    "\n",
    "print(\"results\", targets_sarimax, exogs_sarimax, losses_sarimax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfed84ae-223f-4e71-a4ff-03e7cfbee570",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test [[ 65.53191489]\n",
      " [ 63.4893617 ]\n",
      " [ 66.55319149]\n",
      " [ 71.65957447]\n",
      " [ 78.80851064]\n",
      " [ 91.06382979]\n",
      " [100.25531915]\n",
      " [111.4893617 ]\n",
      " [122.72340426]\n",
      " [132.93617021]\n",
      " [139.06382979]\n",
      " [142.12765957]\n",
      " [142.12765957]\n",
      " [145.19148936]\n",
      " [144.17021277]\n",
      " [138.04255319]\n",
      " [126.80851064]\n",
      " [129.87234043]\n",
      " [128.85106383]\n",
      " [129.87234043]\n",
      " [130.89361702]\n",
      " [130.89361702]\n",
      " [128.85106383]\n",
      " [125.78723404]] pred [[ 98.27366 ]\n",
      " [ 98.27366 ]\n",
      " [ 98.27366 ]\n",
      " [ 80.462494]\n",
      " [ 98.27366 ]\n",
      " [ 98.27366 ]\n",
      " [ 98.27366 ]\n",
      " [ 98.27366 ]\n",
      " [156.5473  ]\n",
      " [156.5473  ]\n",
      " [138.41231 ]\n",
      " [138.41231 ]\n",
      " [138.41231 ]\n",
      " [138.41231 ]\n",
      " [138.41231 ]\n",
      " [138.41231 ]\n",
      " [138.41231 ]\n",
      " [ 98.27366 ]\n",
      " [ 98.27366 ]\n",
      " [ 98.27366 ]\n",
      " [ 98.27366 ]\n",
      " [ 98.27366 ]\n",
      " [ 98.27366 ]\n",
      " [ 98.27366 ]]\n",
      "model trained, with test sMAPE score 18.0070; test RMSE score: 22.9770.\n",
      "Iteration: 0\n",
      "Iteration 0, Loss: 0.1318691521883011, Grad_X_e: [[0.50237495 0.04999399 0.04999399 0.04999399]\n",
      " [0.50237477 0.0499938  0.0499938  0.0499938 ]\n",
      " [0.5023745  0.04999353 0.04999353 0.04999353]\n",
      " [0.50237477 0.53244996 0.04999384 0.04999384]\n",
      " [0.50237393 0.049993   0.049993   0.049993  ]\n",
      " [0.5023736  0.04999264 0.04999264 0.04999264]\n",
      " [0.5023733  0.04999235 0.04999235 0.04999235]\n",
      " [0.5023729  0.04999198 0.04999198 0.04999198]\n",
      " [0.954748   0.04998606 0.04998606 0.04998606]\n",
      " [0.9547464  0.04998451 0.04998451 0.04998451]\n",
      " [0.95474815 0.5412143  0.04998624 0.04998624]\n",
      " [0.954747   0.5412132  0.04998513 0.04998513]\n",
      " [0.95474523 0.5412114  0.04998335 0.04998335]\n",
      " [0.95474327 0.54120946 0.04998138 0.04998138]\n",
      " [0.95474076 0.54120696 0.04997887 0.04997887]\n",
      " [0.954738   0.54120415 0.0499761  0.0499761 ]\n",
      " [0.95473367 0.5411998  0.04997175 0.04997175]\n",
      " [0.5023646  0.04998359 0.04998359 0.04998359]\n",
      " [0.50236267 0.04998169 0.04998169 0.04998169]\n",
      " [0.5023606  0.04997965 0.04997965 0.04997965]\n",
      " [0.50235754 0.04997661 0.04997661 0.04997661]\n",
      " [0.5023534  0.04997247 0.04997247 0.04997247]\n",
      " [0.5023464  0.04996546 0.04996546 0.04996546]\n",
      " [0.50233454 0.04995358 0.04995358 0.04995358]]\n",
      "Iteration: 1\n",
      "Iteration 1, Loss: 0.1318691521883011, Grad_X_e: [[0.5523305  0.09994952 0.09994952 0.09994952]\n",
      " [0.55232865 0.09994769 0.09994769 0.09994769]\n",
      " [0.55234486 0.09996395 0.09996395 0.09996395]\n",
      " [0.55233806 0.58241326 0.09995712 0.09995712]\n",
      " [0.55232066 0.0999397  0.0999397  0.0999397 ]\n",
      " [0.5523394  0.09995845 0.09995845 0.09995845]\n",
      " [0.5523141  0.09993318 0.09993318 0.09993318]\n",
      " [0.55232316 0.09994222 0.09994222 0.09994222]\n",
      " [1.0046518  0.09988984 0.09988984 0.09988984]\n",
      " [1.0046366  0.09987476 0.09987476 0.09987476]\n",
      " [1.0046781  0.5911443  0.09991626 0.09991626]\n",
      " [1.0046121  0.59107834 0.09985024 0.09985024]\n",
      " [1.0046574  0.59112364 0.09989555 0.09989555]\n",
      " [1.0046046  0.5910707  0.09984263 0.09984263]\n",
      " [1.0045244  0.5909906  0.0997625  0.0997625 ]\n",
      " [1.0044806  0.5909468  0.09971872 0.09971872]\n",
      " [1.0044909  0.590957   0.09972896 0.09972896]\n",
      " [0.5522466  0.09986559 0.09986559 0.09986559]\n",
      " [0.55226415 0.09988318 0.09988318 0.09988318]\n",
      " [0.5522485  0.09986754 0.09986754 0.09986754]\n",
      " [0.55217016 0.0997892  0.0997892  0.0997892 ]\n",
      " [0.55203754 0.0996566  0.0996566  0.0996566 ]\n",
      " [0.5521237  0.09974279 0.09974279 0.09974279]\n",
      " [0.55199677 0.09961582 0.09961582 0.09961582]]\n",
      "Iteration: 2\n",
      "Iteration 2, Loss: 0.1318691521883011, Grad_X_e: [[0.602219   0.14983805 0.14983805 0.14983805]\n",
      " [0.60222566 0.14984472 0.14984472 0.14984472]\n",
      " [0.6022492  0.1498683  0.1498683  0.1498683 ]\n",
      " [0.6022356  0.6323108  0.14985466 0.14985466]\n",
      " [0.6022013  0.14982031 0.14982031 0.14982031]\n",
      " [0.60222787 0.14984691 0.14984691 0.14984691]\n",
      " [0.60218126 0.14980036 0.14980036 0.14980036]\n",
      " [0.6021824  0.14980143 0.14980143 0.14980143]\n",
      " [1.0543687  0.1496068  0.1496068  0.1496068 ]\n",
      " [1.0543108  0.14954886 0.14954886 0.14954886]\n",
      " [1.0544369  0.6409031  0.14967507 0.14967507]\n",
      " [1.0543443  0.6408105  0.14958242 0.14958242]\n",
      " [1.054348   0.6408142  0.1495861  0.1495861 ]\n",
      " [1.0541848  0.6406509  0.14942282 0.14942282]\n",
      " [1.0541574  0.6406236  0.1493955  0.1493955 ]\n",
      " [1.0539628  0.640429   0.14920096 0.14920096]\n",
      " [1.0537142  0.6401803  0.14895225 0.14895225]\n",
      " [0.6019436  0.14956264 0.14956264 0.14956264]\n",
      " [0.6019682  0.14958727 0.14958727 0.14958727]\n",
      " [0.60184234 0.14946139 0.14946139 0.14946139]\n",
      " [0.6015871  0.14920613 0.14920613 0.14920613]\n",
      " [0.6012832  0.14890222 0.14890222 0.14890222]\n",
      " [0.6012344  0.14885344 0.14885344 0.14885344]\n",
      " [0.6010788  0.14869785 0.14869785 0.14869785]]\n",
      "Iteration: 3\n",
      "Iteration 3, Loss: 0.1318691521883011, Grad_X_e: [[0.6520386  0.19965763 0.19965763 0.19965763]\n",
      " [0.65202844 0.19964752 0.19964752 0.19964752]\n",
      " [0.65203583 0.1996549  0.1996549  0.1996549 ]\n",
      " [0.6520495  0.6821247  0.19966856 0.19966856]\n",
      " [0.651972   0.19959104 0.19959104 0.19959104]\n",
      " [0.65201473 0.1996338  0.1996338  0.1996338 ]\n",
      " [0.6519256  0.1995447  0.1995447  0.1995447 ]\n",
      " [0.65188307 0.19950208 0.19950208 0.19950208]\n",
      " [1.1037964  0.19903442 0.19903442 0.19903442]\n",
      " [1.1037031  0.19894126 0.19894126 0.19894126]\n",
      " [1.1039667  0.69043285 0.19920483 0.19920483]\n",
      " [1.1037487  0.69021493 0.19898684 0.19898684]\n",
      " [1.1036141  0.6900802  0.19885215 0.19885215]\n",
      " [1.1034658  0.689932   0.19870386 0.19870386]\n",
      " [1.1033717  0.689838   0.19860986 0.19860986]\n",
      " [1.1027552  0.68922144 0.19799337 0.19799337]\n",
      " [1.1020365  0.6885026  0.19727457 0.19727457]\n",
      " [0.6513638  0.1989828  0.1989828  0.1989828 ]\n",
      " [0.65129495 0.198914   0.198914   0.198914  ]\n",
      " [0.6508573  0.1984764  0.1984764  0.1984764 ]\n",
      " [0.65035653 0.19797558 0.19797558 0.19797558]\n",
      " [0.649709   0.197328   0.197328   0.197328  ]\n",
      " [0.6493615  0.19698057 0.19698057 0.19698057]\n",
      " [0.64815944 0.19577852 0.19577852 0.19577852]]\n",
      "Iteration: 4\n",
      "Iteration 4, Loss: 0.1318691521883011, Grad_X_e: [[0.7017637  0.24938276 0.24938276 0.24938276]\n",
      " [0.7017194  0.24933846 0.24933846 0.24933846]\n",
      " [0.7017107  0.24932976 0.24932976 0.24932976]\n",
      " [0.70177406 0.73184925 0.24939317 0.24939317]\n",
      " [0.7016308  0.24924979 0.24924979 0.24924979]\n",
      " [0.701692   0.24931103 0.24931103 0.24931103]\n",
      " [0.7015447  0.24916375 0.24916375 0.24916375]\n",
      " [0.70143396 0.24905296 0.24905296 0.24905296]\n",
      " [1.1529813  0.24821928 0.24821928 0.24821928]\n",
      " [1.1527882  0.24802625 0.24802625 0.24802625]\n",
      " [1.1532453  0.73971146 0.24848346 0.24848346]\n",
      " [1.1528242  0.7392905  0.24806236 0.24806236]\n",
      " [1.1525998  0.73906595 0.2478379  0.2478379 ]\n",
      " [1.1523082  0.73877436 0.24754626 0.24754626]\n",
      " [1.1521109  0.7385771  0.247349   0.247349  ]\n",
      " [1.1508291  0.73729527 0.2460672  0.2460672 ]\n",
      " [1.1495686  0.7360347  0.24480668 0.24480668]\n",
      " [0.7004062  0.24802521 0.24802521 0.24802521]\n",
      " [0.7002187  0.24783775 0.24783775 0.24783775]\n",
      " [0.6993965  0.24701554 0.24701554 0.24701554]\n",
      " [0.6985695  0.24618852 0.24618852 0.24618852]\n",
      " [0.6972373  0.24485634 0.24485634 0.24485634]\n",
      " [0.6963165  0.24393556 0.24393556 0.24393556]\n",
      " [0.6928633  0.24048233 0.24048233 0.24048233]]\n",
      "Iteration: 5\n",
      "Iteration 5, Loss: 0.1318691521883011, Grad_X_e: [[0.7513694  0.29898846 0.29898846 0.29898846]\n",
      " [0.75125957 0.2988786  0.2988786  0.2988786 ]\n",
      " [0.75121236 0.29883143 0.29883143 0.29883143]\n",
      " [0.7513643  0.7814395  0.29898342 0.29898342]\n",
      " [0.7511458  0.29876482 0.29876482 0.29876482]\n",
      " [0.7511809  0.29879993 0.29879993 0.29879993]\n",
      " [0.7509797  0.29859874 0.29859874 0.29859874]\n",
      " [0.75080156 0.29842055 0.29842055 0.29842055]\n",
      " [1.2017093  0.2969473  0.2969473  0.2969473 ]\n",
      " [1.2013859  0.29662395 0.29662395 0.29662395]\n",
      " [1.2020732  0.7885394  0.2973114  0.2973114 ]\n",
      " [1.2014096  0.78787583 0.29664773 0.29664773]\n",
      " [1.2010137  0.7874798  0.29625177 0.29625177]\n",
      " [1.2004708  0.7869369  0.2957088  0.2957088 ]\n",
      " [1.1999491  0.78641534 0.29518718 0.29518718]\n",
      " [1.197613   0.78407925 0.29285118 0.29285118]\n",
      " [1.1954786  0.78194475 0.2907167  0.2907167 ]\n",
      " [0.74895847 0.2965775  0.2965775  0.2965775 ]\n",
      " [0.7484198  0.29603887 0.29603887 0.29603887]\n",
      " [0.7469335  0.29455256 0.29455256 0.29455256]\n",
      " [0.7456711  0.29329014 0.29329014 0.29329014]\n",
      " [0.7434977  0.29111677 0.29111677 0.29111677]\n",
      " [0.74088126 0.28850034 0.28850034 0.28850034]\n",
      " [0.733356   0.280975   0.280975   0.280975  ]]\n",
      "Iteration: 6\n",
      "Iteration 6, Loss: 0.1318691521883011, Grad_X_e: [[0.8008302  0.34844923 0.34844923 0.34844923]\n",
      " [0.80064774 0.34826678 0.34826678 0.34826678]\n",
      " [0.80056304 0.34818208 0.34818208 0.34818208]\n",
      " [0.80079705 0.83087224 0.34841618 0.34841618]\n",
      " [0.8004635  0.34808257 0.34808257 0.34808257]\n",
      " [0.80048007 0.3480991  0.3480991  0.3480991 ]\n",
      " [0.80022436 0.34784338 0.34784338 0.34784338]\n",
      " [0.79994935 0.3475683  0.3475683  0.3475683 ]\n",
      " [1.2499013  0.34513932 0.34513932 0.34513932]\n",
      " [1.2493893  0.34462732 0.34462732 0.34462732]\n",
      " [1.2504374  0.83690363 0.34567562 0.34567562]\n",
      " [1.2494792  0.8359454  0.34471732 0.34471732]\n",
      " [1.2488205  0.8352866  0.34405857 0.34405857]\n",
      " [1.2478012  0.83426726 0.34303918 0.34303918]\n",
      " [1.2468257  0.8332919  0.3420637  0.3420637 ]\n",
      " [1.243189   0.82965523 0.33842713 0.33842713]\n",
      " [1.2395735  0.8260396  0.3348116  0.3348116 ]\n",
      " [0.7968952  0.34451422 0.34451422 0.34451422]\n",
      " [0.7959614  0.34358042 0.34358042 0.34358042]\n",
      " [0.7935603  0.3411794  0.3411794  0.3411794 ]\n",
      " [0.7915634  0.33918244 0.33918244 0.33918244]\n",
      " [0.7880477  0.33566675 0.33566675 0.33566675]\n",
      " [0.7830596  0.33067864 0.33067864 0.33067864]\n",
      " [0.76938057 0.31699955 0.31699955 0.31699955]]\n",
      "Iteration: 7\n",
      "Iteration 7, Loss: 0.1318691521883011, Grad_X_e: [[0.85012066 0.3977397  0.3977397  0.3977397 ]\n",
      " [0.84983826 0.3974573  0.3974573  0.3974573 ]\n",
      " [0.84973234 0.39735138 0.39735138 0.39735138]\n",
      " [0.8500667  0.8801419  0.39768583 0.39768583]\n",
      " [0.84957933 0.3971984  0.3971984  0.3971984 ]\n",
      " [0.84953225 0.3971513  0.3971513  0.3971513 ]\n",
      " [0.84924    0.396859   0.396859   0.396859  ]\n",
      " [0.84883916 0.39645815 0.39645815 0.39645815]\n",
      " [1.2974635  0.39270157 0.39270157 0.39270157]\n",
      " [1.296755   0.391993   0.391993   0.391993  ]\n",
      " [1.2981771  0.8846434  0.39341536 0.39341536]\n",
      " [1.2969116  0.8833778  0.39214972 0.39214972]\n",
      " [1.2958707  0.88233674 0.39110866 0.39110866]\n",
      " [1.2942299  0.88069594 0.38946784 0.38946784]\n",
      " [1.2923512  0.87881744 0.38758928 0.38758928]\n",
      " [1.2871835  0.87364984 0.38242173 0.38242173]\n",
      " [1.2815596  0.86802566 0.37679768 0.37679768]\n",
      " [0.8440772  0.39169627 0.39169627 0.39169627]\n",
      " [0.84244716 0.39006624 0.39006624 0.39006624]\n",
      " [0.8390366  0.38665563 0.38665563 0.38665563]\n",
      " [0.8360853  0.38370436 0.38370436 0.38370436]\n",
      " [0.8303581  0.37797713 0.37797713 0.37797713]\n",
      " [0.82193816 0.3695572  0.3695572  0.3695572 ]\n",
      " [0.7997732  0.3473922  0.3473922  0.3473922 ]]\n",
      "Iteration: 8\n",
      "Iteration 8, Loss: 0.1318691521883011, Grad_X_e: [[0.89921486 0.44683388 0.44683388 0.44683388]\n",
      " [0.8988078  0.4464269  0.4464269  0.4464269 ]\n",
      " [0.89868987 0.44630888 0.44630888 0.44630888]\n",
      " [0.89912415 0.92919934 0.44674325 0.44674325]\n",
      " [0.8984588  0.44607785 0.44607785 0.44607785]\n",
      " [0.89830756 0.4459266  0.4459266  0.4459266 ]\n",
      " [0.89798737 0.44560632 0.44560632 0.44560632]\n",
      " [0.8974309  0.44504988 0.44504988 0.44504988]\n",
      " [1.34436    0.439598   0.439598   0.439598  ]\n",
      " [1.3433363  0.43857437 0.43857437 0.43857437]\n",
      " [1.3452634  0.93172956 0.44050157 0.44050157]\n",
      " [1.3435751  0.9300414  0.43881327 0.43881327]\n",
      " [1.3419998  0.92846584 0.4372378  0.4372378 ]\n",
      " [1.339538   0.926004   0.43477592 0.43477592]\n",
      " [1.3365581  0.9230243  0.4317961  0.4317961 ]\n",
      " [1.3291676  0.9156339  0.4244058  0.4244058 ]\n",
      " [1.320765   0.9072311  0.41600314 0.41600314]\n",
      " [0.89034975 0.4379688  0.4379688  0.4379688 ]\n",
      " [0.8879234  0.4355425  0.4355425  0.4355425 ]\n",
      " [0.88308924 0.4307083  0.4307083  0.4307083 ]\n",
      " [0.8788347  0.42645374 0.42645374 0.42645374]\n",
      " [0.87008923 0.4177083  0.4177083  0.4177083 ]\n",
      " [0.85688853 0.40450758 0.40450758 0.40450758]\n",
      " [0.82401234 0.37163135 0.37163135 0.37163135]]\n",
      "Iteration: 9\n",
      "Iteration 9, Loss: 0.1318691521883011, Grad_X_e: [[0.9480667  0.49568573 0.49568573 0.49568573]\n",
      " [0.9475523  0.4951714  0.4951714  0.4951714 ]\n",
      " [0.9473831  0.49500215 0.49500215 0.49500215]\n",
      " [0.947944   0.9780192  0.4955631  0.4955631 ]\n",
      " [0.9470425  0.4946616  0.4946616  0.4946616 ]\n",
      " [0.9467736  0.49439263 0.49439263 0.49439263]\n",
      " [0.94639856 0.49401748 0.49401748 0.49401748]\n",
      " [0.9457128  0.49333182 0.49333182 0.49333182]\n",
      " [1.3904628  0.48570073 0.48570073 0.48570073]\n",
      " [1.3888855  0.48412356 0.48412356 0.48412356]\n",
      " [1.3915781  0.9780442  0.48681626 0.48681626]\n",
      " [1.3893267  0.9757929  0.4845648  0.4845648 ]\n",
      " [1.387027   0.9734931  0.48226506 0.48226506]\n",
      " [1.383483   0.96994907 0.47872096 0.47872096]\n",
      " [1.3789421  0.9654084  0.47418016 0.47418016]\n",
      " [1.3688775  0.9553439  0.4641158  0.4641158 ]\n",
      " [1.356424   0.94289    0.45166206 0.45166206]\n",
      " [0.935541   0.48316005 0.48316005 0.48316005]\n",
      " [0.9320359  0.479655   0.479655   0.479655  ]\n",
      " [0.92556715 0.47318622 0.47318622 0.47318622]\n",
      " [0.9193575  0.46697652 0.46697652 0.46697652]\n",
      " [0.9065138  0.45413288 0.45413288 0.45413288]\n",
      " [0.8867216  0.4343407  0.4343407  0.4343407 ]\n",
      " [0.84086376 0.3884828  0.3884828  0.3884828 ]]\n",
      "Iteration: 10\n",
      "Iteration 10, Loss: 0.1318691521883011, Grad_X_e: [[0.99667317 0.5442922  0.5442922  0.5442922 ]\n",
      " [0.99603075 0.5436498  0.5436498  0.5436498 ]\n",
      " [0.9957848  0.54340386 0.54340386 0.54340386]\n",
      " [0.996489   1.0265641  0.5441081  0.5441081 ]\n",
      " [0.99529916 0.54291826 0.54291826 0.54291826]\n",
      " [0.9949093  0.5425283  0.5425283  0.5425283 ]\n",
      " [0.9944371  0.542056   0.542056   0.542056  ]\n",
      " [0.99362075 0.54123974 0.54123974 0.54123974]\n",
      " [1.4355943  0.53083223 0.53083223 0.53083223]\n",
      " [1.4333358  0.5285738  0.5285738  0.5285738 ]\n",
      " [1.4369938  1.0234599  0.532232   0.532232  ]\n",
      " [1.4340547  1.0205209  0.5292929  0.5292929 ]\n",
      " [1.4306971  1.0171632  0.5259351  0.5259351 ]\n",
      " [1.4258646  1.0123307  0.52110255 0.52110255]\n",
      " [1.4191344  1.0056006  0.5143724  0.5143724 ]\n",
      " [1.4059947  0.992461   0.5012329  0.5012329 ]\n",
      " [1.3880379  0.974504   0.48327604 0.48327604]\n",
      " [0.9795137  0.52713275 0.52713275 0.52713275]\n",
      " [0.97453785 0.52215695 0.52215695 0.52215695]\n",
      " [0.96611416 0.51373327 0.51373327 0.51373327]\n",
      " [0.9571423  0.50476134 0.50476134 0.50476134]\n",
      " [0.93932146 0.48694056 0.48694056 0.48694056]\n",
      " [0.9111781  0.45879716 0.45879716 0.45879716]\n",
      " [0.8510588  0.39867786 0.39867786 0.39867786]]\n",
      "Iteration: 11\n",
      "Iteration 11, Loss: 0.1318691521883011, Grad_X_e: [[1.0449848  0.59260386 0.59260386 0.59260386]\n",
      " [1.0442033  0.59182227 0.59182227 0.59182227]\n",
      " [1.0438662  0.59148526 0.59148526 0.59148526]\n",
      " [1.0447222  1.0747974  0.5923413  0.5923413 ]\n",
      " [1.0431955  0.59081465 0.59081465 0.59081465]\n",
      " [1.0426617  0.59028065 0.59028065 0.59028065]\n",
      " [1.0420641  0.589683   0.589683   0.589683  ]\n",
      " [1.0410917  0.5887106  0.5887106  0.5887106 ]\n",
      " [1.4796101  0.574848   0.574848   0.574848  ]\n",
      " [1.4764981  0.57173616 0.57173616 0.57173616]\n",
      " [1.4813728  1.0678389  0.57661104 0.57661104]\n",
      " [1.477537   1.0640032  0.5727752  0.5727752 ]\n",
      " [1.4729695  1.0594356  0.56820756 0.56820756]\n",
      " [1.466454   1.0529201  0.561692   0.561692  ]\n",
      " [1.456915   1.0433812  0.55215305 0.55215305]\n",
      " [1.4398954  1.0263617  0.5351336  0.5351336 ]\n",
      " [1.4150617  1.0015278  0.51029986 0.51029986]\n",
      " [1.0221106  0.5697296  0.5697296  0.5697296 ]\n",
      " [1.0152949  0.56291395 0.56291395 0.56291395]\n",
      " [1.0045187  0.55213785 0.55213785 0.55213785]\n",
      " [0.99186903 0.5394881  0.5394881  0.5394881 ]\n",
      " [0.96814907 0.5157682  0.5157682  0.5157682 ]\n",
      " [0.92998236 0.4776014  0.4776014  0.4776014 ]\n",
      " [0.85458696 0.40220603 0.40220603 0.40220603]]\n",
      "Iteration: 12\n",
      "Iteration 12, Loss: 0.1318691521883011, Grad_X_e: [[1.0929763  0.6405953  0.6405953  0.6405953 ]\n",
      " [1.0920422  0.63966113 0.63966113 0.63966113]\n",
      " [1.0915972  0.6392163  0.6392163  0.6392163 ]\n",
      " [1.0926183  1.1226935  0.64023745 0.64023745]\n",
      " [1.0906968  0.638316   0.638316   0.638316  ]\n",
      " [1.089993   0.63761204 0.63761204 0.63761204]\n",
      " [1.0892386  0.6368575  0.6368575  0.6368575 ]\n",
      " [1.0880625  0.63568145 0.63568145 0.63568145]\n",
      " [1.5223979  0.61763585 0.61763585 0.61763585]\n",
      " [1.5181677  0.6134057  0.6134057  0.6134057 ]\n",
      " [1.5245665  1.1110326  0.61980474 0.61980474]\n",
      " [1.5196903  1.1061565  0.6149284  0.6149284 ]\n",
      " [1.5135949  1.1000609  0.60883284 0.60883284]\n",
      " [1.5049168  1.0913829  0.60015476 0.60015476]\n",
      " [1.4920259  1.0784919  0.5872638  0.5872638 ]\n",
      " [1.4701868  1.0566531  0.565425   0.565425  ]\n",
      " [1.4374886  1.0239546  0.5327267  0.5327267 ]\n",
      " [1.0630953  0.61071444 0.61071444 0.61071444]\n",
      " [1.0540699  0.601689   0.601689   0.601689  ]\n",
      " [1.0404407  0.5880598  0.5880598  0.5880598 ]\n",
      " [1.0231752  0.5707943  0.5707943  0.5707943 ]\n",
      " [0.9924278  0.54004693 0.54004693 0.54004693]\n",
      " [0.9428886  0.49050763 0.49050763 0.49050763]\n",
      " [0.85212183 0.3997409  0.3997409  0.3997409 ]]\n",
      "Iteration: 13\n",
      "Iteration 13, Loss: 0.1318691521883011, Grad_X_e: [[1.1406211  0.68824005 0.68824005 0.68824005]\n",
      " [1.1395305  0.6871495  0.6871495  0.6871495 ]\n",
      " [1.1389465  0.6865657  0.6865657  0.6865657 ]\n",
      " [1.1401504  1.1702256  0.68776953 0.68776953]\n",
      " [1.1377667  0.6853859  0.6853859  0.6853859 ]\n",
      " [1.1368638  0.6844829  0.6844829  0.6844829 ]\n",
      " [1.1359178  0.6835366  0.6835366  0.6835366 ]\n",
      " [1.1345053  0.6821242  0.6821242  0.6821242 ]\n",
      " [1.5638785  0.65911657 0.65911657 0.65911657]\n",
      " [1.5582372  0.65347517 0.65347517 0.65347517]\n",
      " [1.5665044  1.1529704  0.6617426  0.6617426 ]\n",
      " [1.5603069  1.1467731  0.6555451  0.6555451 ]\n",
      " [1.5523696  1.1388357  0.6476075  0.6476075 ]\n",
      " [1.5410571  1.1275232  0.6362951  0.6362951 ]\n",
      " [1.5241764  1.1106424  0.6194144  0.6194144 ]\n",
      " [1.49673    1.0831963  0.5919681  0.5919681 ]\n",
      " [1.4552841  1.0417502  0.5505223  0.5505223 ]\n",
      " [1.1023391  0.6499583  0.6499583  0.6499583 ]\n",
      " [1.0906004  0.6382194  0.6382194  0.6382194 ]\n",
      " [1.0737102  0.6213293  0.6213293  0.6213293 ]\n",
      " [1.050935   0.5985541  0.5985541  0.5985541 ]\n",
      " [1.0119556  0.55957466 0.55957466 0.55957466]\n",
      " [0.95016354 0.4977826  0.4977826  0.4977826 ]\n",
      " [0.84478104 0.39240012 0.39240012 0.39240012]]\n",
      "Iteration: 14\n",
      "Iteration 14, Loss: 0.1318691521883011, Grad_X_e: [[1.1878916  0.73551065 0.73551065 0.73551065]\n",
      " [1.186637   0.7342561  0.7342561  0.7342561 ]\n",
      " [1.1858951  0.73351425 0.73351425 0.73351425]\n",
      " [1.1873025  1.2173777  0.73492163 0.73492163]\n",
      " [1.1843824  0.73200154 0.73200154 0.73200154]\n",
      " [1.183249   0.730868   0.730868   0.730868  ]\n",
      " [1.1820737  0.7296925  0.7296925  0.7296925 ]\n",
      " [1.180388   0.72800684 0.72800684 0.72800684]\n",
      " [1.6038597  0.69909775 0.69909775 0.69909775]\n",
      " [1.5965211  0.6917591  0.6917591  0.6917591 ]\n",
      " [1.6070551  1.1935211  0.7022934  0.7022934 ]\n",
      " [1.5992216  1.1856878  0.69445974 0.69445974]\n",
      " [1.5891396  1.1756057  0.68437743 0.68437743]\n",
      " [1.5747405  1.1612066  0.6699785  0.6699785 ]\n",
      " [1.553162   1.139628   0.64840007 0.64840007]\n",
      " [1.5193628  1.1058291  0.61460096 0.61460096]\n",
      " [1.4684051  1.0548712  0.56364334 0.56364334]\n",
      " [1.1396276  0.68724674 0.68724674 0.68724674]\n",
      " [1.1246831  0.67230225 0.67230225 0.67230225]\n",
      " [1.1039233  0.65154237 0.65154237 0.65154237]\n",
      " [1.0748612  0.62248015 0.62248015 0.62248015]\n",
      " [1.0267134  0.57433236 0.57433236 0.57433236]\n",
      " [0.9522707  0.4998897  0.4998897  0.4998897 ]\n",
      " [0.833461   0.38108006 0.38108006 0.38108006]]\n",
      "Iteration: 15\n",
      "Iteration 15, Loss: 0.1318691521883011, Grad_X_e: [[1.2347596  0.7823787  0.7823787  0.7823787 ]\n",
      " [1.233317   0.78093606 0.78093606 0.78093606]\n",
      " [1.2323945  0.7800136  0.7800136  0.7800136 ]\n",
      " [1.2340313  1.2641065  0.7816505  0.7816505 ]\n",
      " [1.2305031  0.77812225 0.77812225 0.77812225]\n",
      " [1.2290876  0.7767066  0.7767066  0.7767066 ]\n",
      " [1.2276403  0.775259   0.775259   0.775259  ]\n",
      " [1.225637   0.7732558  0.7732558  0.7732558 ]\n",
      " [1.6421895  0.73742753 0.73742753 0.73742753]\n",
      " [1.6328185  0.72805643 0.72805643 0.72805643]\n",
      " [1.6459771  1.2324432  0.7412155  0.7412155 ]\n",
      " [1.6362535  1.2227197  0.73149157 0.73149157]\n",
      " [1.6236601  1.2101262  0.71889794 0.71889794]\n",
      " [1.6056308  1.1920968  0.7008688  0.7008688 ]\n",
      " [1.5787587  1.1652248  0.6739968  0.6739968 ]\n",
      " [1.5377693  1.1242356  0.6330075  0.6330075 ]\n",
      " [1.4768142  1.0632802  0.5720524  0.5720524 ]\n",
      " [1.1747983  0.7224174  0.7224174  0.7224174 ]\n",
      " [1.1560943  0.7037135  0.7037135  0.7037135 ]\n",
      " [1.1308765  0.6784955  0.6784955  0.6784955 ]\n",
      " [1.094799   0.642418   0.642418   0.642418  ]\n",
      " [1.036856   0.5844751  0.5844751  0.5844751 ]\n",
      " [0.94982004 0.49743906 0.49743906 0.49743906]\n",
      " [0.8188961  0.36651522 0.36651522 0.36651522]]\n",
      "Iteration: 16\n",
      "Iteration 16, Loss: 0.1318691521883011, Grad_X_e: [[1.2812083  0.8288274  0.8288274  0.8288274 ]\n",
      " [1.2795525  0.8271715  0.8271715  0.8271715 ]\n",
      " [1.2784246  0.82604384 0.82604384 0.82604384]\n",
      " [1.2803199  1.3103951  0.82793903 0.82793903]\n",
      " [1.2760872  0.8237063  0.8237063  0.8237063 ]\n",
      " [1.2743529  0.8219719  0.8219719  0.8219719 ]\n",
      " [1.2725877  0.82020634 0.82020634 0.82020634]\n",
      " [1.2702181  0.81783694 0.81783694 0.81783694]\n",
      " [1.6787581  0.77399623 0.77399623 0.77399623]\n",
      " [1.6670445  0.76228255 0.76228255 0.76228255]\n",
      " [1.683226   1.2696921  0.77846444 0.77846444]\n",
      " [1.6713305  1.2577966  0.76656854 0.76656854]\n",
      " [1.6558206  1.2422867  0.75105846 0.75105846]\n",
      " [1.6335645  1.2200305  0.72880244 0.72880244]\n",
      " [1.6008478  1.1873139  0.69608593 0.69608593]\n",
      " [1.5520828  1.1385491  0.6473209  0.6473209 ]\n",
      " [1.4808362  1.0673022  0.5760744  0.5760744 ]\n",
      " [1.2076004  0.7552195  0.7552195  0.7552195 ]\n",
      " [1.1846826  0.7323018  0.7323018  0.7323018 ]\n",
      " [1.1543529  0.7019718  0.7019718  0.7019718 ]\n",
      " [1.1105919  0.6582108  0.6582108  0.6582108 ]\n",
      " [1.0425236  0.5901427  0.5901427  0.5901427 ]\n",
      " [0.94333243 0.49095148 0.49095148 0.49095148]\n",
      " [0.80219644 0.34981555 0.34981555 0.34981555]]\n",
      "Iteration: 17\n",
      "Iteration 17, Loss: 0.1318691521883011, Grad_X_e: [[1.3272063  0.8748254  0.8748254  0.8748254 ]\n",
      " [1.32531    0.872929   0.872929   0.872929  ]\n",
      " [1.3239492  0.8715684  0.8715684  0.8715684 ]\n",
      " [1.3261359  1.3562111  0.87375504 0.87375504]\n",
      " [1.321092   0.8687112  0.8687112  0.8687112 ]\n",
      " [1.3190153  0.8666343  0.8666343  0.8666343 ]\n",
      " [1.316864   0.8644827  0.8644827  0.8644827 ]\n",
      " [1.3140937  0.8617125  0.8617125  0.8617125 ]\n",
      " [1.7134434  0.80868155 0.80868155 0.80868155]\n",
      " [1.6990316  0.7942696  0.7942696  0.7942696 ]\n",
      " [1.7186896  1.3051556  0.813928   0.813928  ]\n",
      " [1.7043018  1.290768   0.7995399  0.7995399 ]\n",
      " [1.6854175  1.2718836  0.7806553  0.7806553 ]\n",
      " [1.65846    1.2449261  0.75369793 0.75369793]\n",
      " [1.6193018  1.2057679  0.7145398  0.7145398 ]\n",
      " [1.5622736  1.1487399  0.6575118  0.6575118 ]\n",
      " [1.4807593  1.0672253  0.57599753 0.57599753]\n",
      " [1.2379203  0.78553945 0.78553945 0.78553945]\n",
      " [1.2103735  0.7579926  0.7579926  0.7579926 ]\n",
      " [1.1743548  0.7219737  0.7219737  0.7219737 ]\n",
      " [1.1223673  0.66998625 0.66998625 0.66998625]\n",
      " [1.0440136  0.59163266 0.59163266 0.59163266]\n",
      " [0.93363684 0.4812559  0.4812559  0.4812559 ]\n",
      " [0.78432816 0.33194727 0.33194727 0.33194727]]\n",
      "Iteration: 18\n",
      "Iteration 18, Loss: 0.1318691521883011, Grad_X_e: [[1.3727349  0.920354   0.920354   0.920354  ]\n",
      " [1.3705554  0.91817445 0.91817445 0.91817445]\n",
      " [1.3689458  0.91656494 0.91656494 0.91656494]\n",
      " [1.3714463  1.4015214  0.9190655  0.9190655 ]\n",
      " [1.3654907  0.91310984 0.91310984 0.91310984]\n",
      " [1.3630428  0.9106618  0.9106618  0.9106618 ]\n",
      " [1.3604549  0.90807366 0.90807366 0.90807366]\n",
      " [1.3572232  0.90484196 0.90484196 0.90484196]\n",
      " [1.7461702  0.8414084  0.8414084  0.8414084 ]\n",
      " [1.7286707  0.8239087  0.8239087  0.8239087 ]\n",
      " [1.7522451  1.3387111  0.8474835  0.8474835 ]\n",
      " [1.7350713  1.3215375  0.8303094  0.8303094 ]\n",
      " [1.7123959  1.298862   0.8076337  0.8076337 ]\n",
      " [1.6802231  1.2666892  0.7754611  0.7754611 ]\n",
      " [1.6341094  1.2205755  0.7293474  0.7293474 ]\n",
      " [1.5685997  1.155066   0.6638378  0.6638378 ]\n",
      " [1.4771693  1.0636353  0.5724076  0.5724076 ]\n",
      " [1.2657079  0.8133271  0.8133271  0.8133271 ]\n",
      " [1.233079   0.7806981  0.7806981  0.7806981 ]\n",
      " [1.1908711  0.73849005 0.73849005 0.73849005]\n",
      " [1.1302369  0.6778559  0.6778559  0.6778559 ]\n",
      " [1.041915   0.5895341  0.5895341  0.5895341 ]\n",
      " [0.92144126 0.46906033 0.46906033 0.46906033]\n",
      " [0.76616657 0.31378567 0.31378567 0.31378567]]\n",
      "Iteration: 19\n",
      "Iteration 19, Loss: 0.1318691521883011, Grad_X_e: [[1.4177538  0.9653729  0.9653729  0.9653729 ]\n",
      " [1.4152614  0.96288043 0.96288043 0.96288043]\n",
      " [1.4133751  0.9609943  0.9609943  0.9609943 ]\n",
      " [1.4162246  1.4462998  0.9638438  0.9638438 ]\n",
      " [1.4092368  0.956856   0.956856   0.956856  ]\n",
      " [1.4063737  0.95399266 0.95399266 0.95399266]\n",
      " [1.4032922  0.950911   0.950911   0.950911  ]\n",
      " [1.3995306  0.94714946 0.94714946 0.94714946]\n",
      " [1.7767622  0.87200046 0.87200046 0.87200046]\n",
      " [1.7558057  0.85104376 0.85104376 0.85104376]\n",
      " [1.7837889  1.370255   0.8790274  0.8790274 ]\n",
      " [1.7634636  1.3499298  0.8587017  0.8587017 ]\n",
      " [1.736566   1.323032   0.8318037  0.8318037 ]\n",
      " [1.6987025  1.2851685  0.7939404  0.7939404 ]\n",
      " [1.6452539  1.23172    0.7404919  0.7404919 ]\n",
      " [1.5712206  1.157687   0.6664587  0.6664587 ]\n",
      " [1.4703326  1.0567987  0.56557095 0.56557095]\n",
      " [1.2907814  0.83840066 0.83840066 0.83840066]\n",
      " [1.2526073  0.80022657 0.80022657 0.80022657]\n",
      " [1.203826   0.7514449  0.7514449  0.7514449 ]\n",
      " [1.1343691  0.6819882  0.6819882  0.6819882 ]\n",
      " [1.0365012  0.58412015 0.58412015 0.58412015]\n",
      " [0.90717244 0.4547915  0.4547915  0.4547915 ]\n",
      " [0.7485292  0.2961483  0.2961483  0.2961483 ]]\n",
      "Iteration: 20\n",
      "Iteration 20, Loss: 0.1318691521883011, Grad_X_e: [[1.4622366  1.0098557  1.0098557  1.0098557 ]\n",
      " [1.4594066  1.0070256  1.0070256  1.0070256 ]\n",
      " [1.4572209  1.0048401  1.0048401  1.0048401 ]\n",
      " [1.4604504  1.4905256  1.0080696  1.0080696 ]\n",
      " [1.45231    0.9999292  0.9999292  0.9999292 ]\n",
      " [1.4489746  0.9965935  0.9965935  0.9965935 ]\n",
      " [1.4453484  0.9929672  0.9929672  0.9929672 ]\n",
      " [1.4409845  0.9886033  0.9886033  0.9886033 ]\n",
      " [1.8051604  0.9003987  0.9003987  0.9003987 ]\n",
      " [1.7803106  0.87554866 0.87554866 0.87554866]\n",
      " [1.8132381  1.3997042  0.90847665 0.90847665]\n",
      " [1.7894012  1.3758674  0.8846393  0.8846393 ]\n",
      " [1.7578586  1.3443247  0.85309637 0.85309637]\n",
      " [1.7139469  1.300413   0.8091849  0.8091849 ]\n",
      " [1.6530116  1.2394776  0.7482496  0.7482496 ]\n",
      " [1.5704877  1.156954   0.6657259  0.6657259 ]\n",
      " [1.460805   1.0472711  0.5560434  0.5560434 ]\n",
      " [1.3130753  0.86069465 0.86069465 0.86069465]\n",
      " [1.2690085  0.81662774 0.81662774 0.81662774]\n",
      " [1.213316   0.7609349  0.7609349  0.7609349 ]\n",
      " [1.1351163  0.6827353  0.6827353  0.6827353 ]\n",
      " [1.0282506  0.5758696  0.5758696  0.5758696 ]\n",
      " [0.89158857 0.43920764 0.43920764 0.43920764]\n",
      " [0.7323381  0.27995715 0.27995715 0.27995715]]\n",
      "Iteration: 21\n",
      "Iteration 21, Loss: 0.1318691521883011, Grad_X_e: [[1.5061492  1.0537683  1.0537683  1.0537683 ]\n",
      " [1.5029455  1.0505645  1.0505645  1.0505645 ]\n",
      " [1.5004246  1.0480438  1.0480438  1.0480438 ]\n",
      " [1.5040867  1.5341619  1.051706   1.051706  ]\n",
      " [1.4946597  1.0422789  1.0422789  1.0422789 ]\n",
      " [1.4908     1.0384189  1.0384189  1.0384189 ]\n",
      " [1.4865612  1.03418    1.03418    1.03418   ]\n",
      " [1.4815264  1.0291452  1.0291452  1.0291452 ]\n",
      " [1.8312325  0.9264709  0.9264709  0.9264709 ]\n",
      " [1.8021665  0.89740455 0.89740455 0.89740455]\n",
      " [1.840471   1.4269371  0.93570954 0.93570954]\n",
      " [1.8127623  1.3992285  0.9080004  0.9080004 ]\n",
      " [1.7762402  1.3627063  0.871478   0.871478  ]\n",
      " [1.7259442  1.3124102  0.82118213 0.82118213]\n",
      " [1.6573946  1.2438607  0.75263274 0.75263274]\n",
      " [1.5665773  1.1530436  0.66181546 0.66181546]\n",
      " [1.4489951  1.0354612  0.5442335  0.5442335 ]\n",
      " [1.3324752  0.8800945  0.8800945  0.8800945 ]\n",
      " [1.2822236  0.82984287 0.82984287 0.82984287]\n",
      " [1.219483   0.76710194 0.76710194 0.76710194]\n",
      " [1.1326594  0.6802784  0.6802784  0.6802784 ]\n",
      " [1.0175844  0.5652035  0.5652035  0.5652035 ]\n",
      " [0.875181   0.42280006 0.42280006 0.42280006]\n",
      " [0.71818566 0.26580474 0.26580474 0.26580474]]\n",
      "Iteration: 22\n",
      "Iteration 22, Loss: 0.1318691521883011, Grad_X_e: [[1.5494857  1.0971048  1.0971048  1.0971048 ]\n",
      " [1.5458716  1.0934906  1.0934906  1.0934906 ]\n",
      " [1.5429697  1.0905889  1.0905889  1.0905889 ]\n",
      " [1.5471193  1.5771945  1.0947385  1.0947385 ]\n",
      " [1.5362629  1.0838821  1.0838821  1.0838821 ]\n",
      " [1.5318334  1.0794523  1.0794523  1.0794523 ]\n",
      " [1.5269109  1.0745298  1.0745298  1.0745298 ]\n",
      " [1.5211438  1.0687627  1.0687627  1.0687627 ]\n",
      " [1.8549716  0.95021    0.95021    0.95021   ]\n",
      " [1.8213068  0.916545   0.916545   0.916545  ]\n",
      " [1.8654531  1.4519192  0.9606917  0.9606917 ]\n",
      " [1.8336018  1.420068   0.9288399  0.9288399 ]\n",
      " [1.7917558  1.3782219  0.8869936  0.8869936 ]\n",
      " [1.7348795  1.3213456  0.8301175  0.8301175 ]\n",
      " [1.6587067  1.2451727  0.75394475 0.75394475]\n",
      " [1.5599855  1.1464518  0.6552236  0.6552236 ]\n",
      " [1.4355849  1.022051   0.5308233  0.5308233 ]\n",
      " [1.3490309  0.8966502  0.8966502  0.8966502 ]\n",
      " [1.2924358  0.8400551  0.8400551  0.8400551 ]\n",
      " [1.2225105  0.7701294  0.7701294  0.7701294 ]\n",
      " [1.1274892  0.6751082  0.6751082  0.6751082 ]\n",
      " [1.0050322  0.5526512  0.5526512  0.5526512 ]\n",
      " [0.8585998  0.4062188  0.4062188  0.4062188 ]\n",
      " [0.7067884  0.2544075  0.2544075  0.2544075 ]]\n",
      "Iteration: 23\n",
      "Iteration 23, Loss: 0.1318691521883011, Grad_X_e: [[1.5922158  1.1398349  1.1398349  1.1398349 ]\n",
      " [1.5881596  1.1357785  1.1357785  1.1357785 ]\n",
      " [1.5848289  1.1324481  1.1324481  1.1324481 ]\n",
      " [1.5895237  1.6195989  1.1371429  1.1371429 ]\n",
      " [1.5770848  1.124704   1.124704   1.124704  ]\n",
      " [1.5720241  1.119643   1.119643   1.119643  ]\n",
      " [1.5663633  1.1139822  1.1139822  1.1139822 ]\n",
      " [1.5597833  1.1074022  1.1074022  1.1074022 ]\n",
      " [1.876328   0.9715663  0.9715663  0.9715663 ]\n",
      " [1.8377775  0.93301564 0.93301564 0.93301564]\n",
      " [1.8881104  1.4745765  0.98334897 0.98334897]\n",
      " [1.851852   1.4383183  0.94709015 0.94709015]\n",
      " [1.8044401  1.3909062  0.8996779  0.8996779 ]\n",
      " [1.7408683  1.3273344  0.83610636 0.83610636]\n",
      " [1.6571615  1.2436275  0.75239956 0.75239956]\n",
      " [1.551153   1.1376193  0.6463911  0.6463911 ]\n",
      " [1.4210275  1.0074936  0.516266   0.516266  ]\n",
      " [1.3627812  0.9104005  0.9104005  0.9104005 ]\n",
      " [1.2997112  0.8473305  0.8473305  0.8473305 ]\n",
      " [1.2226195  0.77023846 0.77023846 0.77023846]\n",
      " [1.1199116  0.66753054 0.66753054 0.66753054]\n",
      " [0.9911473  0.5387663  0.5387663  0.5387663 ]\n",
      " [0.84245384 0.39007285 0.39007285 0.39007285]\n",
      " [0.69836265 0.24598175 0.24598175 0.24598175]]\n",
      "Iteration: 24\n",
      "Iteration 24, Loss: 0.1318691521883011, Grad_X_e: [[1.6343161  1.1819352  1.1819352  1.1819352 ]\n",
      " [1.6297833  1.1774023  1.1774023  1.1774023 ]\n",
      " [1.6259735  1.1735927  1.1735927  1.1735927 ]\n",
      " [1.6312747  1.6613499  1.1788939  1.1788939 ]\n",
      " [1.6170987  1.1647179  1.1647179  1.1647179 ]\n",
      " [1.6113416  1.1589605  1.1589605  1.1589605 ]\n",
      " [1.6048703  1.1524892  1.1524892  1.1524892 ]\n",
      " [1.5974149  1.1450337  1.1450337  1.1450337 ]\n",
      " [1.8952463  0.99048465 0.99048465 0.99048465]\n",
      " [1.8515354  0.9467736  0.9467736  0.9467736 ]\n",
      " [1.9084283  1.4948944  1.0036669  1.0036669 ]\n",
      " [1.8675164  1.4539826  0.9627544  0.9627544 ]\n",
      " [1.8143635  1.4008296  0.9096012  0.9096012 ]\n",
      " [1.744064   1.33053    0.839302   0.839302  ]\n",
      " [1.6531247  1.2395908  0.7483627  0.7483627 ]\n",
      " [1.5403392  1.1268055  0.63557744 0.63557744]\n",
      " [1.4057362  0.99220234 0.5009747  0.5009747 ]\n",
      " [1.3737564  0.92137575 0.92137575 0.92137575]\n",
      " [1.3042052  0.8518245  0.8518245  0.8518245 ]\n",
      " [1.2200093  0.7676282  0.7676282  0.7676282 ]\n",
      " [1.1103309  0.6579499  0.6579499  0.6579499 ]\n",
      " [0.9763525  0.52397156 0.52397156 0.52397156]\n",
      " [0.82732064 0.37493965 0.37493965 0.37493965]\n",
      " [0.69310856 0.24072768 0.24072768 0.24072768]]\n",
      "Iteration: 25\n",
      "Iteration 25, Loss: 0.1318691521883011, Grad_X_e: [[1.6757545  1.2233737  1.2233737  1.2233737 ]\n",
      " [1.670707   1.2183261  1.2183261  1.2183261 ]\n",
      " [1.6663736  1.2139928  1.2139928  1.2139928 ]\n",
      " [1.6723379  1.7024131  1.2199571  1.2199571 ]\n",
      " [1.6562558  1.2038751  1.2038751  1.2038751 ]\n",
      " [1.6497424  1.1973612  1.1973612  1.1973612 ]\n",
      " [1.6423938  1.1900127  1.1900127  1.1900127 ]\n",
      " [1.6339792  1.1815981  1.1815981  1.1815981 ]\n",
      " [1.9117013  1.0069396  1.0069396  1.0069396 ]\n",
      " [1.8626544  0.95789254 0.95789254 0.95789254]\n",
      " [1.9263527  1.5128188  1.0215913  1.0215913 ]\n",
      " [1.8805916  1.4670578  0.9758297  0.9758297 ]\n",
      " [1.8215432  1.4080093  0.91678095 0.91678095]\n",
      " [1.7446046  1.3310707  0.83984256 0.83984256]\n",
      " [1.6468092  1.2332753  0.7420473  0.7420473 ]\n",
      " [1.5280485  1.1145148  0.6232867  0.6232867 ]\n",
      " [1.3901755  0.97664154 0.48541394 0.48541394]\n",
      " [1.382023   0.9296423  0.9296423  0.9296423 ]\n",
      " [1.3060087  0.85362804 0.85362804 0.85362804]\n",
      " [1.214971   0.7625898  0.7625898  0.7625898 ]\n",
      " [1.0990456  0.6466646  0.6466646  0.6466646 ]\n",
      " [0.96111757 0.5087366  0.5087366  0.5087366 ]\n",
      " [0.81353414 0.3611532  0.3611532  0.3611532 ]\n",
      " [0.69090265 0.23852177 0.23852177 0.23852177]]\n",
      "Iteration: 26\n",
      "Iteration 26, Loss: 0.1318691521883011, Grad_X_e: [[1.7165065  1.2641256  1.2641256  1.2641256 ]\n",
      " [1.7109032  1.2585223  1.2585223  1.2585223 ]\n",
      " [1.7059983  1.2536175  1.2536175  1.2536175 ]\n",
      " [1.7126864  1.7427616  1.2603056  1.2603056 ]\n",
      " [1.6945376  1.2421569  1.2421569  1.2421569 ]\n",
      " [1.6871924  1.2348113  1.2348113  1.2348113 ]\n",
      " [1.6789057  1.2265246  1.2265246  1.2265246 ]\n",
      " [1.6694423  1.2170612  1.2170612  1.2170612 ]\n",
      " [1.9257325  1.0209708  1.0209708  1.0209708 ]\n",
      " [1.8711983  0.9664363  0.9664363  0.9664363 ]\n",
      " [1.9419258  1.5283918  1.0371643  1.0371643 ]\n",
      " [1.8911461  1.4776123  0.9863841  0.9863841 ]\n",
      " [1.8261678  1.4126339  0.92140555 0.92140555]\n",
      " [1.7427156  1.3291817  0.83795357 0.83795357]\n",
      " [1.6385808  1.2250469  0.7338188  0.7338188 ]\n",
      " [1.5146704  1.1011367  0.6099086  0.6099086 ]\n",
      " [1.3748645  0.96133053 0.47010294 0.47010294]\n",
      " [1.3876816  0.935301   0.935301   0.935301  ]\n",
      " [1.3053501  0.85296947 0.85296947 0.85296947]\n",
      " [1.2078227  0.75544155 0.75544155 0.75544155]\n",
      " [1.0864574  0.6340764  0.6340764  0.6340764 ]\n",
      " [0.9458789  0.49349797 0.49349797 0.49349797]\n",
      " [0.8015239  0.349143   0.349143   0.349143  ]\n",
      " [0.69163233 0.23925143 0.23925143 0.23925143]]\n",
      "Iteration: 27\n",
      "Iteration 27, Loss: 0.1318691521883011, Grad_X_e: [[1.7565463  1.3041654  1.3041654  1.3041654 ]\n",
      " [1.7503518  1.2979709  1.2979709  1.2979709 ]\n",
      " [1.7448246  1.2924439  1.2924439  1.2924439 ]\n",
      " [1.7523012  1.7823764  1.2999204  1.2999204 ]\n",
      " [1.731902   1.2795212  1.2795212  1.2795212 ]\n",
      " [1.7236675  1.2712864  1.2712864  1.2712864 ]\n",
      " [1.7143629  1.2619817  1.2619817  1.2619817 ]\n",
      " [1.7037671  1.2513859  1.2513859  1.2513859 ]\n",
      " [1.9373714  1.0326097  1.0326097  1.0326097 ]\n",
      " [1.8772628  0.97250086 0.97250086 0.97250086]\n",
      " [1.9551476  1.5416137  1.0503862  1.0503862 ]\n",
      " [1.8992387  1.4857049  0.99447674 0.99447674]\n",
      " [1.8283198  1.4147859  0.9235575  0.9235575 ]\n",
      " [1.738601   1.325067   0.83383894 0.83383894]\n",
      " [1.6287118  1.2151779  0.72394985 0.72394985]\n",
      " [1.5005615  1.0870278  0.59579974 0.59579974]\n",
      " [1.360205   0.9466711  0.45544353 0.45544353]\n",
      " [1.3908654  0.93848485 0.93848485 0.93848485]\n",
      " [1.3024359  0.8500553  0.8500553  0.8500553 ]\n",
      " [1.198908   0.7465268  0.7465268  0.7465268 ]\n",
      " [1.0729326  0.62055165 0.62055165 0.62055165]\n",
      " [0.9310459  0.47866496 0.47866496 0.47866496]\n",
      " [0.79158604 0.33920512 0.33920512 0.33920512]\n",
      " [0.69489235 0.24251148 0.24251148 0.24251148]]\n",
      "Iteration: 28\n",
      "Iteration 28, Loss: 0.1318691521883011, Grad_X_e: [[1.7958391  1.3434582  1.3434582  1.3434582 ]\n",
      " [1.7890226  1.3366417  1.3366417  1.3366417 ]\n",
      " [1.7828188  1.330438   1.330438   1.330438  ]\n",
      " [1.7911441  1.8212193  1.3387634  1.3387634 ]\n",
      " [1.7683166  1.3159359  1.3159359  1.3159359 ]\n",
      " [1.759129   1.3067479  1.3067479  1.3067479 ]\n",
      " [1.748733   1.2963519  1.2963519  1.2963519 ]\n",
      " [1.736914   1.2845329  1.2845329  1.2845329 ]\n",
      " [1.9466784  1.0419167  1.0419167  1.0419167 ]\n",
      " [1.8809338  0.9761718  0.9761718  0.9761718 ]\n",
      " [1.9660478  1.5525138  1.0612863  1.0612863 ]\n",
      " [1.9049597  1.4914259  1.0001976  1.0001976 ]\n",
      " [1.8281614  1.4146274  0.9233991  0.9233991 ]\n",
      " [1.7324959  1.318962   0.8277339  0.8277339 ]\n",
      " [1.6175084  1.2039745  0.7127464  0.7127464 ]\n",
      " [1.4860508  1.0725172  0.5812892  0.5812892 ]\n",
      " [1.3464882  0.9329543  0.4417267  0.4417267 ]\n",
      " [1.3916947  0.939314   0.939314   0.939314  ]\n",
      " [1.2974532  0.8450726  0.8450726  0.8450726 ]\n",
      " [1.1884818  0.7361007  0.7361007  0.7361007 ]\n",
      " [1.058808   0.606427   0.606427   0.606427  ]\n",
      " [0.916918   0.46453702 0.46453702 0.46453702]\n",
      " [0.78388697 0.33150607 0.33150607 0.33150607]\n",
      " [0.70031714 0.24793628 0.24793628 0.24793628]]\n",
      "Iteration: 29\n",
      "Iteration 29, Loss: 0.1318691521883011, Grad_X_e: [[1.8343836  1.3820027  1.3820027  1.3820027 ]\n",
      " [1.8269022  1.3745213  1.3745213  1.3745213 ]\n",
      " [1.8199699  1.3675891  1.3675891  1.3675891 ]\n",
      " [1.8292035  1.8592787  1.3768227  1.3768227 ]\n",
      " [1.8037584  1.3513777  1.3513777  1.3513777 ]\n",
      " [1.7935488  1.3411677  1.3411677  1.3411677 ]\n",
      " [1.7819945  1.3296133  1.3296133  1.3296133 ]\n",
      " [1.7688625  1.3164814  1.3164814  1.3164814 ]\n",
      " [1.9537594  1.0489978  1.0489978  1.0489978 ]\n",
      " [1.8824097  0.9776478  0.9776478  0.9776478 ]\n",
      " [1.9747354  1.5612015  1.069974   1.069974  ]\n",
      " [1.9084476  1.4949138  1.0036856  1.0036856 ]\n",
      " [1.8259281  1.4123942  0.92116576 0.92116576]\n",
      " [1.7247155  1.3111815  0.8199534  0.8199534 ]\n",
      " [1.605309   1.1917751  0.700547   0.700547  ]\n",
      " [1.4715161  1.0579824  0.56675446 0.56675446]\n",
      " [1.3340821  0.92054826 0.42932066 0.42932066]\n",
      " [1.3903644  0.93798375 0.93798375 0.93798375]\n",
      " [1.2906947  0.8383141  0.8383141  0.8383141 ]\n",
      " [1.1768904  0.72450924 0.72450924 0.72450924]\n",
      " [1.0444986  0.59211767 0.59211767 0.59211767]\n",
      " [0.90386784 0.45148686 0.45148686 0.45148686]\n",
      " [0.77846956 0.32608867 0.32608867 0.32608867]\n",
      " [0.7074231  0.25504225 0.25504225 0.25504225]]\n",
      "Iteration: 30\n",
      "Iteration 30, Loss: 0.1318691521883011, Grad_X_e: [[1.8721588  1.4197779  1.4197779  1.4197779 ]\n",
      " [1.8639665  1.4115856  1.4115856  1.4115856 ]\n",
      " [1.856255   1.4038743  1.4038743  1.4038743 ]\n",
      " [1.8664654  1.8965406  1.4140847  1.4140847 ]\n",
      " [1.8382137  1.385833   1.385833   1.385833  ]\n",
      " [1.8269087  1.3745276  1.3745276  1.3745276 ]\n",
      " [1.8141228  1.3617417  1.3617417  1.3617417 ]\n",
      " [1.7995886  1.3472074  1.3472074  1.3472074 ]\n",
      " [1.9587094  1.0539477  1.0539477  1.0539477 ]\n",
      " [1.8818712  0.9771093  0.9771093  0.9771093 ]\n",
      " [1.9812738  1.5677398  1.0765123  1.0765123 ]\n",
      " [1.9098282  1.4962944  1.0050662  1.0050662 ]\n",
      " [1.8218338  1.4082999  0.9170716  0.9170716 ]\n",
      " [1.7155209  1.3019869  0.81075877 0.81075877]\n",
      " [1.5924248  1.1788908  0.68766266 0.68766266]\n",
      " [1.4573842  1.0438505  0.5526226  0.5526226 ]\n",
      " [1.3232435  0.9097097  0.4184821  0.4184821 ]\n",
      " [1.3870955  0.93471485 0.93471485 0.93471485]\n",
      " [1.2824267  0.8300461  0.8300461  0.8300461 ]\n",
      " [1.1644508  0.71206963 0.71206963 0.71206963]\n",
      " [1.0303217  0.5779408  0.5779408  0.5779408 ]\n",
      " [0.8922494  0.43986842 0.43986842 0.43986842]\n",
      " [0.7754331  0.3230522  0.3230522  0.3230522 ]\n",
      " [0.7157689  0.26338804 0.26338804 0.26338804]]\n",
      "Iteration: 31\n",
      "Iteration 31, Loss: 0.1318691521883011, Grad_X_e: [[1.9091425  1.4567616  1.4567616  1.4567616 ]\n",
      " [1.9001948  1.4478139  1.4478139  1.4478139 ]\n",
      " [1.89165    1.4392692  1.4392692  1.4392692 ]\n",
      " [1.90291    1.9329852  1.4505292  1.4505292 ]\n",
      " [1.8716604  1.4192797  1.4192797  1.4192797 ]\n",
      " [1.859188   1.4068068  1.4068068  1.4068068 ]\n",
      " [1.8450844  1.3927033  1.3927033  1.3927033 ]\n",
      " [1.8290656  1.3766844  1.3766844  1.3766844 ]\n",
      " [1.9616317  1.05687    1.05687    1.05687   ]\n",
      " [1.8794619  0.97470003 0.97470003 0.97470003]\n",
      " [1.9857711  1.5722371  1.0810096  1.0810096 ]\n",
      " [1.9092544  1.4957206  1.0044924  1.0044924 ]\n",
      " [1.816096   1.402562   0.9113337  0.9113337 ]\n",
      " [1.7051766  1.2916427  0.8004145  0.8004145 ]\n",
      " [1.5791734  1.1656395  0.6744113  0.6744113 ]\n",
      " [1.4439455  1.0304118  0.5391839  0.5391839 ]\n",
      " [1.3141692  0.90063536 0.40940776 0.40940776]\n",
      " [1.3820888  0.92970824 0.92970824 0.92970824]\n",
      " [1.2728922  0.82051164 0.82051164 0.82051164]\n",
      " [1.1514845  0.6991033  0.6991033  0.6991033 ]\n",
      " [1.0165751  0.5641942  0.5641942  0.5641942 ]\n",
      " [0.8822606  0.42987964 0.42987964 0.42987964]\n",
      " [0.7746961  0.32231522 0.32231522 0.32231522]\n",
      " [0.72479266 0.27241182 0.27241182 0.27241182]]\n",
      "Iteration: 32\n",
      "Iteration 32, Loss: 0.1318691521883011, Grad_X_e: [[1.9453115  1.4929307  1.4929307  1.4929307 ]\n",
      " [1.935565   1.4831841  1.4831841  1.4831841 ]\n",
      " [1.9261341  1.4737533  1.4737533  1.4737533 ]\n",
      " [1.9385111  1.9685863  1.4861304  1.4861304 ]\n",
      " [1.9040743  1.4516937  1.4516937  1.4516937 ]\n",
      " [1.8903573  1.4379761  1.4379761  1.4379761 ]\n",
      " [1.8748575  1.4224764  1.4224764  1.4224764 ]\n",
      " [1.8572716  1.4048904  1.4048904  1.4048904 ]\n",
      " [1.962656   1.0578943  1.0578943  1.0578943 ]\n",
      " [1.8753537  0.9705918  0.9705918  0.9705918 ]\n",
      " [1.9883424  1.5748085  1.083581   1.083581  ]\n",
      " [1.9069049  1.4933711  1.0021429  1.0021429 ]\n",
      " [1.80889    1.395356   0.9041277  0.9041277 ]\n",
      " [1.6939251  1.2803912  0.78916305 0.78916305]\n",
      " [1.5658195  1.1522856  0.6610574  0.6610574 ]\n",
      " [1.4314364  1.0179027  0.52667475 0.52667475]\n",
      " [1.3070458  0.893512   0.4022844  0.4022844 ]\n",
      " [1.3755496  0.923169   0.923169   0.923169  ]\n",
      " [1.262339   0.8099584  0.8099584  0.8099584 ]\n",
      " [1.13823    0.6858487  0.6858487  0.6858487 ]\n",
      " [1.0035393  0.55115837 0.55115837 0.55115837]\n",
      " [0.8740898  0.42170876 0.42170876 0.42170876]\n",
      " [0.77601045 0.32362953 0.32362953 0.32362953]\n",
      " [0.7339727  0.2815919  0.2815919  0.2815919 ]]\n",
      "Iteration: 33\n",
      "Iteration 33, Loss: 0.1318691521883011, Grad_X_e: [[1.9806554  1.5282745  1.5282745  1.5282745 ]\n",
      " [1.9700637  1.5176828  1.5176828  1.5176828 ]\n",
      " [1.9596957  1.5073149  1.5073149  1.5073149 ]\n",
      " [1.9732562  2.0033314  1.5208755  1.5208755 ]\n",
      " [1.9354358  1.4830551  1.4830551  1.4830551 ]\n",
      " [1.9204048  1.4680237  1.4680237  1.4680237 ]\n",
      " [1.9034318  1.4510506  1.4510506  1.4510506 ]\n",
      " [1.8841897  1.4318086  1.4318086  1.4318086 ]\n",
      " [1.961937   1.0571753  1.0571753  1.0571753 ]\n",
      " [1.8697656  0.9650037  0.9650037  0.9650037 ]\n",
      " [1.9891098  1.5755758  1.0843483  1.0843483 ]\n",
      " [1.9029425  1.4894087  0.9981805  0.9981805 ]\n",
      " [1.8004442  1.3869103  0.89568204 0.89568204]\n",
      " [1.6820424  1.2685084  0.77728033 0.77728033]\n",
      " [1.5526414  1.1391075  0.64787924 0.64787924]\n",
      " [1.420081   1.0065473  0.5153194  0.5153194 ]\n",
      " [1.3019048  0.888371   0.3971434  0.3971434 ]\n",
      " [1.3677094  0.91532886 0.91532886 0.91532886]\n",
      " [1.2510456  0.7986649  0.7986649  0.7986649 ]\n",
      " [1.1249692  0.67258793 0.67258793 0.67258793]\n",
      " [0.99147946 0.5390985  0.5390985  0.5390985 ]\n",
      " [0.86782086 0.41543987 0.41543987 0.41543987]\n",
      " [0.7792084  0.3268275  0.3268275  0.3268275 ]\n",
      " [0.7428213  0.2904404  0.2904404  0.2904404 ]]\n",
      "Iteration: 34\n",
      "Iteration 34, Loss: 0.1318691521883011, Grad_X_e: [[2.0151596  1.5627787  1.5627787  1.5627787 ]\n",
      " [2.0036757  1.5512948  1.5512948  1.5512948 ]\n",
      " [1.9923158  1.539935   1.539935   1.539935  ]\n",
      " [2.007126   2.0372014  1.5547453  1.5547453 ]\n",
      " [1.965735   1.5133543  1.5133543  1.5133543 ]\n",
      " [1.9493194  1.4969382  1.4969382  1.4969382 ]\n",
      " [1.9307938  1.4784126  1.4784126  1.4784126 ]\n",
      " [1.9098082  1.457427   1.457427   1.457427  ]\n",
      " [1.9596336  1.0548719  1.0548719  1.0548719 ]\n",
      " [1.8628876  0.95812577 0.95812577 0.95812577]\n",
      " [1.9882196  1.5746857  1.0834582  1.0834582 ]\n",
      " [1.897556   1.4840221  0.992794   0.992794  ]\n",
      " [1.790991   1.377457   0.8862288  0.8862288 ]\n",
      " [1.6697835  1.2562495  0.76502144 0.76502144]\n",
      " [1.5399159  1.126382   0.6351537  0.6351537 ]\n",
      " [1.4101118  0.99657816 0.50535023 0.50535023]\n",
      " [1.2987773  0.88524354 0.39401594 0.39401594]\n",
      " [1.3587917  0.9064112  0.9064112  0.9064112 ]\n",
      " [1.239269   0.7868884  0.7868884  0.7868884 ]\n",
      " [1.1119965  0.6596152  0.6596152  0.6596152 ]\n",
      " [0.98062783 0.5282469  0.5282469  0.5282469 ]\n",
      " [0.86351055 0.4111296  0.4111296  0.4111296 ]\n",
      " [0.7840196  0.33163863 0.33163863 0.33163863]\n",
      " [0.7508802  0.29849932 0.29849932 0.29849932]]\n",
      "Iteration: 35\n",
      "Iteration 35, Loss: 0.1318691521883011, Grad_X_e: [[2.0488033  1.5964226  1.5964226  1.5964226 ]\n",
      " [2.0363815  1.5840006  1.5840006  1.5840006 ]\n",
      " [2.0239818  1.5716009  1.5716009  1.5716009 ]\n",
      " [2.0401077  2.070183   1.5877268  1.5877268 ]\n",
      " [1.9949535  1.5425729  1.5425729  1.5425729 ]\n",
      " [1.9770838  1.5247027  1.5247027  1.5247027 ]\n",
      " [1.9569312  1.5045501  1.5045501  1.5045501 ]\n",
      " [1.9341198  1.4817387  1.4817387  1.4817387 ]\n",
      " [1.9558821  1.0511204  1.0511204  1.0511204 ]\n",
      " [1.8549148  0.950153   0.950153   0.950153  ]\n",
      " [1.9858145  1.5722805  1.081053   1.081053  ]\n",
      " [1.890928   1.4773942  0.986166   0.986166  ]\n",
      " [1.780755   1.3672211  0.87599295 0.87599295]\n",
      " [1.6573852  1.2438513  0.75262326 0.75262326]\n",
      " [1.5278548  1.1143209  0.62309265 0.62309265]\n",
      " [1.4016691  0.9881356  0.49690765 0.49690765]\n",
      " [1.2975959  0.8840621  0.3928345  0.3928345 ]\n",
      " [1.3490016  0.89662105 0.89662105 0.89662105]\n",
      " [1.2272347  0.7748541  0.7748541  0.7748541 ]\n",
      " [1.0995258  0.6471445  0.6471445  0.6471445 ]\n",
      " [0.97114706 0.5187661  0.5187661  0.5187661 ]\n",
      " [0.8611413  0.4087604  0.4087604  0.4087604 ]\n",
      " [0.7901668  0.33778587 0.33778587 0.33778587]\n",
      " [0.7578021  0.30542126 0.30542126 0.30542126]]\n",
      "Iteration: 36\n",
      "Iteration 36, Loss: 0.1318691521883011, Grad_X_e: [[2.0815747  1.6291939  1.6291939  1.6291939 ]\n",
      " [2.0681734  1.6157925  1.6157925  1.6157925 ]\n",
      " [2.0546765  1.6022956  1.6022956  1.6022956 ]\n",
      " [2.0721889  2.1022642  1.619808   1.619808  ]\n",
      " [2.023084   1.5707031  1.5707031  1.5707031 ]\n",
      " [2.0036857  1.5513045  1.5513045  1.5513045 ]\n",
      " [1.9818404  1.5294592  1.5294592  1.5294592 ]\n",
      " [1.9571186  1.5047375  1.5047375  1.5047375 ]\n",
      " [1.9508533  1.0460917  1.0460917  1.0460917 ]\n",
      " [1.8460476  0.9412858  0.9412858  0.9412858 ]\n",
      " [1.9820426  1.5685086  1.0772811  1.0772811 ]\n",
      " [1.8832567  1.4697229  0.97849464 0.97849464]\n",
      " [1.7699556  1.3564217  0.8651936  0.8651936 ]\n",
      " [1.6450821  1.2315482  0.74032015 0.74032015]\n",
      " [1.516658   1.103124   0.61189586 0.61189586]\n",
      " [1.3948463  0.98131275 0.49008483 0.49008483]\n",
      " [1.2982258  0.88469195 0.39346436 0.39346436]\n",
      " [1.3385508  0.88617015 0.88617015 0.88617015]\n",
      " [1.2151653  0.76278466 0.76278466 0.76278466]\n",
      " [1.0877743  0.63539296 0.63539296 0.63539296]\n",
      " [0.96317226 0.5107913  0.5107913  0.5107913 ]\n",
      " [0.8606263  0.40824535 0.40824535 0.40824535]\n",
      " [0.7972759  0.344895   0.344895   0.344895  ]\n",
      " [0.7633028  0.31092194 0.31092194 0.31092194]]\n",
      "Iteration: 37\n",
      "Iteration 37, Loss: 0.1318691521883011, Grad_X_e: [[2.1134624  1.6610817  1.6610817  1.6610817 ]\n",
      " [2.0990338  1.6466529  1.6466529  1.6466529 ]\n",
      " [2.084387   1.6320062  1.6320062  1.6320062 ]\n",
      " [2.103353   2.1334283  1.6509722  1.6509722 ]\n",
      " [2.0501094  1.5977286  1.5977286  1.5977286 ]\n",
      " [2.0291204  1.5767392  1.5767392  1.5767392 ]\n",
      " [2.0055103  1.5531293  1.5531293  1.5531293 ]\n",
      " [1.9788038  1.5264226  1.5264226  1.5264226 ]\n",
      " [1.9446948  1.0399331  1.0399331  1.0399331 ]\n",
      " [1.8364705  0.93170863 0.93170863 0.93170863]\n",
      " [1.9770576  1.5635237  1.0722961  1.0722961 ]\n",
      " [1.8747135  1.4611797  0.9699515  0.9699515 ]\n",
      " [1.7587954  1.3452615  0.8540334  0.8540334 ]\n",
      " [1.6330932  1.2195593  0.72833127 0.72833127]\n",
      " [1.5064975  1.0929636  0.6017354  0.6017354 ]\n",
      " [1.3897119  0.9761783  0.48495036 0.48495036]\n",
      " [1.3004946  0.8869607  0.39573312 0.39573312]\n",
      " [1.3276461  0.87526554 0.87526554 0.87526554]\n",
      " [1.2032548  0.7508742  0.7508742  0.7508742 ]\n",
      " [1.0769148  0.6245335  0.6245335  0.6245335 ]\n",
      " [0.95677286 0.5043919  0.5043919  0.5043919 ]\n",
      " [0.8618381  0.4094572  0.4094572  0.4094572 ]\n",
      " [0.8049712  0.3525903  0.3525903  0.3525903 ]\n",
      " [0.7671582  0.31477734 0.31477734 0.31477734]]\n",
      "Iteration: 38\n",
      "Iteration 38, Loss: 0.1318691521883011, Grad_X_e: [[2.1444614  1.6920805  1.6920805  1.6920805 ]\n",
      " [2.12896    1.6765789  1.6765789  1.6765789 ]\n",
      " [2.1131098  1.6607289  1.6607289  1.6607289 ]\n",
      " [2.133601   2.1636763  1.6812202  1.6812202 ]\n",
      " [2.0760312  1.6236504  1.6236504  1.6236504 ]\n",
      " [2.0533879  1.6010066  1.6010066  1.6010066 ]\n",
      " [2.0279434  1.5755625  1.5755625  1.5755625 ]\n",
      " [1.9991876  1.5468065  1.5468065  1.5468065 ]\n",
      " [1.9375889  1.0328273  1.0328273  1.0328273 ]\n",
      " [1.8263862  0.9216244  0.9216244  0.9216244 ]\n",
      " [1.9710281  1.5574942  1.0662667  1.0662667 ]\n",
      " [1.8654982  1.4519644  0.9607361  0.9607361 ]\n",
      " [1.7474862  1.3339523  0.8427243  0.8427243 ]\n",
      " [1.6216098  1.2080759  0.7168479  0.7168479 ]\n",
      " [1.4975364  1.0840025  0.5927743  0.5927743 ]\n",
      " [1.3862698  0.97273624 0.48150828 0.48150828]\n",
      " [1.3042448  0.8907109  0.39948332 0.39948332]\n",
      " [1.3164911  0.8641105  0.8641105  0.8641105 ]\n",
      " [1.1917261  0.73934555 0.73934555 0.73934555]\n",
      " [1.067127   0.6147456  0.6147456  0.6147456 ]\n",
      " [0.95201564 0.49963465 0.49963465 0.49963465]\n",
      " [0.864612   0.4122311  0.4122311  0.4122311 ]\n",
      " [0.8129351  0.36055422 0.36055422 0.36055422]\n",
      " [0.76928824 0.31690735 0.31690735 0.31690735]]\n",
      "Iteration: 39\n",
      "Iteration 39, Loss: 0.1318691521883011, Grad_X_e: [[2.1745605  1.7221795  1.7221795  1.7221795 ]\n",
      " [2.1579397  1.7055585  1.7055585  1.7055585 ]\n",
      " [2.140839   1.6884582  1.6884582  1.6884582 ]\n",
      " [2.1629176  2.192993   1.7105367  1.7105367 ]\n",
      " [2.1008413  1.6484604  1.6484604  1.6484604 ]\n",
      " [2.076485   1.6241038  1.6241038  1.6241038 ]\n",
      " [2.0491424  1.5967616  1.5967616  1.5967616 ]\n",
      " [2.0182748  1.5658937  1.5658937  1.5658937 ]\n",
      " [1.9296846  1.024923   1.024923   1.024923  ]\n",
      " [1.8159604  0.9111986  0.9111986  0.9111986 ]\n",
      " [1.9640994  1.5505655  1.059338   1.059338  ]\n",
      " [1.8557736  1.4422398  0.9510115  0.9510115 ]\n",
      " [1.736201   1.3226671  0.8314391  0.8314391 ]\n",
      " [1.6107973  1.1972634  0.7060354  0.7060354 ]\n",
      " [1.48986    1.0763261  0.5850979  0.5850979 ]\n",
      " [1.3844634  0.9709299  0.47970197 0.47970197]\n",
      " [1.3091811  0.89564717 0.40441963 0.40441963]\n",
      " [1.3052628  0.8528822  0.8528822  0.8528822 ]\n",
      " [1.1807325  0.72835195 0.72835195 0.72835195]\n",
      " [1.0585147  0.60613334 0.60613334 0.60613334]\n",
      " [0.94886446 0.49648345 0.49648345 0.49648345]\n",
      " [0.8686986  0.4163177  0.4163177  0.4163177 ]\n",
      " [0.8207943  0.3684134  0.3684134  0.3684134 ]\n",
      " [0.7697042  0.31732333 0.31732333 0.31732333]]\n",
      "Iteration: 40\n",
      "Iteration 40, Loss: 0.1318691521883011, Grad_X_e: [[2.2037551  1.751374   1.751374   1.751374  ]\n",
      " [2.185969   1.733588   1.733588   1.733588  ]\n",
      " [2.1675684  1.7151875  1.7151875  1.7151875 ]\n",
      " [2.1912973  2.2213726  1.7389164  1.7389164 ]\n",
      " [2.1245425  1.6721615  1.6721615  1.6721615 ]\n",
      " [2.0984135  1.6460323  1.6460323  1.6460323 ]\n",
      " [2.0691152  1.6167343  1.6167343  1.6167343 ]\n",
      " [2.0360773  1.5836961  1.5836961  1.5836961 ]\n",
      " [1.9211527  1.016391   1.016391   1.016391  ]\n",
      " [1.80538    0.90061826 0.90061826 0.90061826]\n",
      " [1.9564286  1.5428947  1.0516672  1.0516672 ]\n",
      " [1.8457171  1.4321833  0.94095504 0.94095504]\n",
      " [1.7251256  1.3115916  0.8203635  0.8203635 ]\n",
      " [1.6008041  1.1872702  0.6960422  0.6960422 ]\n",
      " [1.4835415  1.0700076  0.5787793  0.5787793 ]\n",
      " [1.3842212  0.9706877  0.4794597  0.4794597 ]\n",
      " [1.3150702  0.9015362  0.4103087  0.4103087 ]\n",
      " [1.29415    0.84176934 0.84176934 0.84176934]\n",
      " [1.1704394  0.71805876 0.71805876 0.71805876]\n",
      " [1.0511768  0.5987955  0.5987955  0.5987955 ]\n",
      " [0.9472876  0.4949066  0.4949066  0.4949066 ]\n",
      " [0.8738691  0.42148823 0.42148823 0.42148823]\n",
      " [0.8282318  0.3758509  0.3758509  0.3758509 ]\n",
      " [0.76854664 0.31616575 0.31616575 0.31616575]]\n",
      "Iteration: 41\n",
      "Iteration 41, Loss: 0.1318691521883011, Grad_X_e: [[2.23204    1.7796587  1.7796587  1.7796587 ]\n",
      " [2.2130413  1.76066    1.76066    1.76066   ]\n",
      " [2.193297   1.740916   1.740916   1.740916  ]\n",
      " [2.2187326  2.248808   1.7663516  1.7663516 ]\n",
      " [2.1471317  1.6947505  1.6947505  1.6947505 ]\n",
      " [2.1191797  1.6667985  1.6667985  1.6667985 ]\n",
      " [2.0878701  1.6354891  1.6354891  1.6354891 ]\n",
      " [2.0526078  1.6002266  1.6002266  1.6002266 ]\n",
      " [1.9121507  1.0073891  1.0073891  1.0073891 ]\n",
      " [1.7948074  0.8900457  0.8900457  0.8900457 ]\n",
      " [1.9481698  1.5346359  1.0434084  1.0434084 ]\n",
      " [1.8354931  1.4219593  0.93073106 0.93073106]\n",
      " [1.7144083  1.3008744  0.8096463  0.8096463 ]\n",
      " [1.5917555  1.1782216  0.68699354 0.68699354]\n",
      " [1.4786153  1.0650814  0.5738531  0.5738531 ]\n",
      " [1.3854151  0.97188157 0.48065358 0.48065358]\n",
      " [1.3216327  0.9080988  0.41687125 0.41687125]\n",
      " [1.2833105  0.8309299  0.8309299  0.8309299 ]\n",
      " [1.1609738  0.7085932  0.7085932  0.7085932 ]\n",
      " [1.0451735  0.5927923  0.5927923  0.5927923 ]\n",
      " [0.9471964  0.4948154  0.4948154  0.4948154 ]\n",
      " [0.87986284 0.42748198 0.42748198 0.42748198]\n",
      " [0.8349672  0.38258627 0.38258627 0.38258627]\n",
      " [0.7659878  0.31360695 0.31360695 0.31360695]]\n",
      "Iteration: 42\n",
      "Iteration 42, Loss: 0.1318691521883011, Grad_X_e: [[2.2594078  1.8070266  1.8070266  1.8070266 ]\n",
      " [2.2391582  1.786777   1.786777   1.786777  ]\n",
      " [2.2180228  1.765642   1.765642   1.765642  ]\n",
      " [2.2452226  2.2752979  1.7928416  1.7928416 ]\n",
      " [2.1686137  1.7162325  1.7162325  1.7162325 ]\n",
      " [2.1387887  1.6864073  1.6864073  1.6864073 ]\n",
      " [2.1054206  1.6530395  1.6530395  1.6530395 ]\n",
      " [2.0678868  1.6155056  1.6155056  1.6155056 ]\n",
      " [1.9028294  0.99806774 0.99806774 0.99806774]\n",
      " [1.784394   0.87963223 0.87963223 0.87963223]\n",
      " [1.93947    1.5259361  1.0347086  1.0347086 ]\n",
      " [1.8252596  1.4117258  0.9204976  0.9204976 ]\n",
      " [1.704195   1.2906611  0.799433   0.799433  ]\n",
      " [1.5837475  1.1702136  0.67898554 0.67898554]\n",
      " [1.4750801  1.0615462  0.5703179  0.5703179 ]\n",
      " [1.3878858  0.9743523  0.48312432 0.48312432]\n",
      " [1.3285847  0.91505075 0.42382318 0.42382318]\n",
      " [1.2728922  0.8205116  0.8205116  0.8205116 ]\n",
      " [1.1524415  0.70006084 0.70006084 0.70006084]\n",
      " [1.0405202  0.58813894 0.58813894 0.58813894]\n",
      " [0.948459   0.496078   0.496078   0.496078  ]\n",
      " [0.88641393 0.4340331  0.4340331  0.4340331 ]\n",
      " [0.8407703  0.3883894  0.3883894  0.3883894 ]\n",
      " [0.76229686 0.30991596 0.30991596 0.30991596]]\n",
      "Iteration: 43\n",
      "Iteration 43, Loss: 0.1318691521883011, Grad_X_e: [[2.2858596  1.8334785  1.8334785  1.8334785 ]\n",
      " [2.26432    1.8119388  1.8119388  1.8119388 ]\n",
      " [2.2417455  1.7893647  1.7893647  1.7893647 ]\n",
      " [2.2707713  2.3008466  1.8183903  1.8183903 ]\n",
      " [2.188996   1.736615   1.736615   1.736615  ]\n",
      " [2.157254   1.7048726  1.7048726  1.7048726 ]\n",
      " [2.1217828  1.6694015  1.6694015  1.6694015 ]\n",
      " [2.0819397  1.6295583  1.6295583  1.6295583 ]\n",
      " [1.8933333  0.98857164 0.98857164 0.98857164]\n",
      " [1.7742866  0.8695248  0.8695248  0.8695248 ]\n",
      " [1.9304749  1.516941   1.0257134  1.0257134 ]\n",
      " [1.8151636  1.4016298  0.9104017  0.9104017 ]\n",
      " [1.6946223  1.2810884  0.7898602  0.7898602 ]\n",
      " [1.576856   1.1633221  0.67209405 0.67209405]\n",
      " [1.4729096  1.0593756  0.56814736 0.56814736]\n",
      " [1.3914661  0.97793263 0.48670465 0.48670465]\n",
      " [1.3356757  0.9221418  0.43091425 0.43091425]\n",
      " [1.2630336  0.8106529  0.8106529  0.8106529 ]\n",
      " [1.1449351  0.6925544  0.6925544  0.6925544 ]\n",
      " [1.0372155  0.5848342  0.5848342  0.5848342 ]\n",
      " [0.95095503 0.49857402 0.49857402 0.49857402]\n",
      " [0.8932636  0.44088274 0.44088274 0.44088274]\n",
      " [0.84547484 0.3930939  0.3930939  0.3930939 ]\n",
      " [0.75778866 0.30540773 0.30540773 0.30540773]]\n",
      "Iteration: 44\n",
      "Iteration 44, Loss: 0.1318691521883011, Grad_X_e: [[2.3113954  1.8590144  1.8590144  1.8590144 ]\n",
      " [2.2885244  1.8361433  1.8361433  1.8361433 ]\n",
      " [2.2644668  1.8120859  1.8120859  1.8120859 ]\n",
      " [2.295376   2.3254514  1.842995   1.842995  ]\n",
      " [2.208289   1.7559077  1.7559077  1.7559077 ]\n",
      " [2.1745882  1.7222067  1.7222067  1.7222067 ]\n",
      " [2.1369777  1.6845963  1.6845963  1.6845963 ]\n",
      " [2.0947921  1.6424109  1.6424109  1.6424109 ]\n",
      " [1.8837965  0.97903484 0.97903484 0.97903484]\n",
      " [1.7646049  0.8598431  0.8598431  0.8598431 ]\n",
      " [1.921314   1.5077801  1.0165526  1.0165526 ]\n",
      " [1.8053368  1.391803   0.90057486 0.90057486]\n",
      " [1.6857924  1.2722585  0.78103036 0.78103036]\n",
      " [1.5711228  1.1575888  0.6663608  0.6663608 ]\n",
      " [1.4720355  1.0585016  0.5672734  0.5672734 ]\n",
      " [1.3959594  0.9824259  0.4911979  0.4911979 ]\n",
      " [1.3426335  0.92909956 0.437872   0.437872  ]\n",
      " [1.2538393  0.80145854 0.80145854 0.80145854]\n",
      " [1.138513   0.6861323  0.6861323  0.6861323 ]\n",
      " [1.035225   0.5828437  0.5828437  0.5828437 ]\n",
      " [0.9545014  0.5021204  0.5021204  0.5021204 ]\n",
      " [0.90014577 0.44776493 0.44776493 0.44776493]\n",
      " [0.84895784 0.39657694 0.39657694 0.39657694]\n",
      " [0.75282204 0.30044112 0.30044112 0.30044112]]\n",
      "Iteration: 45\n",
      "Iteration 45, Loss: 0.1318691521883011, Grad_X_e: [[2.3360147  1.8836339  1.8836339  1.8836339 ]\n",
      " [2.3117733  1.8593923  1.8593923  1.8593923 ]\n",
      " [2.2861915  1.8338106  1.8338106  1.8338106 ]\n",
      " [2.3190398  2.3491151  1.8666589  1.8666589 ]\n",
      " [2.2265036  1.7741224  1.7741224  1.7741224 ]\n",
      " [2.1908083  1.7384267  1.7384267  1.7384267 ]\n",
      " [2.1510267  1.6986455  1.6986455  1.6986455 ]\n",
      " [2.1064742  1.6540929  1.6540929  1.6540929 ]\n",
      " [1.8743486  0.9695871  0.9695871  0.9695871 ]\n",
      " [1.7554612  0.85069937 0.85069937 0.85069937]\n",
      " [1.9121153  1.4985814  1.0073539  1.0073539 ]\n",
      " [1.7958963  1.3823625  0.8911343  0.8911343 ]\n",
      " [1.6777891  1.2642552  0.773027   0.773027  ]\n",
      " [1.5665687  1.1530348  0.66180676 0.66180676]\n",
      " [1.4723784  1.0588444  0.5676162  0.5676162 ]\n",
      " [1.4011738  0.9876403  0.4964123  0.4964123 ]\n",
      " [1.3492177  0.9356837  0.4444562  0.4444562 ]\n",
      " [1.2454077  0.79302704 0.79302704 0.79302704]\n",
      " [1.1332086  0.6808279  0.6808279  0.6808279 ]\n",
      " [1.0344923  0.58211094 0.58211094 0.58211094]\n",
      " [0.9589143  0.50653327 0.50653327 0.50653327]\n",
      " [0.9068264  0.45444554 0.45444554 0.45444554]\n",
      " [0.85117096 0.39879003 0.39879003 0.39879003]\n",
      " [0.7477295  0.29534853 0.29534853 0.29534853]]\n",
      "Iteration: 46\n",
      "Iteration 46, Loss: 0.1318691521883011, Grad_X_e: [[2.3597188  1.9073379  1.9073379  1.9073379 ]\n",
      " [2.3340695  1.8816886  1.8816886  1.8816886 ]\n",
      " [2.3069246  1.8545438  1.8545438  1.8545438 ]\n",
      " [2.3417647  2.37184    1.8893839  1.8893839 ]\n",
      " [2.2436519  1.7912706  1.7912706  1.7912706 ]\n",
      " [2.205932   1.7535503  1.7535503  1.7535503 ]\n",
      " [2.1639535  1.7115723  1.7115723  1.7115723 ]\n",
      " [2.1170185  1.6646372  1.6646372  1.6646372 ]\n",
      " [1.8651028  0.9603413  0.9603413  0.9603413 ]\n",
      " [1.7469487  0.84218687 0.84218687 0.84218687]\n",
      " [1.9029975  1.4894636  0.99823606 0.99823606]\n",
      " [1.7869403  1.3734065  0.88217837 0.88217837]\n",
      " [1.6706812  1.2571473  0.7659191  0.7659191 ]\n",
      " [1.5631838  1.1496499  0.6584218  0.6584218 ]\n",
      " [1.4738197  1.0602858  0.56905764 0.56905764]\n",
      " [1.4068885  0.993355   0.502127   0.502127  ]\n",
      " [1.3552179  0.941684   0.45045647 0.45045647]\n",
      " [1.2378126  0.7854319  0.7854319  0.7854319 ]\n",
      " [1.129032   0.6766513  0.6766513  0.6766513 ]\n",
      " [1.034922   0.5825407  0.5825407  0.5825407 ]\n",
      " [0.9639927  0.5116117  0.5116117  0.5116117 ]\n",
      " [0.91308737 0.4607065  0.4607065  0.4607065 ]\n",
      " [0.85211015 0.39972922 0.39972922 0.39972922]\n",
      " [0.7428196  0.29043862 0.29043862 0.29043862]]\n",
      "Iteration: 47\n",
      "Iteration 47, Loss: 0.1318691521883011, Grad_X_e: [[2.3825138  1.9301329  1.9301329  1.9301329 ]\n",
      " [2.3554182  1.9030373  1.9030373  1.9030373 ]\n",
      " [2.3266747  1.8742938  1.8742938  1.8742938 ]\n",
      " [2.363553   2.3936284  1.9111723  1.9111723 ]\n",
      " [2.2597513  1.8073702  1.8073702  1.8073702 ]\n",
      " [2.2199805  1.7675989  1.7675989  1.7675989 ]\n",
      " [2.1757863  1.723405   1.723405   1.723405  ]\n",
      " [2.1264617  1.6740805  1.6740805  1.6740805 ]\n",
      " [1.8561642  0.9514027  0.9514027  0.9514027 ]\n",
      " [1.7391461  0.83438426 0.83438426 0.83438426]\n",
      " [1.894071   1.480537   0.98930955 0.98930955]\n",
      " [1.7785581  1.3650243  0.8737962  0.8737962 ]\n",
      " [1.6645175  1.2509836  0.7597554  0.7597554 ]\n",
      " [1.5609442  1.1474103  0.6561822  0.6561822 ]\n",
      " [1.4762331  1.0626992  0.571471   0.571471  ]\n",
      " [1.4128939  0.9993604  0.5081324  0.5081324 ]\n",
      " [1.3604586  0.9469247  0.45569715 0.45569715]\n",
      " [1.2311138  0.778733   0.778733   0.778733  ]\n",
      " [1.1259673  0.6735866  0.6735866  0.6735866 ]\n",
      " [1.0364053  0.584024   0.584024   0.584024  ]\n",
      " [0.9695407  0.5171597  0.5171597  0.5171597 ]\n",
      " [0.91873205 0.4663512  0.4663512  0.4663512 ]\n",
      " [0.8518386  0.39945766 0.39945766 0.39945766]\n",
      " [0.7383678  0.28598678 0.28598678 0.28598678]]\n",
      "Iteration: 48\n",
      "Iteration 48, Loss: 0.1318691521883011, Grad_X_e: [[2.404402   1.9520212  1.9520212  1.9520212 ]\n",
      " [2.3758276  1.9234467  1.9234467  1.9234467 ]\n",
      " [2.345454   1.8930731  1.8930731  1.8930731 ]\n",
      " [2.3844118  2.4144871  1.932031   1.932031  ]\n",
      " [2.2748206  1.8224396  1.8224396  1.8224396 ]\n",
      " [2.232978   1.7805965  1.7805965  1.7805965 ]\n",
      " [2.1865559  1.7341746  1.7341746  1.7341746 ]\n",
      " [2.134842   1.6824608  1.6824608  1.6824608 ]\n",
      " [1.8476313  0.94286984 0.94286984 0.94286984]\n",
      " [1.7321213  0.8273595  0.8273595  0.8273595 ]\n",
      " [1.8854362  1.4719023  0.98067474 0.98067474]\n",
      " [1.7708337  1.3572999  0.8660718  0.8660718 ]\n",
      " [1.6593276  1.2457937  0.7545654  0.7545654 ]\n",
      " [1.5598016  1.1462677  0.6550396  0.6550396 ]\n",
      " [1.4794884  1.0659544  0.5747262  0.5747262 ]\n",
      " [1.4189904  1.0054568  0.5142289  0.5142289 ]\n",
      " [1.3648174  0.95128345 0.46005592 0.46005592]\n",
      " [1.2253515  0.7729707  0.7729707  0.7729707 ]\n",
      " [1.1239897  0.67160904 0.67160904 0.67160904]\n",
      " [1.0388247  0.58644336 0.58644336 0.58644336]\n",
      " [0.9753665  0.52298546 0.52298546 0.52298546]\n",
      " [0.92361784 0.47123697 0.47123697 0.47123697]\n",
      " [0.8504735  0.39809263 0.39809263 0.39809263]\n",
      " [0.73461854 0.2822375  0.2822375  0.2822375 ]]\n",
      "Iteration: 49\n",
      "Iteration 49, Loss: 0.1318691521883011, Grad_X_e: [[2.425391   1.9730102  1.9730102  1.9730102 ]\n",
      " [2.3953063  1.9429256  1.9429256  1.9429256 ]\n",
      " [2.363273   1.910892   1.910892   1.910892  ]\n",
      " [2.4043496  2.4344249  1.9519687  1.9519687 ]\n",
      " [2.2888796  1.8364987  1.8364987  1.8364987 ]\n",
      " [2.2449515  1.7925698  1.7925698  1.7925698 ]\n",
      " [2.1962948  1.7439137  1.7439137  1.7439137 ]\n",
      " [2.1422012  1.68982    1.68982    1.68982   ]\n",
      " [1.839588   0.9348265  0.9348265  0.9348265 ]\n",
      " [1.7259232  0.8211614  0.8211614  0.8211614 ]\n",
      " [1.8771815  1.4636476  0.9724201  0.9724201 ]\n",
      " [1.7638311  1.3502973  0.8590692  0.8590692 ]\n",
      " [1.6551232  1.2415893  0.75036097 0.75036097]\n",
      " [1.5596884  1.1461545  0.6549265  0.6549265 ]\n",
      " [1.483438   1.0699041  0.5786758  0.5786758 ]\n",
      " [1.4249986  1.0114651  0.5202371  0.5202371 ]\n",
      " [1.3682078  0.9546739  0.46344635 0.46344635]\n",
      " [1.2205511  0.7681703  0.7681703  0.7681703 ]\n",
      " [1.1230466  0.67066604 0.67066604 0.67066604]\n",
      " [1.0420498  0.58966845 0.58966845 0.58966845]\n",
      " [0.98127604 0.528895   0.528895   0.528895  ]\n",
      " [0.92763186 0.475251   0.475251   0.475251  ]\n",
      " [0.8481741  0.39579323 0.39579323 0.39579323]\n",
      " [0.7317636  0.27938256 0.27938256 0.27938256]]\n",
      "Iteration: 50\n",
      "Iteration 50, Loss: 0.1318691521883011, Grad_X_e: [[2.4454877  1.9931068  1.9931068  1.9931068 ]\n",
      " [2.4138646  1.9614838  1.9614838  1.9614838 ]\n",
      " [2.3801453  1.9277644  1.9277644  1.9277644 ]\n",
      " [2.4233778  2.453453   1.9709969  1.9709969 ]\n",
      " [2.3019502  1.8495694  1.8495694  1.8495694 ]\n",
      " [2.2559276  1.803546   1.803546   1.803546  ]\n",
      " [2.2050383  1.7526572  1.7526572  1.7526572 ]\n",
      " [2.1485817  1.6962006  1.6962006  1.6962006 ]\n",
      " [1.8321048  0.92734325 0.92734325 0.92734325]\n",
      " [1.7205824  0.81582063 0.81582063 0.81582063]\n",
      " [1.8693871  1.4558532  0.9646258  0.9646258 ]\n",
      " [1.7575932  1.3440593  0.8528312  0.8528312 ]\n",
      " [1.6518958  1.2383618  0.74713343 0.74713343]\n",
      " [1.5605233  1.1469893  0.6557613  0.6557613 ]\n",
      " [1.4879225  1.0743886  0.58316034 0.58316034]\n",
      " [1.4307406  1.017207   0.525979   0.525979  ]\n",
      " [1.3705842  0.9570504  0.46582285 0.46582285]\n",
      " [1.2167139  0.7643331  0.7643331  0.7643331 ]\n",
      " [1.1230712  0.6706906  0.6706906  0.6706906 ]\n",
      " [1.0459336  0.5935523  0.5935523  0.5935523 ]\n",
      " [0.9870953  0.5347143  0.5347143  0.5347143 ]\n",
      " [0.93069625 0.47831535 0.47831535 0.47831535]\n",
      " [0.8451213  0.39274046 0.39274046 0.39274046]\n",
      " [0.72990894 0.2775279  0.2775279  0.2775279 ]]\n",
      "Iteration: 51\n",
      "Iteration 51, Loss: 0.1318691521883011, Grad_X_e: [[2.4647017  2.0123208  2.0123208  2.0123208 ]\n",
      " [2.431515   1.9791342  1.9791342  1.9791342 ]\n",
      " [2.3960857  1.9437048  1.9437048  1.9437048 ]\n",
      " [2.441507   2.4715824  1.9891261  1.9891261 ]\n",
      " [2.3140576  1.8616768  1.8616768  1.8616768 ]\n",
      " [2.2659373  1.8135558  1.8135558  1.8135558 ]\n",
      " [2.212824   1.7604431  1.7604431  1.7604431 ]\n",
      " [2.1540284  1.7016473  1.7016473  1.7016473 ]\n",
      " [1.8252397  0.9204781  0.9204781  0.9204781 ]\n",
      " [1.716116   0.8113542  0.8113542  0.8113542 ]\n",
      " [1.862119   1.448585   0.9573575  0.9573575 ]\n",
      " [1.7521544  1.3386205  0.8473924  0.8473924 ]\n",
      " [1.6496245  1.2360905  0.7448622  0.7448622 ]\n",
      " [1.5622073  1.1486734  0.65744543 0.65744543]\n",
      " [1.4927856  1.0792516  0.58802336 0.58802336]\n",
      " [1.4360623  1.0225288  0.53130066 0.53130066]\n",
      " [1.3719393  0.95840544 0.4671779  0.4671779 ]\n",
      " [1.2138253  0.7614445  0.7614445  0.7614445 ]\n",
      " [1.1239829  0.67160225 0.67160225 0.67160225]\n",
      " [1.0503272  0.5979458  0.5979458  0.5979458 ]\n",
      " [0.99266255 0.54028153 0.54028153 0.54028153]\n",
      " [0.9327797  0.48039883 0.48039883 0.48039883]\n",
      " [0.8415169  0.38913605 0.38913605 0.38913605]\n",
      " [0.72909254 0.2767115  0.2767115  0.2767115 ]]\n",
      "Iteration: 52\n",
      "Iteration 52, Loss: 0.1318691521883011, Grad_X_e: [[2.483043   2.030662   2.030662   2.030662  ]\n",
      " [2.4482708  1.9958899  1.9958899  1.9958899 ]\n",
      " [2.411111   1.9587302  1.9587302  1.9587302 ]\n",
      " [2.4587476  2.488823   2.0063667  2.0063667 ]\n",
      " [2.3252285  1.8728477  1.8728477  1.8728477 ]\n",
      " [2.2750127  1.8226312  1.8226312  1.8226312 ]\n",
      " [2.2196903  1.7673094  1.7673094  1.7673094 ]\n",
      " [2.1585884  1.7062072  1.7062072  1.7062072 ]\n",
      " [1.819039   0.9142775  0.9142775  0.9142775 ]\n",
      " [1.7125248  0.80776304 0.80776304 0.80776304]\n",
      " [1.8554319  1.441898   0.95067054 0.95067054]\n",
      " [1.7475327  1.3339989  0.84277076 0.84277076]\n",
      " [1.6482723  1.2347383  0.74350995 0.74350995]\n",
      " [1.5646361  1.1511022  0.6598742  0.6598742 ]\n",
      " [1.4978731  1.0843391  0.5931109  0.5931109 ]\n",
      " [1.4408382  1.0273046  0.5360765  0.5360765 ]\n",
      " [1.3723083  0.95877445 0.46754688 0.46754688]\n",
      " [1.2118592  0.7594784  0.7594784  0.7594784 ]\n",
      " [1.1256905  0.6733098  0.6733098  0.6733098 ]\n",
      " [1.0550816  0.60270023 0.60270023 0.60270023]\n",
      " [0.9978361  0.5454551  0.5454551  0.5454551 ]\n",
      " [0.93388414 0.48150325 0.48150325 0.48150325]\n",
      " [0.8375824  0.3852015  0.3852015  0.3852015 ]\n",
      " [0.72928274 0.2769017  0.2769017  0.2769017 ]]\n",
      "Iteration: 53\n",
      "Iteration 53, Loss: 0.1318691521883011, Grad_X_e: [[2.5005233  2.0481424  2.0481424  2.0481424 ]\n",
      " [2.4641461  2.0117652  2.0117652  2.0117652 ]\n",
      " [2.42524    1.9728591  1.9728591  1.9728591 ]\n",
      " [2.4751146  2.50519    2.0227337  2.0227337 ]\n",
      " [2.3354895  1.8831086  1.8831086  1.8831086 ]\n",
      " [2.283187   1.8308055  1.8308055  1.8308055 ]\n",
      " [2.2256768  1.7732958  1.7732958  1.7732958 ]\n",
      " [2.1623087  1.7099274  1.7099274  1.7099274 ]\n",
      " [1.8135356  0.9087741  0.9087741  0.9087741 ]\n",
      " [1.7097946  0.8050329  0.8050329  0.8050329 ]\n",
      " [1.8493687  1.4358348  0.94460726 0.94460726]\n",
      " [1.7437322  1.3301984  0.8389702  0.8389702 ]\n",
      " [1.6477844  1.2342504  0.743022   0.743022  ]\n",
      " [1.5676929  1.154159   0.662931   0.662931  ]\n",
      " [1.5030355  1.0895016  0.59827334 0.59827334]\n",
      " [1.4449629  1.0314293  0.5402011  0.5402011 ]\n",
      " [1.3717556  0.95822185 0.4669943  0.4669943 ]\n",
      " [1.2107718  0.75839096 0.75839096 0.75839096]\n",
      " [1.1280905  0.67570984 0.67570984 0.67570984]\n",
      " [1.0600501  0.6076688  0.6076688  0.6076688 ]\n",
      " [1.0024927  0.5501116  0.5501116  0.5501116 ]\n",
      " [0.9340493  0.4816684  0.4816684  0.4816684 ]\n",
      " [0.83352685 0.38114598 0.38114598 0.38114598]\n",
      " [0.7303767  0.27799565 0.27799565 0.27799565]]\n",
      "Iteration: 54\n",
      "Iteration 54, Loss: 0.1318691521883011, Grad_X_e: [[2.5171573  2.0647764  2.0647764  2.0647764 ]\n",
      " [2.4791572  2.0267763  2.0267763  2.0267763 ]\n",
      " [2.4384925  1.9861116  1.9861116  1.9861116 ]\n",
      " [2.4906216  2.5206969  2.0382407  2.0382407 ]\n",
      " [2.3448694  1.8924886  1.8924886  1.8924886 ]\n",
      " [2.290495   1.8381135  1.8381135  1.8381135 ]\n",
      " [2.2308257  1.7784445  1.7784445  1.7784445 ]\n",
      " [2.1652377  1.7128563  1.7128563  1.7128563 ]\n",
      " [1.8087506  0.9039892  0.9039892  0.9039892 ]\n",
      " [1.7078989  0.8031371  0.8031371  0.8031371 ]\n",
      " [1.8439612  1.4304273  0.9391998  0.9391998 ]\n",
      " [1.7407466  1.3272128  0.8359845  0.8359845 ]\n",
      " [1.6480988  1.2345649  0.74333644 0.74333644]\n",
      " [1.5712582  1.1577243  0.66649634 0.66649634]\n",
      " [1.5081345  1.0946006  0.6033723  0.6033723 ]\n",
      " [1.448365   1.0348314  0.54360324 0.54360324]\n",
      " [1.3703841  0.9568504  0.46562284 0.46562284]\n",
      " [1.2105114  0.7581306  0.7581306  0.7581306 ]\n",
      " [1.1310742  0.6786935  0.6786935  0.6786935 ]\n",
      " [1.0650927  0.61271137 0.61271137 0.61271137]\n",
      " [1.0065346  0.55415344 0.55415344 0.55415344]\n",
      " [0.9333505  0.48096958 0.48096958 0.48096958]\n",
      " [0.8295573  0.37717643 0.37717643 0.37717643]\n",
      " [0.73223037 0.2798493  0.2798493  0.2798493 ]]\n",
      "Iteration: 55\n",
      "Iteration 55, Loss: 0.1318691521883011, Grad_X_e: [[2.532957   2.0805762  2.0805762  2.0805762 ]\n",
      " [2.4933224  2.0409415  2.0409415  2.0409415 ]\n",
      " [2.4508893  1.9985085  1.9985085  1.9985085 ]\n",
      " [2.5052845  2.5353599  2.0529037  2.0529037 ]\n",
      " [2.3533993  1.9010185  1.9010185  1.9010185 ]\n",
      " [2.2969725  1.8445913  1.8445913  1.8445913 ]\n",
      " [2.235179   1.7827978  1.7827978  1.7827978 ]\n",
      " [2.1674235  1.7150422  1.7150422  1.7150422 ]\n",
      " [1.8046936  0.89993215 0.89993215 0.89993215]\n",
      " [1.7067989  0.80203724 0.80203724 0.80203724]\n",
      " [1.8392291  1.4256952  0.93446773 0.93446773]\n",
      " [1.738554   1.3250202  0.8337919  0.8337919 ]\n",
      " [1.649143   1.235609   0.7443806  0.7443806 ]\n",
      " [1.5752116  1.1616777  0.6704498  0.6704498 ]\n",
      " [1.5130442  1.0995103  0.60828197 0.60828197]\n",
      " [1.4509957  1.0374621  0.5462339  0.5462339 ]\n",
      " [1.3683137  0.95478004 0.46355247 0.46355247]\n",
      " [1.2110138  0.75863296 0.75863296 0.75863296]\n",
      " [1.1345294  0.68214864 0.68214864 0.68214864]\n",
      " [1.0700771  0.6176958  0.6176958  0.6176958 ]\n",
      " [1.0098882  0.557507   0.557507   0.557507  ]\n",
      " [0.93188524 0.47950435 0.47950435 0.47950435]\n",
      " [0.82585627 0.3734754  0.3734754  0.3734754 ]\n",
      " [0.7346545  0.28227344 0.28227344 0.28227344]]\n",
      "Iteration: 56\n",
      "Iteration 56, Loss: 0.1318691521883011, Grad_X_e: [[2.5479379  2.095557   2.095557   2.095557  ]\n",
      " [2.5066595  2.0542786  2.0542786  2.0542786 ]\n",
      " [2.4624522  2.0100713  2.0100713  2.0100713 ]\n",
      " [2.519122   2.5491972  2.066741   2.066741  ]\n",
      " [2.36111    1.9087293  1.9087293  1.9087293 ]\n",
      " [2.3026564  1.850275   1.850275   1.850275  ]\n",
      " [2.2387788  1.7863976  1.7863976  1.7863976 ]\n",
      " [2.168916   1.7165347  1.7165347  1.7165347 ]\n",
      " [1.8013637  0.89660233 0.89660233 0.89660233]\n",
      " [1.7064453  0.8016837  0.8016837  0.8016837 ]\n",
      " [1.8351831  1.4216492  0.9304217  0.9304217 ]\n",
      " [1.737124   1.3235902  0.83236194 0.83236194]\n",
      " [1.6508343  1.2373004  0.74607193 0.74607193]\n",
      " [1.5794339  1.1659     0.67467207 0.67467207]\n",
      " [1.5176506  1.1041167  0.6128884  0.6128884 ]\n",
      " [1.4528333  1.0392997  0.54807144 0.54807144]\n",
      " [1.365688   0.9521544  0.4609268  0.4609268 ]\n",
      " [1.2122043  0.7598235  0.7598235  0.7598235 ]\n",
      " [1.1383427  0.68596196 0.68596196 0.68596196]\n",
      " [1.0748812  0.6224999  0.6224999  0.6224999 ]\n",
      " [1.0125108  0.56012964 0.56012964 0.56012964]\n",
      " [0.92977566 0.4773948  0.4773948  0.4773948 ]\n",
      " [0.82258373 0.37020287 0.37020287 0.37020287]\n",
      " [0.73743683 0.2850558  0.2850558  0.2850558 ]]\n",
      "Iteration: 57\n",
      "Iteration 57, Loss: 0.1318691521883011, Grad_X_e: [[2.562117   2.1097362  2.1097362  2.1097362 ]\n",
      " [2.5191865  2.0668056  2.0668056  2.0668056 ]\n",
      " [2.4732037  2.0208228  2.0208228  2.0208228 ]\n",
      " [2.5321517  2.562227   2.0797708  2.0797708 ]\n",
      " [2.3680336  1.915653   1.915653   1.915653  ]\n",
      " [2.3075833  1.8552018  1.8552018  1.8552018 ]\n",
      " [2.2416682  1.789287   1.789287   1.789287  ]\n",
      " [2.169763   1.7173818  1.7173818  1.7173818 ]\n",
      " [1.7987485  0.89398706 0.89398706 0.89398706]\n",
      " [1.70678    0.80201834 0.80201834 0.80201834]\n",
      " [1.8318228  1.4182888  0.9270613  0.9270613 ]\n",
      " [1.7364153  1.3228815  0.8316532  0.8316532 ]\n",
      " [1.6530845  1.2395506  0.7483222  0.7483222 ]\n",
      " [1.5838064  1.1702725  0.6790446  0.6790446 ]\n",
      " [1.5218555  1.1083215  0.61709327 0.61709327]\n",
      " [1.4538814  1.0403478  0.54911953 0.54911953]\n",
      " [1.3626541  0.9491205  0.45789292 0.45789292]\n",
      " [1.2140025  0.7616217  0.7616217  0.7616217 ]\n",
      " [1.1424017  0.6900209  0.6900209  0.6900209 ]\n",
      " [1.0793941  0.6270127  0.6270127  0.6270127 ]\n",
      " [1.0143801  0.5619989  0.5619989  0.5619989 ]\n",
      " [0.92715585 0.47477502 0.47477502 0.47477502]\n",
      " [0.81986016 0.36747926 0.36747926 0.36747926]\n",
      " [0.7403559  0.28797483 0.28797483 0.28797483]]\n",
      "Iteration: 58\n",
      "Iteration 58, Loss: 0.1318691521883011, Grad_X_e: [[2.5755112  2.1231303  2.1231303  2.1231303 ]\n",
      " [2.5309246  2.0785437  2.0785437  2.0785437 ]\n",
      " [2.4831686  2.0307877  2.0307877  2.0307877 ]\n",
      " [2.5443935  2.5744689  2.0920126  2.0920126 ]\n",
      " [2.3742044  1.9218237  1.9218237  1.9218237 ]\n",
      " [2.3117924  1.859411   1.859411   1.859411  ]\n",
      " [2.2438915  1.7915101  1.7915101  1.7915101 ]\n",
      " [2.170016   1.7176349  1.7176349  1.7176349 ]\n",
      " [1.7968282  0.8920667  0.8920667  0.8920667 ]\n",
      " [1.7077432  0.8029815  0.8029815  0.8029815 ]\n",
      " [1.8291415  1.4156076  0.92438    0.92438   ]\n",
      " [1.7363791  1.3228453  0.83161706 0.83161706]\n",
      " [1.6558073  1.2422733  0.75104487 0.75104487]\n",
      " [1.588222   1.1746881  0.6834602  0.6834602 ]\n",
      " [1.5255828  1.1120489  0.6208206  0.6208206 ]\n",
      " [1.4541795  1.040646   0.54941773 0.54941773]\n",
      " [1.359374   0.9458404  0.45461285 0.45461285]\n",
      " [1.2163239  0.7639431  0.7639431  0.7639431 ]\n",
      " [1.1466006  0.69421977 0.69421977 0.69421977]\n",
      " [1.0835301  0.6311487  0.6311487  0.6311487 ]\n",
      " [1.0155042  0.56312305 0.56312305 0.56312305]\n",
      " [0.9241751  0.47179425 0.47179425 0.47179425]\n",
      " [0.81777865 0.36539775 0.36539775 0.36539775]\n",
      " [0.7432072  0.2908261  0.2908261  0.2908261 ]]\n",
      "Iteration: 59\n",
      "Iteration 59, Loss: 0.1318691521883011, Grad_X_e: [[2.5881374  2.1357565  2.1357565  2.1357565 ]\n",
      " [2.5418942  2.0895133  2.0895133  2.0895133 ]\n",
      " [2.4923723  2.0399914  2.0399914  2.0399914 ]\n",
      " [2.5558667  2.585942   2.1034858  2.1034858 ]\n",
      " [2.379655   1.9272742  1.9272742  1.9272742 ]\n",
      " [2.3153212  1.86294    1.86294    1.86294   ]\n",
      " [2.2454915  1.79311    1.79311    1.79311   ]\n",
      " [2.169723   1.7173419  1.7173419  1.7173419 ]\n",
      " [1.7955722  0.89081067 0.89081067 0.89081067]\n",
      " [1.7092618  0.80450016 0.80450016 0.80450016]\n",
      " [1.8271184  1.4135845  0.92235684 0.92235684]\n",
      " [1.7369586  1.3234248  0.83219653 0.83219653]\n",
      " [1.6589082  1.2453743  0.75414586 0.75414586]\n",
      " [1.5925729  1.179039   0.68781114 0.68781114]\n",
      " [1.5287695  1.1152356  0.6240073  0.6240073 ]\n",
      " [1.4537791  1.0402455  0.54901737 0.54901737]\n",
      " [1.3559971  0.9424634  0.45123586 0.45123586]\n",
      " [1.2190801  0.7666993  0.7666993  0.7666993 ]\n",
      " [1.1508334  0.69845253 0.69845253 0.69845253]\n",
      " [1.087212   0.6348306  0.6348306  0.6348306 ]\n",
      " [1.015908   0.56352687 0.56352687 0.56352687]\n",
      " [0.9209781  0.46859732 0.46859732 0.46859732]\n",
      " [0.81638795 0.36400706 0.36400706 0.36400706]\n",
      " [0.7457952  0.29341406 0.29341406 0.29341406]]\n",
      "Iteration: 60\n",
      "Iteration 60, Loss: 0.1318691521883011, Grad_X_e: [[2.6000144  2.1476336  2.1476336  2.1476336 ]\n",
      " [2.5521178  2.099737   2.099737   2.099737  ]\n",
      " [2.5008397  2.0484588  2.0484588  2.0484588 ]\n",
      " [2.5665922  2.5966675  2.1142113  2.1142113 ]\n",
      " [2.3844197  1.932039   1.932039   1.932039  ]\n",
      " [2.31821    1.8658286  1.8658286  1.8658286 ]\n",
      " [2.2465127  1.7941312  1.7941312  1.7941312 ]\n",
      " [2.1689327  1.7165514  1.7165514  1.7165514 ]\n",
      " [1.794944   0.89018255 0.89018255 0.89018255]\n",
      " [1.7112623  0.80650073 0.80650073 0.80650073]\n",
      " [1.8257277  1.4121938  0.9209662  0.9209662 ]\n",
      " [1.738092   1.3245581  0.83332986 0.83332986]\n",
      " [1.6622955  1.2487615  0.75753313 0.75753313]\n",
      " [1.5967662  1.1832323  0.69200444 0.69200444]\n",
      " [1.5313749  1.117841   0.62661266 0.62661266]\n",
      " [1.4527519  1.0392183  0.54799014 0.54799014]\n",
      " [1.3526686  0.939135   0.44790748 0.44790748]\n",
      " [1.2221829  0.7698021  0.7698021  0.7698021 ]\n",
      " [1.1550022  0.7026214  0.7026214  0.7026214 ]\n",
      " [1.0903805  0.6379992  0.6379992  0.6379992 ]\n",
      " [1.0156409  0.5632597  0.5632597  0.5632597 ]\n",
      " [0.9177136  0.4653328  0.4653328  0.4653328 ]\n",
      " [0.8157003  0.3633194  0.3633194  0.3633194 ]\n",
      " [0.7479587  0.29557756 0.29557756 0.29557756]]\n",
      "Iteration: 61\n",
      "Iteration 61, Loss: 0.1318691521883011, Grad_X_e: [[2.6111622  2.1587813  2.1587813  2.1587813 ]\n",
      " [2.561618   2.1092372  2.1092372  2.1092372 ]\n",
      " [2.5085971  2.0562162  2.0562162  2.0562162 ]\n",
      " [2.5765922  2.6066675  2.1242113  2.1242113 ]\n",
      " [2.3885334  1.9361527  1.9361527  1.9361527 ]\n",
      " [2.3204973  1.8681159  1.8681159  1.8681159 ]\n",
      " [2.2469969  1.7946155  1.7946155  1.7946155 ]\n",
      " [2.167692   1.7153107  1.7153107  1.7153107 ]\n",
      " [1.794899   0.8901375  0.8901375  0.8901375 ]\n",
      " [1.7136672  0.80890554 0.80890554 0.80890554]\n",
      " [1.8249365  1.4114026  0.92017496 0.92017496]\n",
      " [1.7397113  1.3261775  0.83494925 0.83494925]\n",
      " [1.6658767  1.2523428  0.7611144  0.7611144 ]\n",
      " [1.600718   1.1871841  0.6959563  0.6959563 ]\n",
      " [1.5333766  1.1198426  0.62861437 0.62861437]\n",
      " [1.4511855  1.0376519  0.5464237  0.5464237 ]\n",
      " [1.3495232  0.9359896  0.44476208 0.44476208]\n",
      " [1.225542   0.7731611  0.7731611  0.7731611 ]\n",
      " [1.159021   0.7066402  0.7066402  0.7066402 ]\n",
      " [1.0929981  0.6406168  0.6406168  0.6406168 ]\n",
      " [1.0147688  0.56238776 0.56238776 0.56238776]\n",
      " [0.9145147  0.46213397 0.46213397 0.46213397]\n",
      " [0.81568915 0.36330825 0.36330825 0.36330825]\n",
      " [0.74957526 0.2971941  0.2971941  0.2971941 ]]\n",
      "Iteration: 62\n",
      "Iteration 62, Loss: 0.1318691521883011, Grad_X_e: [[2.6216009  2.16922    2.16922    2.16922   ]\n",
      " [2.5704188  2.118038   2.118038   2.118038  ]\n",
      " [2.515672   2.063291   2.063291   2.063291  ]\n",
      " [2.5858889  2.6159642  2.133508   2.133508  ]\n",
      " [2.392031   1.9396503  1.9396503  1.9396503 ]\n",
      " [2.3222225  1.869841   1.869841   1.869841  ]\n",
      " [2.2469888  1.7946073  1.7946073  1.7946073 ]\n",
      " [2.166047   1.7136658  1.7136658  1.7136658 ]\n",
      " [1.7953887  0.8906272  0.8906272  0.8906272 ]\n",
      " [1.7163998  0.81163824 0.81163824 0.81163824]\n",
      " [1.8247062  1.4111723  0.9199446  0.9199446 ]\n",
      " [1.7417482  1.3282144  0.8369861  0.8369861 ]\n",
      " [1.6695653  1.2560314  0.764803   0.764803  ]\n",
      " [1.6043574  1.1908234  0.6995957  0.6995957 ]\n",
      " [1.5347717  1.1212378  0.6300095  0.6300095 ]\n",
      " [1.4491792  1.0356456  0.5444174  0.5444174 ]\n",
      " [1.3466752  0.93314165 0.44191408 0.44191408]\n",
      " [1.2290704  0.77668965 0.77668965 0.77668965]\n",
      " [1.1628122  0.7104314  0.7104314  0.7104314 ]\n",
      " [1.0950395  0.6426581  0.6426581  0.6426581 ]\n",
      " [1.0133709  0.5609898  0.5609898  0.5609898 ]\n",
      " [0.9115085  0.45912772 0.45912772 0.45912772]\n",
      " [0.8162967  0.3639158  0.3639158  0.3639158 ]\n",
      " [0.7505782  0.29819706 0.29819706 0.29819706]]\n",
      "Iteration: 63\n",
      "Iteration 63, Loss: 0.1318691521883011, Grad_X_e: [[2.6313522  2.1789713  2.1789713  2.1789713 ]\n",
      " [2.5785441  2.1261632  2.1261632  2.1261632 ]\n",
      " [2.5220914  2.0697105  2.0697105  2.0697105 ]\n",
      " [2.5945053  2.6245806  2.1421244  2.1421244 ]\n",
      " [2.394947   1.9425665  1.9425665  1.9425665 ]\n",
      " [2.3234236  1.8710421  1.8710421  1.8710421 ]\n",
      " [2.2465303  1.7941487  1.7941487  1.7941487 ]\n",
      " [2.1640434  1.7116623  1.7116623  1.7116623 ]\n",
      " [1.7963601  0.8915986  0.8915986  0.8915986 ]\n",
      " [1.7193824  0.81462085 0.81462085 0.81462085]\n",
      " [1.8249915  1.4114575  0.9202299  0.9202299 ]\n",
      " [1.7441314  1.3305976  0.83936936 0.83936936]\n",
      " [1.6732783  1.2597444  0.76851606 0.76851606]\n",
      " [1.6076252  1.1940913  0.7028636  0.7028636 ]\n",
      " [1.5355729  1.122039   0.63081074 0.63081074]\n",
      " [1.4468386  1.033305   0.5420769  0.5420769 ]\n",
      " [1.3442186  0.93068504 0.4394575  0.4394575 ]\n",
      " [1.2326868  0.7803059  0.7803059  0.7803059 ]\n",
      " [1.1663095  0.71392864 0.71392864 0.71392864]\n",
      " [1.0965     0.6441186  0.6441186  0.6441186 ]\n",
      " [1.0115392  0.55915815 0.55915815 0.55915815]\n",
      " [0.90880054 0.45641977 0.45641977 0.45641977]\n",
      " [0.817441   0.3650601  0.3650601  0.3650601 ]\n",
      " [0.750953   0.29857185 0.29857185 0.29857185]]\n",
      "Iteration: 64\n",
      "Iteration 64, Loss: 0.1318691521883011, Grad_X_e: [[2.640437   2.188056   2.188056   2.188056  ]\n",
      " [2.5860178  2.133637   2.133637   2.133637  ]\n",
      " [2.5278833  2.0755024  2.0755024  2.0755024 ]\n",
      " [2.6024644  2.6325397  2.1500835  2.1500835 ]\n",
      " [2.397317   1.9449365  1.9449365  1.9449365 ]\n",
      " [2.324138   1.8717564  1.8717564  1.8717564 ]\n",
      " [2.2456627  1.7932812  1.7932812  1.7932812 ]\n",
      " [2.1617239  1.7093428  1.7093428  1.7093428 ]\n",
      " [1.7977557  0.8929941  0.8929941  0.8929941 ]\n",
      " [1.722538   0.8177765  0.8177765  0.8177765 ]\n",
      " [1.8257432  1.4122093  0.9209817  0.9209817 ]\n",
      " [1.7467909  1.3332571  0.8420288  0.8420288 ]\n",
      " [1.6769376  1.2634037  0.7721753  0.7721753 ]\n",
      " [1.6104735  1.1969396  0.7057118  0.7057118 ]\n",
      " [1.5358075  1.1222736  0.63104534 0.63104534]\n",
      " [1.4442706  1.030737   0.5395089  0.5395089 ]\n",
      " [1.3422226  0.92868894 0.4374614  0.4374614 ]\n",
      " [1.2363099  0.78392905 0.78392905 0.78392905]\n",
      " [1.1694578  0.7170769  0.7170769  0.7170769 ]\n",
      " [1.0973903  0.6450088  0.6450088  0.6450088 ]\n",
      " [1.0093693  0.5569882  0.5569882  0.5569882 ]\n",
      " [0.9064731  0.4540923  0.4540923  0.4540923 ]\n",
      " [0.8190149  0.36663404 0.36663404 0.36663404]\n",
      " [0.7507196  0.2983384  0.2983384  0.2983384 ]]\n",
      "Iteration: 65\n",
      "Iteration 65, Loss: 0.1318691521883011, Grad_X_e: [[2.6488767  2.1964958  2.1964958  2.1964958 ]\n",
      " [2.5928652  2.1404843  2.1404843  2.1404843 ]\n",
      " [2.5330758  2.080695   2.080695   2.080695  ]\n",
      " [2.609791   2.6398664  2.1574101  2.1574101 ]\n",
      " [2.3991756  1.9467951  1.9467951  1.9467951 ]\n",
      " [2.3244047  1.8720232  1.8720232  1.8720232 ]\n",
      " [2.2444263  1.7920449  1.7920449  1.7920449 ]\n",
      " [2.1591318  1.7067508  1.7067508  1.7067508 ]\n",
      " [1.7995174  0.8947557  0.8947557  0.8947557 ]\n",
      " [1.7257951  0.8210336  0.8210336  0.8210336 ]\n",
      " [1.8269107  1.4133768  0.9221492  0.9221492 ]\n",
      " [1.7496554  1.3361216  0.8448932  0.8448932 ]\n",
      " [1.6804737  1.2669398  0.7757114  0.7757114 ]\n",
      " [1.612871   1.1993371  0.70810926 0.70810926]\n",
      " [1.5355189  1.121985   0.6307567  0.6307567 ]\n",
      " [1.4415846  1.028051   0.5368229  0.5368229 ]\n",
      " [1.340737   0.9272034  0.43597588 0.43597588]\n",
      " [1.2398691  0.7874882  0.7874882  0.7874882 ]\n",
      " [1.1722137  0.71983284 0.71983284 0.71983284]\n",
      " [1.097735   0.6453535  0.6453535  0.6453535 ]\n",
      " [1.0069628  0.55458176 0.55458176 0.55458176]\n",
      " [0.9045944  0.45221364 0.45221364 0.45221364]\n",
      " [0.8209005  0.3685196  0.3685196  0.3685196 ]\n",
      " [0.74995077 0.29756954 0.29756954 0.29756954]]\n",
      "Iteration: 66\n",
      "Iteration 66, Loss: 0.1318691521883011, Grad_X_e: [[2.6566937  2.2043128  2.2043128  2.2043128 ]\n",
      " [2.5991108  2.14673    2.14673    2.14673   ]\n",
      " [2.5376964  2.0853155  2.0853155  2.0853155 ]\n",
      " [2.6165085  2.646584   2.1641278  2.1641278 ]\n",
      " [2.4005566  1.948176   1.948176   1.948176  ]\n",
      " [2.3242598  1.8718781  1.8718781  1.8718781 ]\n",
      " [2.2428591  1.7904776  1.7904776  1.7904776 ]\n",
      " [2.1563072  1.7039261  1.7039261  1.7039261 ]\n",
      " [1.8015835  0.89682186 0.89682186 0.89682186]\n",
      " [1.7290834  0.82432187 0.82432187 0.82432187]\n",
      " [1.8284385  1.4149046  0.923677   0.923677  ]\n",
      " [1.752656   1.3391222  0.8478939  0.8478939 ]\n",
      " [1.6838245  1.2702906  0.7790623  0.7790623 ]\n",
      " [1.6147968  1.2012628  0.71003497 0.71003497]\n",
      " [1.5347574  1.1212234  0.6299951  0.6299951 ]\n",
      " [1.4388814  1.0253478  0.5341198  0.5341198 ]\n",
      " [1.3397762  0.9262426  0.43501505 0.43501505]\n",
      " [1.2432971  0.7909162  0.7909162  0.7909162 ]\n",
      " [1.1745447  0.7221638  0.7221638  0.7221638 ]\n",
      " [1.0975709  0.6451894  0.6451894  0.6451894 ]\n",
      " [1.0044197  0.55203867 0.55203867 0.55203867]\n",
      " [0.90320224 0.45082146 0.45082146 0.45082146]\n",
      " [0.8229642  0.37058333 0.37058333 0.37058333]\n",
      " [0.7487441  0.29636288 0.29636288 0.29636288]]\n",
      "Iteration: 67\n",
      "Iteration 67, Loss: 0.1318691521883011, Grad_X_e: [[2.6639113  2.2115304  2.2115304  2.2115304 ]\n",
      " [2.6047802  2.1523993  2.1523993  2.1523993 ]\n",
      " [2.5417736  2.0893927  2.0893927  2.0893927 ]\n",
      " [2.6226423  2.6527178  2.1702616  2.1702616 ]\n",
      " [2.4014943  1.9491138  1.9491138  1.9491138 ]\n",
      " [2.3237395  1.8713579  1.8713579  1.8713579 ]\n",
      " [2.241001   1.7886195  1.7886195  1.7886195 ]\n",
      " [2.15329    1.7009088  1.7009088  1.7009088 ]\n",
      " [1.8038962  0.8991345  0.8991345  0.8991345 ]\n",
      " [1.7323399  0.8275783  0.8275783  0.8275783 ]\n",
      " [1.8302739  1.41674    0.9255124  0.9255124 ]\n",
      " [1.7557288  1.342195   0.8509667  0.8509667 ]\n",
      " [1.6869377  1.2734038  0.7821754  0.7821754 ]\n",
      " [1.616246   1.202712   0.71148413 0.71148413]\n",
      " [1.533588   1.1200541  0.6288258  0.6288258 ]\n",
      " [1.4362589  1.0227253  0.5314973  0.5314973 ]\n",
      " [1.3393413  0.9258078  0.4345802  0.4345802 ]\n",
      " [1.2465374  0.7941566  0.7941566  0.7941566 ]\n",
      " [1.1764349  0.7240539  0.7240539  0.7240539 ]\n",
      " [1.0969471  0.6445656  0.6445656  0.6445656 ]\n",
      " [1.0018398  0.5494588  0.5494588  0.5494588 ]\n",
      " [0.9023157  0.44993493 0.44993493 0.44993493]\n",
      " [0.8250858  0.37270495 0.37270495 0.37270495]\n",
      " [0.74723434 0.29485318 0.29485318 0.29485318]]\n",
      "Iteration: 68\n",
      "Iteration 68, Loss: 0.1318691521883011, Grad_X_e: [[2.670552   2.2181711  2.2181711  2.2181711 ]\n",
      " [2.6098986  2.1575177  2.1575177  2.1575177 ]\n",
      " [2.5453353  2.0929544  2.0929544  2.0929544 ]\n",
      " [2.6282167  2.6582923  2.175836   2.175836  ]\n",
      " [2.402022   1.9496416  1.9496416  1.9496416 ]\n",
      " [2.3228798  1.8704982  1.8704982  1.8704982 ]\n",
      " [2.2388875  1.7865062  1.7865062  1.7865062 ]\n",
      " [2.150116   1.6977346  1.6977346  1.6977346 ]\n",
      " [1.806395   0.90163344 0.90163344 0.90163344]\n",
      " [1.7355038  0.8307422  0.8307422  0.8307422 ]\n",
      " [1.83236    1.4188261  0.92759854 0.92759854]\n",
      " [1.7588114  1.3452775  0.85404927 0.85404927]\n",
      " [1.6897686  1.2762346  0.7850063  0.7850063 ]\n",
      " [1.617222   1.203688   0.7124601  0.7124601 ]\n",
      " [1.5320787  1.1185448  0.6273165  0.6273165 ]\n",
      " [1.4338006  1.020267   0.52903897 0.52903897]\n",
      " [1.3394011  0.9258676  0.43464008 0.43464008]\n",
      " [1.2495382  0.79715735 0.79715735 0.79715735]\n",
      " [1.1778758  0.72549486 0.72549486 0.72549486]\n",
      " [1.0959202  0.6435387  0.6435387  0.6435387 ]\n",
      " [0.99931014 0.5469292  0.5469292  0.5469292 ]\n",
      " [0.9019268  0.44954607 0.44954607 0.44954607]\n",
      " [0.827142   0.37476116 0.37476116 0.37476116]\n",
      " [0.7455559  0.29317474 0.29317474 0.29317474]]\n",
      "Iteration: 69\n",
      "Iteration 69, Loss: 0.1318691521883011, Grad_X_e: [[2.6766388  2.224258   2.224258   2.224258  ]\n",
      " [2.6144917  2.1621108  2.1621108  2.1621108 ]\n",
      " [2.54841    2.096029   2.096029   2.096029  ]\n",
      " [2.6332564  2.663332   2.1808758  2.1808758 ]\n",
      " [2.402172   1.9497917  1.9497917  1.9497917 ]\n",
      " [2.3217154  1.8693337  1.8693337  1.8693337 ]\n",
      " [2.236555   1.7841736  1.7841736  1.7841736 ]\n",
      " [2.146821   1.6944395  1.6944395  1.6944395 ]\n",
      " [1.8090234  0.90426177 0.90426177 0.90426177]\n",
      " [1.7385228  0.8337611  0.8337611  0.8337611 ]\n",
      " [1.8346428  1.4211088  0.9298813  0.9298813 ]\n",
      " [1.7618471  1.3483133  0.857085   0.857085  ]\n",
      " [1.6922818  1.2787479  0.7875195  0.7875195 ]\n",
      " [1.6177427  1.2042087  0.71298075 0.71298075]\n",
      " [1.5303026  1.1167687  0.6255404  0.6255404 ]\n",
      " [1.4315819  1.0180483  0.52682024 0.52682024]\n",
      " [1.3399107  0.9263773  0.43514973 0.43514973]\n",
      " [1.2522608  0.79987997 0.79987997 0.79987997]\n",
      " [1.1788719  0.726491   0.726491   0.726491  ]\n",
      " [1.0945542  0.6421727  0.6421727  0.6421727 ]\n",
      " [0.99691427 0.5445333  0.5445333  0.5445333 ]\n",
      " [0.90201056 0.44962978 0.44962978 0.44962978]\n",
      " [0.8290268  0.37664598 0.37664598 0.37664598]\n",
      " [0.7438498  0.2914687  0.2914687  0.2914687 ]]\n",
      "Iteration: 70\n",
      "Iteration 70, Loss: 0.1318691521883011, Grad_X_e: [[2.682195   2.229814   2.229814   2.229814  ]\n",
      " [2.6185849  2.166204   2.166204   2.166204  ]\n",
      " [2.5510252  2.0986443  2.0986443  2.0986443 ]\n",
      " [2.6377861  2.6678617  2.1854055  2.1854055 ]\n",
      " [2.4019766  1.9495962  1.9495962  1.9495962 ]\n",
      " [2.3202796  1.867898   1.867898   1.867898  ]\n",
      " [2.2340372  1.7816557  1.7816557  1.7816557 ]\n",
      " [2.1434383  1.6910568  1.6910568  1.6910568 ]\n",
      " [1.8117265  0.9069649  0.9069649  0.9069649 ]\n",
      " [1.7413529  0.83659124 0.83659124 0.83659124]\n",
      " [1.8370686  1.4235346  0.93230706 0.93230706]\n",
      " [1.7647848  1.351251   0.86002266 0.86002266]\n",
      " [1.6944517  1.2809178  0.7896893  0.7896893 ]\n",
      " [1.6178359  1.204302   0.713074   0.713074  ]\n",
      " [1.5283353  1.1148014  0.62357306 0.62357306]\n",
      " [1.4296604  1.0161269  0.5248988  0.5248988 ]\n",
      " [1.3408105  0.9272771  0.4360495  0.4360495 ]\n",
      " [1.2546725  0.80229175 0.80229175 0.80229175]\n",
      " [1.1794382  0.72705734 0.72705734 0.72705734]\n",
      " [1.0929168  0.6405353  0.6405353  0.6405353 ]\n",
      " [0.9947242  0.54234326 0.54234326 0.54234326]\n",
      " [0.90252185 0.45014104 0.45014104 0.45014104]\n",
      " [0.83065283 0.37827203 0.37827203 0.37827203]\n",
      " [0.7422496  0.2898685  0.2898685  0.2898685 ]]\n",
      "Iteration: 71\n",
      "Iteration 71, Loss: 0.1318691521883011, Grad_X_e: [[2.687244   2.234863   2.234863   2.234863  ]\n",
      " [2.6222038  2.169823   2.169823   2.169823  ]\n",
      " [2.5532093  2.1008284  2.1008284  2.1008284 ]\n",
      " [2.6418312  2.6719067  2.1894505  2.1894505 ]\n",
      " [2.4014668  1.9490864  1.9490864  1.9490864 ]\n",
      " [2.3186047  1.866223   1.866223   1.866223  ]\n",
      " [2.2313664  1.7789849  1.7789849  1.7789849 ]\n",
      " [2.1399994  1.6876179  1.6876179  1.6876179 ]\n",
      " [1.8144534  0.9096918  0.9096918  0.9096918 ]\n",
      " [1.7439556  0.83919394 0.83919394 0.83919394]\n",
      " [1.8395875  1.4260535  0.93482596 0.93482596]\n",
      " [1.7675806  1.3540468  0.8628185  0.8628185 ]\n",
      " [1.6962632  1.2827293  0.7915008  0.7915008 ]\n",
      " [1.6175365  1.2040026  0.7127747  0.7127747 ]\n",
      " [1.5262545  1.1127206  0.62149227 0.62149227]\n",
      " [1.428083   1.0145494  0.5233214  0.5233214 ]\n",
      " [1.342027   0.9284935  0.43726587 0.43726587]\n",
      " [1.256753   0.80437225 0.80437225 0.80437225]\n",
      " [1.1795998  0.7272189  0.7272189  0.7272189 ]\n",
      " [1.0910811  0.6386996  0.6386996  0.6386996 ]\n",
      " [0.9928016  0.54042065 0.54042065 0.54042065]\n",
      " [0.90340525 0.4510244  0.4510244  0.4510244 ]\n",
      " [0.83196026 0.37957942 0.37957942 0.37957942]\n",
      " [0.74087095 0.28848985 0.28848985 0.28848985]]\n",
      "Iteration: 72\n",
      "Iteration 72, Loss: 0.1318691521883011, Grad_X_e: [[2.6918085  2.2394276  2.2394276  2.2394276 ]\n",
      " [2.6253736  2.1729927  2.1729927  2.1729927 ]\n",
      " [2.5549889  2.102608   2.102608   2.102608  ]\n",
      " [2.6454155  2.675491   2.193035   2.193035  ]\n",
      " [2.400673   1.9482924  1.9482924  1.9482924 ]\n",
      " [2.3167214  1.8643397  1.8643397  1.8643397 ]\n",
      " [2.2285717  1.7761902  1.7761902  1.7761902 ]\n",
      " [2.1365325  1.684151   1.684151   1.684151  ]\n",
      " [1.8171563  0.91239476 0.91239476 0.91239476]\n",
      " [1.7462997  0.8415381  0.8415381  0.8415381 ]\n",
      " [1.8421497  1.4286158  0.9373882  0.9373882 ]\n",
      " [1.7701932  1.3566594  0.86543113 0.86543113]\n",
      " [1.6977074  1.2841735  0.792945   0.792945  ]\n",
      " [1.6168871  1.2033532  0.7121253  0.7121253 ]\n",
      " [1.5241317  1.1105977  0.61936945 0.61936945]\n",
      " [1.4268738  1.0133402  0.5221122  0.5221122 ]\n",
      " [1.3434778  0.9299444  0.4387168  0.4387168 ]\n",
      " [1.2584859  0.80610526 0.80610526 0.80610526]\n",
      " [1.1793872  0.7270064  0.7270064  0.7270064 ]\n",
      " [1.0891155  0.6367339  0.6367339  0.6367339 ]\n",
      " [0.9911884  0.53880745 0.53880745 0.53880745]\n",
      " [0.9045874  0.45220655 0.45220655 0.45220655]\n",
      " [0.8328989  0.38051808 0.38051808 0.38051808]\n",
      " [0.73979515 0.287414   0.287414   0.287414  ]]\n",
      "Iteration: 73\n",
      "Iteration 73, Loss: 0.1318691521883011, Grad_X_e: [[2.6959114  2.2435305  2.2435305  2.2435305 ]\n",
      " [2.628119   2.175738   2.175738   2.175738  ]\n",
      " [2.5563908  2.1040099  2.1040099  2.1040099 ]\n",
      " [2.648564   2.6786397  2.1961834  2.1961834 ]\n",
      " [2.399624   1.9472436  1.9472436  1.9472436 ]\n",
      " [2.314659   1.8622774  1.8622774  1.8622774 ]\n",
      " [2.2256832  1.7733017  1.7733017  1.7733017 ]\n",
      " [2.1330638  1.6806823  1.6806823  1.6806823 ]\n",
      " [1.8197914  0.9150299  0.9150299  0.9150299 ]\n",
      " [1.748363   0.84360135 0.84360135 0.84360135]\n",
      " [1.8447088  1.4311749  0.93994725 0.93994725]\n",
      " [1.7725917  1.3590579  0.8678296  0.8678296 ]\n",
      " [1.6987861  1.2852522  0.79402375 0.79402375]\n",
      " [1.615936   1.2024021  0.71117425 0.71117425]\n",
      " [1.5220352  1.1085013  0.6172731  0.6172731 ]\n",
      " [1.4260452  1.0125116  0.52128357 0.52128357]\n",
      " [1.3450752  0.9315418  0.44031417 0.44031417]\n",
      " [1.2598664  0.80748564 0.80748564 0.80748564]\n",
      " [1.1788402  0.7264593  0.7264593  0.7264593 ]\n",
      " [1.0870894  0.6347078  0.6347078  0.6347078 ]\n",
      " [0.9899136  0.5375326  0.5375326  0.5375326 ]\n",
      " [0.9059907  0.45360985 0.45360985 0.45360985]\n",
      " [0.8334487  0.38106787 0.38106787 0.38106787]\n",
      " [0.73908263 0.28670153 0.28670153 0.28670153]]\n",
      "Iteration: 74\n",
      "Iteration 74, Loss: 0.1318691521883011, Grad_X_e: [[2.6995757  2.2471948  2.2471948  2.2471948 ]\n",
      " [2.630465   2.1780841  2.1780841  2.1780841 ]\n",
      " [2.557441   2.10506    2.10506    2.10506   ]\n",
      " [2.6513004  2.681376   2.1989198  2.1989198 ]\n",
      " [2.3983486  1.945968   1.945968   1.945968  ]\n",
      " [2.3124459  1.8600643  1.8600643  1.8600643 ]\n",
      " [2.2227283  1.7703468  1.7703468  1.7703468 ]\n",
      " [2.129618   1.6772366  1.6772366  1.6772366 ]\n",
      " [1.8223183  0.91755676 0.91755676 0.91755676]\n",
      " [1.7501284  0.84536666 0.84536666 0.84536666]\n",
      " [1.8472235  1.4336896  0.94246197 0.94246197]\n",
      " [1.774749   1.3612152  0.86998695 0.86998695]\n",
      " [1.6995069  1.285973   0.79474443 0.79474443]\n",
      " [1.6147312  1.2011973  0.7099694  0.7099694 ]\n",
      " [1.5200255  1.1064916  0.61526334 0.61526334]\n",
      " [1.4255948  1.0120612  0.5208332  0.5208332 ]\n",
      " [1.3467339  0.93320054 0.44197288 0.44197288]\n",
      " [1.2608942  0.8085134  0.8085134  0.8085134 ]\n",
      " [1.1780005  0.7256197  0.7256197  0.7256197 ]\n",
      " [1.0850669  0.6326853  0.6326853  0.6326853 ]\n",
      " [0.98899233 0.5366114  0.5366114  0.5366114 ]\n",
      " [0.9075315  0.45515066 0.45515066 0.45515066]\n",
      " [0.8336121  0.38123125 0.38123125 0.38123125]\n",
      " [0.73875636 0.28637528 0.28637528 0.28637528]]\n",
      "Iteration: 75\n",
      "Iteration 75, Loss: 0.1318691521883011, Grad_X_e: [[2.7028239  2.250443   2.250443   2.250443  ]\n",
      " [2.6324358  2.180055   2.180055   2.180055  ]\n",
      " [2.5581656  2.1057847  2.1057847  2.1057847 ]\n",
      " [2.6536486  2.6837242  2.201268   2.201268  ]\n",
      " [2.396873   1.9444925  1.9444925  1.9444925 ]\n",
      " [2.3101082  1.8577266  1.8577266  1.8577266 ]\n",
      " [2.219732   1.7673506  1.7673506  1.7673506 ]\n",
      " [2.1262178  1.6738365  1.6738365  1.6738365 ]\n",
      " [1.8247029  0.9199413  0.9199413  0.9199413 ]\n",
      " [1.7515866  0.8468248  0.8468248  0.8468248 ]\n",
      " [1.8496552  1.4361212  0.94489366 0.94489366]\n",
      " [1.7766457  1.3631119  0.8718836  0.8718836 ]\n",
      " [1.6998855  1.2863516  0.7951231  0.7951231 ]\n",
      " [1.6133262  1.1997923  0.70856434 0.70856434]\n",
      " [1.5181563  1.1046224  0.6133941  0.6133941 ]\n",
      " [1.425507   1.0119734  0.52074534 0.52074534]\n",
      " [1.3483715  0.9348381  0.44361043 0.44361043]\n",
      " [1.2615772  0.8091964  0.8091964  0.8091964 ]\n",
      " [1.1769136  0.7245328  0.7245328  0.7245328 ]\n",
      " [1.0831066  0.6307251  0.6307251  0.6307251 ]\n",
      " [0.9884254  0.5360444  0.5360444  0.5360444 ]\n",
      " [0.9091288  0.45674795 0.45674795 0.45674795]\n",
      " [0.83341223 0.38103142 0.38103142 0.38103142]\n",
      " [0.7388082  0.28642714 0.28642714 0.28642714]]\n",
      "Iteration: 76\n",
      "Iteration 76, Loss: 0.1318691521883011, Grad_X_e: [[2.7056782  2.2532973  2.2532973  2.2532973 ]\n",
      " [2.6340547  2.1816738  2.1816738  2.1816738 ]\n",
      " [2.5585892  2.1062083  2.1062083  2.1062083 ]\n",
      " [2.655632   2.6857076  2.2032514  2.2032514 ]\n",
      " [2.3952234  1.942843   1.942843   1.942843  ]\n",
      " [2.3076713  1.8552896  1.8552896  1.8552896 ]\n",
      " [2.216717   1.7643356  1.7643356  1.7643356 ]\n",
      " [2.1228845  1.6705031  1.6705031  1.6705031 ]\n",
      " [1.8269154  0.9221539  0.9221539  0.9221539 ]\n",
      " [1.7527354  0.8479737  0.8479737  0.8479737 ]\n",
      " [1.8519713  1.4384373  0.94720984 0.94720984]\n",
      " [1.778267   1.3647332  0.87350494 0.87350494]\n",
      " [1.699945   1.286411   0.7951825  0.7951825 ]\n",
      " [1.6117762  1.1982423  0.7070144  0.7070144 ]\n",
      " [1.5164742  1.1029403  0.6117121  0.6117121 ]\n",
      " [1.4257532  1.0122197  0.5209917  0.5209917 ]\n",
      " [1.3499165  0.93638307 0.44515535 0.44515535]\n",
      " [1.2619317  0.8095509  0.8095509  0.8095509 ]\n",
      " [1.1756294  0.7232485  0.7232485  0.7232485 ]\n",
      " [1.0812628  0.6288812  0.6288812  0.6288812 ]\n",
      " [0.98820174 0.5358208  0.5358208  0.5358208 ]\n",
      " [0.91070604 0.45832524 0.45832524 0.45832524]\n",
      " [0.83289444 0.3805136  0.3805136  0.3805136 ]\n",
      " [0.7392013  0.28682023 0.28682023 0.28682023]]\n",
      "Iteration: 77\n",
      "Iteration 77, Loss: 0.1318691521883011, Grad_X_e: [[2.7081614  2.2557805  2.2557805  2.2557805 ]\n",
      " [2.6353455  2.1829646  2.1829646  2.1829646 ]\n",
      " [2.5587366  2.1063557  2.1063557  2.1063557 ]\n",
      " [2.6572738  2.6873493  2.204893   2.204893  ]\n",
      " [2.393425   1.9410444  1.9410444  1.9410444 ]\n",
      " [2.305159   1.8527775  1.8527775  1.8527775 ]\n",
      " [2.2137065  1.761325   1.761325   1.761325  ]\n",
      " [2.1196365  1.6672552  1.6672552  1.6672552 ]\n",
      " [1.8289335  0.92417204 0.92417204 0.92417204]\n",
      " [1.7535805  0.8488188  0.8488188  0.8488188 ]\n",
      " [1.8541436  1.4406097  0.9493822  0.9493822 ]\n",
      " [1.7796086  1.3660748  0.8748465  0.8748465 ]\n",
      " [1.6997122  1.2861782  0.79494977 0.79494977]\n",
      " [1.6101345  1.1966006  0.70537263 0.70537263]\n",
      " [1.5150152  1.1014813  0.6102531  0.6102531 ]\n",
      " [1.4262974  1.0127639  0.5215358  0.5215358 ]\n",
      " [1.3513074  0.937774   0.44654632 0.44654632]\n",
      " [1.2619803  0.8095996  0.8095996  0.8095996 ]\n",
      " [1.1741982  0.7218172  0.7218172  0.7218172 ]\n",
      " [1.079578   0.62719643 0.62719643 0.62719643]\n",
      " [0.98830146 0.5359205  0.5359205  0.5359205 ]\n",
      " [0.91219515 0.45981437 0.45981437 0.45981437]\n",
      " [0.83211887 0.37973803 0.37973803 0.37973803]\n",
      " [0.7398796  0.28749856 0.28749856 0.28749856]]\n",
      "Iteration: 78\n",
      "Iteration 78, Loss: 0.1318691521883011, Grad_X_e: [[2.7102945  2.2579136  2.2579136  2.2579136 ]\n",
      " [2.6363306  2.1839497  2.1839497  2.1839497 ]\n",
      " [2.558631   2.10625    2.10625    2.10625   ]\n",
      " [2.6585958  2.6886714  2.2062151  2.2062151 ]\n",
      " [2.391501   1.9391204  1.9391204  1.9391204 ]\n",
      " [2.3025942  1.8502126  1.8502126  1.8502126 ]\n",
      " [2.21072    1.7583386  1.7583386  1.7583386 ]\n",
      " [2.1164904  1.6641091  1.6641091  1.6641091 ]\n",
      " [1.8307381  0.92597663 0.92597663 0.92597663]\n",
      " [1.7541294  0.8493678  0.8493678  0.8493678 ]\n",
      " [1.8561468  1.4426129  0.9513853  0.9513853 ]\n",
      " [1.7806679  1.3671341  0.8759059  0.8759059 ]\n",
      " [1.699216   1.2856821  0.7944536  0.7944536 ]\n",
      " [1.6084521  1.1949182  0.70369023 0.70369023]\n",
      " [1.5138035  1.1002696  0.6090413  0.6090413 ]\n",
      " [1.4270884  1.0135548  0.52232677 0.52232677]\n",
      " [1.3524879  0.9389545  0.44772682 0.44772682]\n",
      " [1.2617481  0.80936736 0.80936736 0.80936736]\n",
      " [1.1726685  0.7202875  0.7202875  0.7202875 ]\n",
      " [1.0780889  0.62570727 0.62570727 0.62570727]\n",
      " [0.988691   0.53631    0.53631    0.53631   ]\n",
      " [0.9135356  0.46115482 0.46115482 0.46115482]\n",
      " [0.8311487  0.37876788 0.37876788 0.37876788]\n",
      " [0.7407581  0.28837708 0.28837708 0.28837708]]\n",
      "Iteration: 79\n",
      "Iteration 79, Loss: 0.1318691521883011, Grad_X_e: [[2.7120993  2.2597184  2.2597184  2.2597184 ]\n",
      " [2.6370323  2.1846514  2.1846514  2.1846514 ]\n",
      " [2.5582955  2.1059146  2.1059146  2.1059146 ]\n",
      " [2.6596205  2.689696   2.2072399  2.2072399 ]\n",
      " [2.3894737  1.9370931  1.9370931  1.9370931 ]\n",
      " [2.2999973  1.8476158  1.8476158  1.8476158 ]\n",
      " [2.2077749  1.7553935  1.7553935  1.7553935 ]\n",
      " [2.1134603  1.661079   1.661079   1.661079  ]\n",
      " [1.8323174  0.92755586 0.92755586 0.92755586]\n",
      " [1.7543979  0.8496363  0.8496363  0.8496363 ]\n",
      " [1.8579614  1.4444275  0.9531999  0.9531999 ]\n",
      " [1.781449   1.3679152  0.8766869  0.8766869 ]\n",
      " [1.6984912  1.2849573  0.7937289  0.7937289 ]\n",
      " [1.6067773  1.1932434  0.7020155  0.7020155 ]\n",
      " [1.5128553  1.0993214  0.6080931  0.6080931 ]\n",
      " [1.4280777  1.0145441  0.5233161  0.5233161 ]\n",
      " [1.3534237  0.9398902  0.44866255 0.44866255]\n",
      " [1.2612637  0.8088831  0.8088831  0.8088831 ]\n",
      " [1.1710885  0.7187075  0.7187075  0.7187075 ]\n",
      " [1.0768248  0.6244431  0.6244431  0.6244431 ]\n",
      " [0.989331   0.53695005 0.53695005 0.53695005]\n",
      " [0.91467816 0.46229738 0.46229738 0.46229738]\n",
      " [0.8300583  0.37767747 0.37767747 0.37767747]\n",
      " [0.7417475  0.28936648 0.28936648 0.28936648]]\n",
      "Iteration: 80\n",
      "Iteration 80, Loss: 0.1318691521883011, Grad_X_e: [[2.7135963  2.2612154  2.2612154  2.2612154 ]\n",
      " [2.6374722  2.1850913  2.1850913  2.1850913 ]\n",
      " [2.5577517  2.1053708  2.1053708  2.1053708 ]\n",
      " [2.660369   2.6904445  2.2079883  2.2079883 ]\n",
      " [2.3873641  1.9349836  1.9349836  1.9349836 ]\n",
      " [2.2973874  1.845006   1.845006   1.845006  ]\n",
      " [2.2048883  1.752507   1.752507   1.752507  ]\n",
      " [2.110558   1.6581768  1.6581768  1.6581768 ]\n",
      " [1.8336619  0.9289004  0.9289004  0.9289004 ]\n",
      " [1.7544038  0.84964234 0.84964234 0.84964234]\n",
      " [1.859572   1.4460381  0.95481056 0.95481056]\n",
      " [1.7819587  1.3684249  0.87719667 0.87719667]\n",
      " [1.697574   1.2840401  0.79281163 0.79281163]\n",
      " [1.6051532  1.1916193  0.7003915  0.7003915 ]\n",
      " [1.512177   1.0986431  0.60741484 0.60741484]\n",
      " [1.4292065  1.0156729  0.52444494 0.52444494]\n",
      " [1.3540897  0.9405562  0.44932857 0.44932857]\n",
      " [1.260561   0.80818033 0.80818033 0.80818033]\n",
      " [1.1695024  0.71712136 0.71712136 0.71712136]\n",
      " [1.0758013  0.62341964 0.62341964 0.62341964]\n",
      " [0.99017525 0.5377943  0.5377943  0.5377943 ]\n",
      " [0.9155819  0.46320108 0.46320108 0.46320108]\n",
      " [0.82892126 0.37654042 0.37654042 0.37654042]\n",
      " [0.7427507  0.29036966 0.29036966 0.29036966]]\n",
      "Iteration: 81\n",
      "Iteration 81, Loss: 0.1318691521883011, Grad_X_e: [[2.7148056  2.2624247  2.2624247  2.2624247 ]\n",
      " [2.6376712  2.1852903  2.1852903  2.1852903 ]\n",
      " [2.55702    2.104639   2.104639   2.104639  ]\n",
      " [2.6608622  2.6909378  2.2084816  2.2084816 ]\n",
      " [2.3851924  1.9328119  1.9328119  1.9328119 ]\n",
      " [2.2947826  1.8424013  1.8424013  1.8424013 ]\n",
      " [2.2020748  1.7496933  1.7496933  1.7496933 ]\n",
      " [2.1077948  1.6554134  1.6554134  1.6554134 ]\n",
      " [1.8347673  0.9300058  0.9300058  0.9300058 ]\n",
      " [1.7541705  0.84940904 0.84940904 0.84940904]\n",
      " [1.8609693  1.4474354  0.95620775 0.95620775]\n",
      " [1.7822139  1.3686801  0.8774519  0.8774519 ]\n",
      " [1.6965027  1.2829688  0.7917403  0.7917403 ]\n",
      " [1.6036206  1.1900867  0.6988589  0.6988589 ]\n",
      " [1.5117687  1.0982348  0.60700655 0.60700655]\n",
      " [1.4304192  1.0168856  0.5256577  0.5256577 ]\n",
      " [1.3544787  0.94094515 0.44971752 0.44971752]\n",
      " [1.2596748  0.80729413 0.80729413 0.80729413]\n",
      " [1.167952   0.71557087 0.71557087 0.71557087]\n",
      " [1.0750316  0.62264997 0.62264997 0.62264997]\n",
      " [0.9911744  0.53879344 0.53879344 0.53879344]\n",
      " [0.9162331  0.4638523  0.4638523  0.4638523 ]\n",
      " [0.82781345 0.37543264 0.37543264 0.37543264]\n",
      " [0.7436901  0.29130906 0.29130906 0.29130906]]\n",
      "Iteration: 82\n",
      "Iteration 82, Loss: 0.1318691521883011, Grad_X_e: [[2.7157476  2.2633667  2.2633667  2.2633667 ]\n",
      " [2.6376505  2.1852696  2.1852696  2.1852696 ]\n",
      " [2.5561218  2.103741   2.103741   2.103741  ]\n",
      " [2.6611204  2.691196   2.2087398  2.2087398 ]\n",
      " [2.3829763  1.9305956  1.9305956  1.9305956 ]\n",
      " [2.2921991  1.8398178  1.8398178  1.8398178 ]\n",
      " [2.1993477  1.7469661  1.7469661  1.7469661 ]\n",
      " [2.1051786  1.6527971  1.6527971  1.6527971 ]\n",
      " [1.8356361  0.9308746  0.9308746  0.9308746 ]\n",
      " [1.753725   0.8489635  0.8489635  0.8489635 ]\n",
      " [1.8621476  1.4486136  0.957386   0.957386  ]\n",
      " [1.7822316  1.3686978  0.87746954 0.87746954]\n",
      " [1.6953168  1.2817829  0.79055434 0.79055434]\n",
      " [1.602212   1.188678   0.6974502  0.6974502 ]\n",
      " [1.5116185  1.0980846  0.6068563  0.6068563 ]\n",
      " [1.431665   1.0181314  0.5269034  0.5269034 ]\n",
      " [1.3545948  0.9410613  0.44983366 0.44983366]\n",
      " [1.2586424  0.8062617  0.8062617  0.8062617 ]\n",
      " [1.1664742  0.71409315 0.71409315 0.71409315]\n",
      " [1.0745164  0.6221347  0.6221347  0.6221347 ]\n",
      " [0.992278   0.539897   0.539897   0.539897  ]\n",
      " [0.9166191  0.46423826 0.46423826 0.46423826]\n",
      " [0.8267933  0.37441254 0.37441254 0.37441254]\n",
      " [0.74449444 0.29211342 0.29211342 0.29211342]]\n",
      "Iteration: 83\n",
      "Iteration 83, Loss: 0.1318691521883011, Grad_X_e: [[2.7164423  2.2640615  2.2640615  2.2640615 ]\n",
      " [2.6374295  2.1850486  2.1850486  2.1850486 ]\n",
      " [2.5550766  2.1026957  2.1026957  2.1026957 ]\n",
      " [2.6611636  2.691239   2.208783   2.208783  ]\n",
      " [2.3807337  1.9283531  1.9283531  1.9283531 ]\n",
      " [2.2896528  1.8372713  1.8372713  1.8372713 ]\n",
      " [2.1967192  1.7443377  1.7443377  1.7443377 ]\n",
      " [2.1027174  1.650336   1.650336   1.650336  ]\n",
      " [1.836274   0.9315125  0.9315125  0.9315125 ]\n",
      " [1.753096   0.84833443 0.84833443 0.84833443]\n",
      " [1.8631067  1.4495728  0.9583452  0.9583452 ]\n",
      " [1.7820327  1.3684989  0.8772707  0.8772707 ]\n",
      " [1.6940546  1.2805207  0.78929216 0.78929216]\n",
      " [1.600957   1.1874231  0.6961953  0.6961953 ]\n",
      " [1.511711   1.0981771  0.6069488  0.6069488 ]\n",
      " [1.4328912  1.0193577  0.5281297  0.5281297 ]\n",
      " [1.3544625  0.94092894 0.44970128 0.44970128]\n",
      " [1.2575     0.80511934 0.80511934 0.80511934]\n",
      " [1.1651046  0.71272355 0.71272355 0.71272355]\n",
      " [1.0742505  0.62186867 0.62186867 0.62186867]\n",
      " [0.99343807 0.5410571  0.5410571  0.5410571 ]\n",
      " [0.9167502  0.46436936 0.46436936 0.46436936]\n",
      " [0.8259202  0.37353942 0.37353942 0.37353942]\n",
      " [0.74511385 0.29273286 0.29273286 0.29273286]]\n",
      "Iteration: 84\n",
      "Iteration 84, Loss: 0.1318691521883011, Grad_X_e: [[2.7169077  2.2645268  2.2645268  2.2645268 ]\n",
      " [2.637027   2.1846461  2.1846461  2.1846461 ]\n",
      " [2.5539024  2.1015215  2.1015215  2.1015215 ]\n",
      " [2.66101    2.6910856  2.2086294  2.2086294 ]\n",
      " [2.3784807  1.9260999  1.9260999  1.9260999 ]\n",
      " [2.2871563  1.8347749  1.8347749  1.8347749 ]\n",
      " [2.1941988  1.7418172  1.7418172  1.7418172 ]\n",
      " [2.100416   1.6480346  1.6480346  1.6480346 ]\n",
      " [1.8366886  0.9319271  0.9319271  0.9319271 ]\n",
      " [1.7523125  0.8475509  0.8475509  0.8475509 ]\n",
      " [1.8638489  1.450315   0.9590873  0.9590873 ]\n",
      " [1.7816393  1.3681055  0.8768773  0.8768773 ]\n",
      " [1.6927512  1.2792172  0.7879887  0.7879887 ]\n",
      " [1.5998768  1.1863428  0.6951151  0.6951151 ]\n",
      " [1.5120201  1.0984862  0.6072579  0.6072579 ]\n",
      " [1.4340502  1.0205166  0.52928865 0.52928865]\n",
      " [1.3541105  0.9405769  0.44934925 0.44934925]\n",
      " [1.2562819  0.8039011  0.8039011  0.8039011 ]\n",
      " [1.1638682  0.7114871  0.7114871  0.7114871 ]\n",
      " [1.0742189  0.6218371  0.6218371  0.6218371 ]\n",
      " [0.9946035  0.54222256 0.54222256 0.54222256]\n",
      " [0.91664237 0.46426153 0.46426153 0.46426153]\n",
      " [0.8252288  0.372848   0.372848   0.372848  ]\n",
      " [0.74551415 0.2931332  0.2931332  0.2931332 ]]\n",
      "Iteration: 85\n",
      "Iteration 85, Loss: 0.1318691521883011, Grad_X_e: [[2.7171626  2.2647817  2.2647817  2.2647817 ]\n",
      " [2.6364613  2.1840804  2.1840804  2.1840804 ]\n",
      " [2.5526168  2.100236   2.100236   2.100236  ]\n",
      " [2.6606786  2.6907542  2.208298   2.208298  ]\n",
      " [2.376232   1.923851   1.923851   1.923851  ]\n",
      " [2.2847214  1.8323399  1.8323399  1.8323399 ]\n",
      " [2.1917942  1.7394125  1.7394125  1.7394125 ]\n",
      " [2.0982776  1.6458963  1.6458963  1.6458963 ]\n",
      " [1.836893   0.9321314  0.9321314  0.9321314 ]\n",
      " [1.7514043  0.8466427  0.8466427  0.8466427 ]\n",
      " [1.8643801  1.4508462  0.9596185  0.9596185 ]\n",
      " [1.7810773  1.3675435  0.8763153  0.8763153 ]\n",
      " [1.6914409  1.277907   0.7866785  0.7866785 ]\n",
      " [1.5989841  1.1854502  0.69422245 0.69422245]\n",
      " [1.5125158  1.0989819  0.60775363 0.60775363]\n",
      " [1.4351015  1.021568   0.53033996 0.53033996]\n",
      " [1.353572   0.9400385  0.44881085 0.44881085]\n",
      " [1.2550223  0.8026416  0.8026416  0.8026416 ]\n",
      " [1.1627854  0.7104043  0.7104043  0.7104043 ]\n",
      " [1.0744008  0.62201905 0.62201905 0.62201905]\n",
      " [0.9957293  0.5433484  0.5433484  0.5433484 ]\n",
      " [0.9163191  0.46393824 0.46393824 0.46393824]\n",
      " [0.8247482  0.37236744 0.37236744 0.37236744]\n",
      " [0.7456757  0.2932947  0.2932947  0.2932947 ]]\n",
      "Iteration: 86\n",
      "Iteration 86, Loss: 0.1318691521883011, Grad_X_e: [[2.7172244  2.2648435  2.2648435  2.2648435 ]\n",
      " [2.6357493  2.1833684  2.1833684  2.1833684 ]\n",
      " [2.551237   2.0988562  2.0988562  2.0988562 ]\n",
      " [2.6601868  2.6902623  2.207806   2.207806  ]\n",
      " [2.3740005  1.9216197  1.9216197  1.9216197 ]\n",
      " [2.282359   1.8299774  1.8299774  1.8299774 ]\n",
      " [2.1895123  1.7371306  1.7371306  1.7371306 ]\n",
      " [2.0963042  1.643923   1.643923   1.643923  ]\n",
      " [1.8369023  0.93214065 0.93214065 0.93214065]\n",
      " [1.7504025  0.8456408  0.8456408  0.8456408 ]\n",
      " [1.8647091  1.4511752  0.95994747 0.95994747]\n",
      " [1.780374   1.3668402  0.875612   0.875612  ]\n",
      " [1.6901542  1.2766203  0.7853918  0.7853918 ]\n",
      " [1.5982883  1.1847544  0.6935266  0.6935266 ]\n",
      " [1.5131662  1.0996323  0.608404   0.608404  ]\n",
      " [1.4360138  1.0224802  0.53125226 0.53125226]\n",
      " [1.3528891  0.9393556  0.44812793 0.44812793]\n",
      " [1.2537553  0.8013746  0.8013746  0.8013746 ]\n",
      " [1.1618726  0.7094915  0.7094915  0.7094915 ]\n",
      " [1.0747714  0.6223896  0.6223896  0.6223896 ]\n",
      " [0.9967759  0.54439497 0.54439497 0.54439497]\n",
      " [0.91581523 0.4634344  0.4634344  0.4634344 ]\n",
      " [0.82448816 0.3721074  0.3721074  0.3721074 ]\n",
      " [0.7456118  0.2932308  0.2932308  0.2932308 ]]\n",
      "Iteration: 87\n",
      "Iteration 87, Loss: 0.1318691521883011, Grad_X_e: [[2.7171097  2.2647288  2.2647288  2.2647288 ]\n",
      " [2.6349077  2.1825268  2.1825268  2.1825268 ]\n",
      " [2.5497787  2.0973978  2.0973978  2.0973978 ]\n",
      " [2.6595511  2.6896267  2.2071705  2.2071705 ]\n",
      " [2.3717995  1.9194185  1.9194185  1.9194185 ]\n",
      " [2.280078   1.8276966  1.8276966  1.8276966 ]\n",
      " [2.1873589  1.7349772  1.7349772  1.7349772 ]\n",
      " [2.0944965  1.6421154  1.6421154  1.6421154 ]\n",
      " [1.836733   0.9319714  0.9319714  0.9319714 ]\n",
      " [1.7493365  0.8445748  0.8445748  0.8445748 ]\n",
      " [1.8648498  1.4513159  0.9600881  0.9600881 ]\n",
      " [1.7795565  1.3660227  0.8747945  0.8747945 ]\n",
      " [1.6889212  1.2753873  0.7841588  0.7841588 ]\n",
      " [1.5977887  1.1842548  0.6930271  0.6930271 ]\n",
      " [1.5139341  1.1004002  0.6091719  0.6091719 ]\n",
      " [1.436762   1.0232284  0.53200036 0.53200036]\n",
      " [1.3521116  0.9385782  0.4473505  0.4473505 ]\n",
      " [1.2525108  0.8001301  0.8001301  0.8001301 ]\n",
      " [1.1611387  0.7087575  0.7087575  0.7087575 ]\n",
      " [1.0753034  0.6229216  0.6229216  0.6229216 ]\n",
      " [0.99770886 0.5453279  0.5453279  0.5453279 ]\n",
      " [0.9151718  0.46279097 0.46279097 0.46279097]\n",
      " [0.8244433  0.3720625  0.3720625  0.3720625 ]\n",
      " [0.74534655 0.29296553 0.29296553 0.29296553]]\n",
      "Iteration: 88\n",
      "Iteration 88, Loss: 0.1318691521883011, Grad_X_e: [[2.7168353  2.2644544  2.2644544  2.2644544 ]\n",
      " [2.6339526  2.1815717  2.1815717  2.1815717 ]\n",
      " [2.5482566  2.0958757  2.0958757  2.0958757 ]\n",
      " [2.6587873  2.6888628  2.2064066  2.2064066 ]\n",
      " [2.3696399  1.9172589  1.9172589  1.9172589 ]\n",
      " [2.277887   1.8255059  1.8255059  1.8255059 ]\n",
      " [2.1853387  1.7329572  1.7329572  1.7329572 ]\n",
      " [2.0928545  1.6404734  1.6404734  1.6404734 ]\n",
      " [1.8364046  0.931643   0.931643   0.931643  ]\n",
      " [1.7482365  0.8434748  0.8434748  0.8434748 ]\n",
      " [1.8648155  1.4512815  0.9600538  0.9600538 ]\n",
      " [1.7786523  1.3651185  0.8738903  0.8738903 ]\n",
      " [1.6877685  1.2742345  0.7830061  0.7830061 ]\n",
      " [1.5974836  1.1839497  0.6927221  0.6927221 ]\n",
      " [1.5147843  1.1012504  0.6100221  0.6100221 ]\n",
      " [1.4373323  1.0237987  0.5325707  0.5325707 ]\n",
      " [1.3512853  0.93775195 0.44652426 0.44652426]\n",
      " [1.2513173  0.79893667 0.79893667 0.79893667]\n",
      " [1.1605904  0.7082093  0.7082093  0.7082093 ]\n",
      " [1.0759676  0.62358564 0.62358564 0.62358564]\n",
      " [0.99850565 0.5461247  0.5461247  0.5461247 ]\n",
      " [0.9144377  0.46205685 0.46205685 0.46205685]\n",
      " [0.8246001  0.37221935 0.37221935 0.37221935]\n",
      " [0.74492186 0.29254085 0.29254085 0.29254085]]\n",
      "Iteration: 89\n",
      "Iteration 89, Loss: 0.1318691521883011, Grad_X_e: [[2.716416   2.264035   2.264035   2.264035  ]\n",
      " [2.632898   2.1805172  2.1805172  2.1805172 ]\n",
      " [2.546685   2.094304   2.094304   2.094304  ]\n",
      " [2.6579106  2.6879861  2.20553    2.20553   ]\n",
      " [2.367531   1.91515    1.91515    1.91515   ]\n",
      " [2.2757924  1.823411   1.823411   1.823411  ]\n",
      " [2.1834538  1.7310723  1.7310723  1.7310723 ]\n",
      " [2.0913758  1.6389948  1.6389948  1.6389948 ]\n",
      " [1.8359367  0.93117505 0.93117505 0.93117505]\n",
      " [1.747128   0.8423663  0.8423663  0.8423663 ]\n",
      " [1.8646219  1.451088   0.9598602  0.9598602 ]\n",
      " [1.7776847  1.3641509  0.87292266 0.87292266]\n",
      " [1.6867135  1.2731795  0.781951   0.781951  ]\n",
      " [1.5973637  1.1838298  0.69260216 0.69260216]\n",
      " [1.5156796  1.1021457  0.6109174  0.6109174 ]\n",
      " [1.4377141  1.0241805  0.5329526  0.5329526 ]\n",
      " [1.3504522  0.93691874 0.44569108 0.44569108]\n",
      " [1.2501966  0.7978159  0.7978159  0.7978159 ]\n",
      " [1.1602215  0.70784026 0.70784026 0.70784026]\n",
      " [1.0767277  0.62434584 0.62434584 0.62434584]\n",
      " [0.9991418  0.54676086 0.54676086 0.54676086]\n",
      " [0.91364837 0.4612675  0.4612675  0.4612675 ]\n",
      " [0.82492614 0.37254536 0.37254536 0.37254536]\n",
      " [0.74438405 0.29200304 0.29200304 0.29200304]]\n",
      "Iteration: 90\n",
      "Iteration 90, Loss: 0.1318691521883011, Grad_X_e: [[2.7158673  2.2634864  2.2634864  2.2634864 ]\n",
      " [2.6317596  2.1793787  2.1793787  2.1793787 ]\n",
      " [2.5450776  2.0926967  2.0926967  2.0926967 ]\n",
      " [2.6569357  2.6870112  2.204555   2.204555  ]\n",
      " [2.365482   1.9131011  1.9131011  1.9131011 ]\n",
      " [2.2738004  1.8214189  1.8214189  1.8214189 ]\n",
      " [2.1817071  1.7293255  1.7293255  1.7293255 ]\n",
      " [2.090058   1.6376771  1.6376771  1.6376771 ]\n",
      " [1.8353524  0.93059075 0.93059075 0.93059075]\n",
      " [1.7460384  0.8412767  0.8412767  0.8412767 ]\n",
      " [1.8642884  1.4507545  0.9595267  0.9595267 ]\n",
      " [1.7766827  1.3631489  0.87192076 0.87192076]\n",
      " [1.6857746  1.2722406  0.7810121  0.7810121 ]\n",
      " [1.5974176  1.1838837  0.6926561  0.6926561 ]\n",
      " [1.5165892  1.1030552  0.6118269  0.6118269 ]\n",
      " [1.437911   1.0243775  0.53314954 0.53314954]\n",
      " [1.3496599  0.9361264  0.44489875 0.44489875]\n",
      " [1.2491708  0.7967901  0.7967901  0.7967901 ]\n",
      " [1.1600299  0.70764863 0.70764863 0.70764863]\n",
      " [1.0775528  0.6251708  0.6251708  0.6251708 ]\n",
      " [0.99960977 0.5472288  0.5472288  0.5472288 ]\n",
      " [0.91285485 0.46047395 0.46047395 0.46047395]\n",
      " [0.82539165 0.37301087 0.37301087 0.37301087]\n",
      " [0.74379575 0.29141474 0.29141474 0.29141474]]\n",
      "Iteration: 91\n",
      "Iteration 91, Loss: 0.1318691521883011, Grad_X_e: [[2.7152038  2.2628229  2.2628229  2.2628229 ]\n",
      " [2.6305504  2.1781695  2.1781695  2.1781695 ]\n",
      " [2.543446   2.0910652  2.0910652  2.0910652 ]\n",
      " [2.6558766  2.6859522  2.203496   2.203496  ]\n",
      " [2.3635004  1.9111195  1.9111195  1.9111195 ]\n",
      " [2.271915   1.8195335  1.8195335  1.8195335 ]\n",
      " [2.1800983  1.7277167  1.7277167  1.7277167 ]\n",
      " [2.0888968  1.6365159  1.6365159  1.6365159 ]\n",
      " [1.834672   0.92991036 0.92991036 0.92991036]\n",
      " [1.7449878  0.84022605 0.84022605 0.84022605]\n",
      " [1.8638328  1.4502989  0.95907104 0.95907104]\n",
      " [1.7756703  1.3621365  0.87090826 0.87090826]\n",
      " [1.6849631  1.2714292  0.7802006  0.7802006 ]\n",
      " [1.5976257  1.1840918  0.69286424 0.69286424]\n",
      " [1.5174783  1.1039444  0.61271614 0.61271614]\n",
      " [1.437928   1.0243944  0.5331665  0.5331665 ]\n",
      " [1.3489404  0.93540686 0.44417918 0.44417918]\n",
      " [1.2482536  0.795873   0.795873   0.795873  ]\n",
      " [1.1600024  0.70762104 0.70762104 0.70762104]\n",
      " [1.0784078  0.62602586 0.62602586 0.62602586]\n",
      " [0.99990416 0.5475232  0.5475232  0.5475232 ]\n",
      " [0.91209215 0.45971128 0.45971128 0.45971128]\n",
      " [0.8259519  0.37357113 0.37357113 0.37357113]\n",
      " [0.7432102  0.29082918 0.29082918 0.29082918]]\n",
      "Iteration: 92\n",
      "Iteration 92, Loss: 0.1318691521883011, Grad_X_e: [[2.7144387  2.2620578  2.2620578  2.2620578 ]\n",
      " [2.629283   2.176902   2.176902   2.176902  ]\n",
      " [2.5418024  2.0894215  2.0894215  2.0894215 ]\n",
      " [2.6547463  2.6848218  2.2023656  2.2023656 ]\n",
      " [2.361593   1.909212   1.909212   1.909212  ]\n",
      " [2.27014    1.8177584  1.8177584  1.8177584 ]\n",
      " [2.1786273  1.7262456  1.7262456  1.7262456 ]\n",
      " [2.0878878  1.635507   1.635507   1.635507  ]\n",
      " [1.8339176  0.929156   0.929156   0.929156  ]\n",
      " [1.7439986  0.83923686 0.83923686 0.83923686]\n",
      " [1.8632746  1.4497406  0.9585128  0.9585128 ]\n",
      " [1.7746689  1.3611351  0.8699069  0.8699069 ]\n",
      " [1.6842893  1.2707554  0.7795268  0.7795268 ]\n",
      " [1.5979702  1.1844363  0.6932088  0.6932088 ]\n",
      " [1.5183188  1.1047848  0.61355656 0.61355656]\n",
      " [1.4377831  1.0242496  0.5330217  0.5330217 ]\n",
      " [1.34833    0.9347965  0.44356886 0.44356886]\n",
      " [1.2474576  0.7950771  0.7950771  0.7950771 ]\n",
      " [1.160128   0.7077467  0.7077467  0.7077467 ]\n",
      " [1.0792625  0.62688065 0.62688065 0.62688065]\n",
      " [1.0000314  0.54765034 0.54765034 0.54765034]\n",
      " [0.91139716 0.45901626 0.45901626 0.45901626]\n",
      " [0.8265659  0.37418517 0.37418517 0.37418517]\n",
      " [0.7426802  0.29029918 0.29029918 0.29029918]]\n",
      "Iteration: 93\n",
      "Iteration 93, Loss: 0.1318691521883011, Grad_X_e: [[2.713585   2.261204   2.261204   2.261204  ]\n",
      " [2.6279693  2.1755884  2.1755884  2.1755884 ]\n",
      " [2.5401566  2.0877757  2.0877757  2.0877757 ]\n",
      " [2.653557   2.6836326  2.2011764  2.2011764 ]\n",
      " [2.359766   1.9073849  1.9073849  1.9073849 ]\n",
      " [2.2684777  1.8160963  1.8160963  1.8160963 ]\n",
      " [2.1772926  1.7249109  1.7249109  1.7249109 ]\n",
      " [2.0870256  1.6346447  1.6346447  1.6346447 ]\n",
      " [1.8331105  0.92834884 0.92834884 0.92834884]\n",
      " [1.7430863  0.8383245  0.8383245  0.8383245 ]\n",
      " [1.8626335  1.4490995  0.9578717  0.9578717 ]\n",
      " [1.7736996  1.3601658  0.86893755 0.86893755]\n",
      " [1.6837571  1.2702231  0.7789945  0.7789945 ]\n",
      " [1.5984281  1.1848942  0.69366664 0.69366664]\n",
      " [1.5190859  1.105552   0.6143237  0.6143237 ]\n",
      " [1.4374949  1.0239613  0.5327334  0.5327334 ]\n",
      " [1.3478454  0.9343119  0.4430843  0.4430843 ]\n",
      " [1.2467911  0.7944106  0.7944106  0.7944106 ]\n",
      " [1.1603853  0.70800394 0.70800394 0.70800394]\n",
      " [1.0800878  0.6277059  0.6277059  0.6277059 ]\n",
      " [0.9999988  0.5476178  0.5476178  0.5476178 ]\n",
      " [0.9107965  0.45841563 0.45841563 0.45841563]\n",
      " [0.8271874  0.37480667 0.37480667 0.37480667]\n",
      " [0.7422418  0.28986079 0.28986079 0.28986079]]\n",
      "Iteration: 94\n",
      "Iteration 94, Loss: 0.1318691521883011, Grad_X_e: [[2.7126546  2.2602737  2.2602737  2.2602737 ]\n",
      " [2.6266203  2.1742394  2.1742394  2.1742394 ]\n",
      " [2.5385182  2.0861373  2.0861373  2.0861373 ]\n",
      " [2.6523201  2.6823957  2.1999395  2.1999395 ]\n",
      " [2.3580236  1.9056424  1.9056424  1.9056424 ]\n",
      " [2.2669296  1.8145483  1.8145483  1.8145483 ]\n",
      " [2.1760921  1.7237104  1.7237104  1.7237104 ]\n",
      " [2.0863042  1.6339233  1.6339233  1.6339233 ]\n",
      " [1.8322706  0.92750907 0.92750907 0.92750907]\n",
      " [1.7422639  0.83750206 0.83750206 0.83750206]\n",
      " [1.8619283  1.4483944  0.9571665  0.9571665 ]\n",
      " [1.7727804  1.3592466  0.8680184  0.8680184 ]\n",
      " [1.6833662  1.2698323  0.7786037  0.7786037 ]\n",
      " [1.598974   1.1854401  0.69421256 0.69421256]\n",
      " [1.5197588  1.1062249  0.6149967  0.6149967 ]\n",
      " [1.4370897  1.0235561  0.53232825 0.53232825]\n",
      " [1.3475019  0.93396837 0.44274077 0.44274077]\n",
      " [1.2462583  0.79387766 0.79387766 0.79387766]\n",
      " [1.1607584  0.70837706 0.70837706 0.70837706]\n",
      " [1.0808582  0.6284763  0.6284763  0.6284763 ]\n",
      " [0.9998228  0.5474418  0.5474418  0.5474418 ]\n",
      " [0.91031235 0.45793146 0.45793146 0.45793146]\n",
      " [0.8277769  0.37539616 0.37539616 0.37539616]\n",
      " [0.7419239  0.2895429  0.2895429  0.2895429 ]]\n",
      "Iteration: 95\n",
      "Iteration 95, Loss: 0.1318691521883011, Grad_X_e: [[2.7116594  2.2592785  2.2592785  2.2592785 ]\n",
      " [2.6252463  2.1728654  2.1728654  2.1728654 ]\n",
      " [2.5368958  2.0845149  2.0845149  2.0845149 ]\n",
      " [2.6510465  2.681122   2.1986659  2.1986659 ]\n",
      " [2.3563693  1.903988   1.903988   1.903988  ]\n",
      " [2.2654965  1.813115   1.813115   1.813115  ]\n",
      " [2.1750224  1.7226408  1.7226408  1.7226408 ]\n",
      " [2.0857167  1.6333357  1.6333357  1.6333357 ]\n",
      " [1.8314167  0.92665523 0.92665523 0.92665523]\n",
      " [1.7415413  0.83677936 0.83677936 0.83677936]\n",
      " [1.8611782  1.4476442  0.95641625 0.95641625]\n",
      " [1.771925   1.3583912  0.86716294 0.86716294]\n",
      " [1.6831121  1.2695782  0.7783497  0.7783497 ]\n",
      " [1.5995849  1.186051   0.69482356 0.69482356]\n",
      " [1.5203218  1.1067879  0.61555964 0.61555964]\n",
      " [1.4365939  1.0230603  0.53183246 0.53183246]\n",
      " [1.3473012  0.9337677  0.4425401  0.4425401 ]\n",
      " [1.2458594  0.79347885 0.79347885 0.79347885]\n",
      " [1.1612236  0.70884234 0.70884234 0.70884234]\n",
      " [1.0815523  0.62917036 0.62917036 0.62917036]\n",
      " [0.99952185 0.54714084 0.54714084 0.54714084]\n",
      " [0.90996027 0.45757934 0.45757934 0.45757934]\n",
      " [0.8282953  0.3759145  0.3759145  0.3759145 ]\n",
      " [0.74174064 0.2893596  0.2893596  0.2893596 ]]\n",
      "Iteration: 96\n",
      "Iteration 96, Loss: 0.1318691521883011, Grad_X_e: [[2.7106097  2.2582288  2.2582288  2.2582288 ]\n",
      " [2.623857   2.1714761  2.1714761  2.1714761 ]\n",
      " [2.535297   2.082916   2.082916   2.082916  ]\n",
      " [2.6497464  2.679822   2.1973658  2.1973658 ]\n",
      " [2.354806   1.9024248  1.9024248  1.9024248 ]\n",
      " [2.2641776  1.8117961  1.8117961  1.8117961 ]\n",
      " [2.1740804  1.7216986  1.7216986  1.7216986 ]\n",
      " [2.085256   1.6328751  1.6328751  1.6328751 ]\n",
      " [1.8305686  0.92580706 0.92580706 0.92580706]\n",
      " [1.7409264  0.8361645  0.8361645  0.8361645 ]\n",
      " [1.8604012  1.4468672  0.9556392  0.9556392 ]\n",
      " [1.7711468  1.357613   0.8663847  0.8663847 ]\n",
      " [1.6829897  1.2694558  0.77822727 0.77822727]\n",
      " [1.6002387  1.1867048  0.69547725 0.69547725]\n",
      " [1.5207652  1.1072313  0.6160029  0.6160029 ]\n",
      " [1.4360374  1.0225039  0.531276   0.531276  ]\n",
      " [1.3472428  0.9337092  0.44248164 0.44248164]\n",
      " [1.245594   0.7932134  0.7932134  0.7932134 ]\n",
      " [1.1617608  0.70937955 0.70937955 0.70937955]\n",
      " [1.0821549  0.62977296 0.62977296 0.62977296]\n",
      " [0.9991187  0.5467377  0.5467377  0.5467377 ]\n",
      " [0.9097426  0.45736164 0.45736164 0.45736164]\n",
      " [0.82871574 0.37633497 0.37633497 0.37633497]\n",
      " [0.74170065 0.28931963 0.28931963 0.28931963]]\n",
      "Iteration: 97\n",
      "Iteration 97, Loss: 0.1318691521883011, Grad_X_e: [[2.709516   2.2571352  2.2571352  2.2571352 ]\n",
      " [2.6224613  2.1700804  2.1700804  2.1700804 ]\n",
      " [2.5337284  2.0813475  2.0813475  2.0813475 ]\n",
      " [2.648429   2.6785045  2.1960483  2.1960483 ]\n",
      " [2.3533363  1.9009552  1.9009552  1.9009552 ]\n",
      " [2.2629724  1.810591   1.810591   1.810591  ]\n",
      " [2.1732612  1.7208794  1.7208794  1.7208794 ]\n",
      " [2.0849147  1.6325338  1.6325338  1.6325338 ]\n",
      " [1.8297412  0.92497975 0.92497975 0.92497975]\n",
      " [1.7404236  0.8356616  0.8356616  0.8356616 ]\n",
      " [1.8596148  1.4460809  0.95485294 0.95485294]\n",
      " [1.7704561  1.3569223  0.865694   0.865694  ]\n",
      " [1.6829906  1.2694566  0.7782281  0.7782281 ]\n",
      " [1.6009104  1.1873765  0.696149   0.696149  ]\n",
      " [1.5210865  1.1075525  0.61632425 0.61632425]\n",
      " [1.4354515  1.0219179  0.53069013 0.53069013]\n",
      " [1.3473161  0.9337826  0.442555   0.442555  ]\n",
      " [1.2454557  0.7930752  0.7930752  0.7930752 ]\n",
      " [1.162346   0.7099648  0.7099648  0.7099648 ]\n",
      " [1.0826538  0.63027185 0.63027185 0.63027185]\n",
      " [0.99864364 0.5462626  0.5462626  0.5462626 ]\n",
      " [0.909664   0.45728302 0.45728302 0.45728302]\n",
      " [0.82901984 0.37663907 0.37663907 0.37663907]\n",
      " [0.7417953  0.28941426 0.28941426 0.28941426]]\n",
      "Iteration: 98\n",
      "Iteration 98, Loss: 0.1318691521883011, Grad_X_e: [[2.7083886  2.2560077  2.2560077  2.2560077 ]\n",
      " [2.621068   2.168687   2.168687   2.168687  ]\n",
      " [2.5321975  2.0798166  2.0798166  2.0798166 ]\n",
      " [2.6471024  2.677178   2.1947217  2.1947217 ]\n",
      " [2.351962   1.899581   1.899581   1.899581  ]\n",
      " [2.2618794  1.8094981  1.8094981  1.8094981 ]\n",
      " [2.1725612  1.7201793  1.7201793  1.7201793 ]\n",
      " [2.084685   1.6323042  1.6323042  1.6323042 ]\n",
      " [1.8289498  0.92418826 0.92418826 0.92418826]\n",
      " [1.7400345  0.83527243 0.83527243 0.83527243]\n",
      " [1.8588364  1.4453025  0.95407444 0.95407444]\n",
      " [1.7698604  1.3563266  0.8650983  0.8650983 ]\n",
      " [1.683106   1.269572   0.77834344 0.77834344]\n",
      " [1.6015791  1.1880451  0.69681764 0.69681764]\n",
      " [1.5212853  1.1077514  0.616523   0.616523  ]\n",
      " [1.4348629  1.0213293  0.5301015  0.5301015 ]\n",
      " [1.3475095  0.9339759  0.4427483  0.4427483 ]\n",
      " [1.2454367  0.7930561  0.7930561  0.7930561 ]\n",
      " [1.1629616  0.71058035 0.71058035 0.71058035]\n",
      " [1.0830425  0.6306606  0.6306606  0.6306606 ]\n",
      " [0.9981224  0.5457414  0.5457414  0.5457414 ]\n",
      " [0.9097167  0.45733577 0.45733577 0.45733577]\n",
      " [0.8292029  0.37682208 0.37682208 0.37682208]\n",
      " [0.74200815 0.28962713 0.28962713 0.28962713]]\n",
      "Iteration: 99\n",
      "Iteration 99, Loss: 0.1318691521883011, Grad_X_e: [[2.7072353  2.2548544  2.2548544  2.2548544 ]\n",
      " [2.6196835  2.1673026  2.1673026  2.1673026 ]\n",
      " [2.530709   2.0783281  2.0783281  2.0783281 ]\n",
      " [2.6457741  2.6758497  2.1933935  2.1933935 ]\n",
      " [2.3506832  1.8983021  1.8983021  1.8983021 ]\n",
      " [2.260896   1.8085146  1.8085146  1.8085146 ]\n",
      " [2.1719744  1.7195927  1.7195927  1.7195927 ]\n",
      " [2.0845582  1.6321774  1.6321774  1.6321774 ]\n",
      " [1.8282069  0.9234453  0.9234453  0.9234453 ]\n",
      " [1.7397574  0.8349954  0.8349954  0.8349954 ]\n",
      " [1.8580785  1.4445446  0.9533165  0.9533165 ]\n",
      " [1.7693624  1.3558286  0.8646003  0.8646003 ]\n",
      " [1.6833194  1.2697855  0.77855694 0.77855694]\n",
      " [1.6022239  1.18869    0.69746244 0.69746244]\n",
      " [1.5213643  1.1078304  0.61660206 0.61660206]\n",
      " [1.4342958  1.0207622  0.5295344  0.5295344 ]\n",
      " [1.3477995  0.93426585 0.4430383  0.4430383 ]\n",
      " [1.2455258  0.7931453  0.7931453  0.7931453 ]\n",
      " [1.1635844  0.71120316 0.71120316 0.71120316]\n",
      " [1.0833167  0.6309347  0.6309347  0.6309347 ]\n",
      " [0.9975765  0.54519546 0.54519546 0.54519546]\n",
      " [0.90988165 0.4575007  0.4575007  0.4575007 ]\n",
      " [0.82925737 0.37687656 0.37687656 0.37687656]\n",
      " [0.7423031  0.2899221  0.2899221  0.2899221 ]]\n",
      "Optimized value of X: [[2.7072353  2.2548544  2.2548544  2.2548544 ]\n",
      " [2.6196835  2.1673026  2.1673026  2.1673026 ]\n",
      " [2.530709   2.0783281  2.0783281  2.0783281 ]\n",
      " [2.6457741  2.6758497  2.1933935  2.1933935 ]\n",
      " [2.3506832  1.8983021  1.8983021  1.8983021 ]\n",
      " [2.260896   1.8085146  1.8085146  1.8085146 ]\n",
      " [2.1719744  1.7195927  1.7195927  1.7195927 ]\n",
      " [2.0845582  1.6321774  1.6321774  1.6321774 ]\n",
      " [1.8282069  0.9234453  0.9234453  0.9234453 ]\n",
      " [1.7397574  0.8349954  0.8349954  0.8349954 ]\n",
      " [1.8580785  1.4445446  0.9533165  0.9533165 ]\n",
      " [1.7693624  1.3558286  0.8646003  0.8646003 ]\n",
      " [1.6833194  1.2697855  0.77855694 0.77855694]\n",
      " [1.6022239  1.18869    0.69746244 0.69746244]\n",
      " [1.5213643  1.1078304  0.61660206 0.61660206]\n",
      " [1.4342958  1.0207622  0.5295344  0.5295344 ]\n",
      " [1.3477995  0.93426585 0.4430383  0.4430383 ]\n",
      " [1.2455258  0.7931453  0.7931453  0.7931453 ]\n",
      " [1.1635844  0.71120316 0.71120316 0.71120316]\n",
      " [1.0833167  0.6309347  0.6309347  0.6309347 ]\n",
      " [0.9975765  0.54519546 0.54519546 0.54519546]\n",
      " [0.90988165 0.4575007  0.4575007  0.4575007 ]\n",
      " [0.82925737 0.37687656 0.37687656 0.37687656]\n",
      " [0.7423031  0.2899221  0.2899221  0.2899221 ]], Best prediction: [0.31652814 0.3225809  0.32318148 0.32479337 0.32379842 0.32237276\n",
      " 0.3213404  0.32039693 0.32481465 0.3148847  0.3117142  0.31185257\n",
      " 0.31092143 0.31247714 0.310533   0.3029477  0.30523092 0.30286905\n",
      " 0.3018363  0.30031556 0.29607853 0.30194417 0.2943347  0.29559645]\n",
      "Original Prediction after scaling inversion: [[146.35345]\n",
      " [148.38718]\n",
      " [148.58897]\n",
      " [149.13057]\n",
      " [148.79626]\n",
      " [148.31725]\n",
      " [147.97038]\n",
      " [147.65337]\n",
      " [149.13773]\n",
      " [145.80125]\n",
      " [144.73596]\n",
      " [144.78246]\n",
      " [144.4696 ]\n",
      " [144.99231]\n",
      " [144.33908]\n",
      " [141.79042]\n",
      " [142.55759]\n",
      " [141.76399]\n",
      " [141.41699]\n",
      " [140.90602]\n",
      " [139.48239]\n",
      " [141.45323]\n",
      " [138.89645]\n",
      " [139.3204 ]]\n"
     ]
    }
   ],
   "source": [
    "#TEST REGRESSION CHECK!!!\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define the loss function\n",
    "def compute_loss(max_bound, min_bound, pred):\n",
    "    mse_loss_ = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "    dist_max = mse_loss_(max_bound, pred)\n",
    "    dist_min = mse_loss_(min_bound, pred)\n",
    "    loss = dist_max + dist_min\n",
    "    return loss\n",
    "\n",
    "# Gradient computation using finite difference method\n",
    "def compute_gradient_finite_difference(model, X_e, max_bound, min_bound, epsilon=1e-4):\n",
    "    # Convert tensor to numpy for compatibility\n",
    "    temp = X_e.numpy()\n",
    "\n",
    "    # Get the original prediction\n",
    "    pred = model.predict(temp)\n",
    "    pred = tf.convert_to_tensor(pred, dtype=tf.float32)\n",
    "\n",
    "    # Original loss\n",
    "    original_loss = compute_loss(max_bound, min_bound, pred)\n",
    "\n",
    "    # Approximate the gradient using finite differences\n",
    "    gradients = np.zeros_like(X_e.numpy())\n",
    "    for i in range(X_e.shape[0]):\n",
    "        X_e_perturbed = X_e.numpy()\n",
    "        X_e_perturbed[i] += epsilon\n",
    "        pred_perturbed = model.predict(X_e_perturbed)\n",
    "        pred_perturbed = tf.convert_to_tensor(pred_perturbed, dtype=tf.float32)\n",
    "\n",
    "        # Compute the perturbed loss\n",
    "        perturbed_loss = compute_loss(max_bound, min_bound, pred_perturbed)\n",
    "\n",
    "        # Finite difference (gradient approximation)\n",
    "        gradients[i] = (perturbed_loss.numpy() - original_loss.numpy()) / epsilon\n",
    "\n",
    "    return gradients\n",
    "\n",
    "# Example dataset (replace with actual dataset)\n",
    "X = random.choice(dataset.X_train_exog)\n",
    "y = random.choice(dataset.X_train_target)\n",
    "X_test_exog = random.choice(dataset.X_test_exog)\n",
    "test_X = dataset.X_train_exog[0]\n",
    "test_y = dataset.X_train_target[0]\n",
    "test_X_test_exog = dataset.X_test_exog[0]\n",
    "test = dataset.X_test_target[0]\n",
    "# Fit a regression model (e.g., OLS)\n",
    "#X_with_intercept = sm.add_constant(X)  # Add intercept term\n",
    "#print(y, X)\n",
    "model = sm.OLS(test_y, test_X).fit()\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "\n",
    "# Initialize the input tensor\n",
    "grad_X_e = tf.convert_to_tensor(test_X_test_exog, dtype=tf.float32)\n",
    "grad_X_e = tf.Variable(grad_X_e, dtype=tf.float32)\n",
    "\n",
    "\n",
    "pred_ols = model.predict(grad_X_e.numpy())\n",
    "pred_ols = tf.convert_to_tensor(pred_ols, dtype=tf.float32)\n",
    "\n",
    "mean_smape, mean_rmse = forecast_metrics_single(test, pred_ols)\n",
    "print(\n",
    "    f\"model trained, with test sMAPE score {mean_smape:0.4f}; test RMSE score: {mean_rmse:0.4f}.\"\n",
    ")\n",
    "\n",
    "# Perform optimization\n",
    "max_iter = 100\n",
    "it = 0\n",
    "while (tf.reduce_any(pred_ols > max_bound) or tf.reduce_any(pred_ols < min_bound)) and (it < max_iter):\n",
    "    # Add intercept to test data\n",
    "    #grad_X_e_with_intercept = sm.add_constant(grad_X_e.numpy())\n",
    "\n",
    "    # Get predictions\n",
    "    pred_ols = model.predict(grad_X_e.numpy())\n",
    "    pred_ols = tf.convert_to_tensor(pred, dtype=tf.float32)\n",
    "\n",
    "    # Check bounds\n",
    "    print(f\"Iteration: {it}\")\n",
    "\n",
    "    # Compute approximate gradients using finite differences\n",
    "    gradients = compute_gradient_finite_difference(model, grad_X_e, max_bound, min_bound)\n",
    "\n",
    "    # Apply gradients using optimizer\n",
    "    gradients_tensor = tf.convert_to_tensor(gradients, dtype=tf.float32)\n",
    "    optimizer.apply_gradients([(gradients_tensor, grad_X_e)])\n",
    "\n",
    "    # Update predictions\n",
    "    grad_X_e_with_intercept = sm.add_constant(grad_X_e.numpy())\n",
    "    pred_ols = model.predict(grad_X_e.numpy())\n",
    "    pred_ols = tf.convert_to_tensor(pred, dtype=tf.float32)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = compute_loss(max_bound, min_bound, pred_ols)\n",
    "    print(f\"Iteration {it}, Loss: {loss.numpy()}, Grad_X_e: {grad_X_e.numpy()}\")\n",
    "    it += 1\n",
    "\n",
    "# Output results\n",
    "print(f\"Optimized value of X: {grad_X_e.numpy()}, Best prediction: {pred_ols.numpy()}\")\n",
    "\n",
    "# Final inversion step if needed\n",
    "scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "Y_preds_original_reg = scaler.inverse_transform(pred_ols.numpy().reshape(-1, 1))  # Ensure correct shape\n",
    "print(\"Original Prediction after scaling inversion:\", Y_preds_original_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c0dfd3f-540a-4424-b22a-23c3c5a42bb7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8036     0.78051201 0.75742402 0.73433602 0.71124803 0.68816004\n",
      "  0.66507205 0.64198406 0.61889607 0.59580807 0.57272008 0.54963209\n",
      "  0.5265441  0.50345611 0.48036812 0.45728012 0.43419213 0.41110414\n",
      "  0.38801615 0.36492816 0.34184017 0.31875217 0.29566418 0.27257619]] A:SNV:ANDB tf.Tensor(\n",
      "[0.31652814 0.3225809  0.32318148 0.32479337 0.32379842 0.32237276\n",
      " 0.3213404  0.32039693 0.32481465 0.3148847  0.3117142  0.31185257\n",
      " 0.31092143 0.31247714 0.310533   0.3029477  0.30523092 0.30286905\n",
      " 0.3018363  0.30031556 0.29607853 0.30194417 0.2943347  0.29559645], shape=(24,), dtype=float32)\n",
      "LOSS tf.Tensor(0.13186915, shape=(), dtype=float32)\n",
      "tf.Tensor(0.13186915, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.06498657e-04 -2.15004876e-01  1.31792665e-01  1.26732793e-02]\n",
      " [ 9.47418623e-04 -2.22991034e-01  1.43324479e-01  1.14585320e-02]\n",
      " [ 1.00237608e-03 -2.34352559e-01  1.57385871e-01  1.07860519e-02]\n",
      " [ 6.01433450e-04 -2.47189373e-01  1.73159361e-01  1.03350561e-02]\n",
      " [ 8.62109937e-06 -2.59610593e-01  1.89476863e-01  9.91722289e-03]\n",
      " [-6.52895018e-04 -2.70212054e-01  2.05294728e-01  9.43609420e-03]\n",
      " [-1.33202935e-03 -2.78043658e-01  2.19768092e-01  8.84615816e-03]\n",
      " [-2.01218179e-03 -2.82480627e-01  2.32213289e-01  8.12813547e-03]\n",
      " [-2.68992595e-03 -2.83125162e-01  2.42064163e-01  7.27699324e-03]\n",
      " [-3.36322468e-03 -2.79756039e-01  2.48847976e-01  6.29644608e-03]\n",
      " [-4.02506627e-03 -2.72310495e-01  2.52178609e-01  5.19625610e-03]\n",
      " [-4.65967972e-03 -2.60882795e-01  2.51760274e-01  3.99029814e-03]\n",
      " [-5.23963151e-03 -2.45729372e-01  2.47396529e-01  2.69465335e-03]\n",
      " [-5.72337210e-03 -2.27273926e-01  2.39000767e-01  1.32541335e-03]\n",
      " [-6.05380489e-03 -2.06108540e-01  2.26604864e-01 -1.03873062e-04]\n",
      " [-6.16038730e-03 -1.82988927e-01  2.10363969e-01 -1.58472860e-03]\n",
      " [-5.97022939e-03 -1.58822760e-01  1.90555796e-01 -3.11517017e-03]\n",
      " [-5.43914409e-03 -1.34655103e-01  1.67574674e-01 -4.69755521e-03]\n",
      " [-4.62278305e-03 -1.11659355e-01  1.41922608e-01 -6.32862886e-03]\n",
      " [-3.81796109e-03 -9.11481753e-02  1.14209354e-01 -7.97446817e-03]\n",
      " [-3.79342330e-03 -7.45974556e-02  8.51918757e-02 -9.51997191e-03]\n",
      " [-5.99678932e-03 -6.35103360e-02  5.59447296e-02 -1.06754955e-02]\n",
      " [-1.19840587e-02 -5.81766516e-02  2.84251384e-02 -1.07931187e-02]\n",
      " [-1.88759174e-02 -5.12532517e-02  7.00714113e-03 -8.40830058e-03]], shape=(24, 4), dtype=float32)\n",
      "[[0.8036     0.78051201 0.75742402 0.73433602 0.71124803 0.68816004\n",
      "  0.66507205 0.64198406 0.61889607 0.59580807 0.57272008 0.54963209\n",
      "  0.5265441  0.50345611 0.48036812 0.45728012 0.43419213 0.41110414\n",
      "  0.38801615 0.36492816 0.34184017 0.31875217 0.29566418 0.27257619]] A:SNV:ANDB tf.Tensor(\n",
      "[0.3804438  0.47716093 0.40678954 0.44399777 0.4606037  0.48005816\n",
      " 0.4977562  0.47036666 0.4898371  0.46418357 0.4781364  0.51945025\n",
      " 0.53317773 0.5337654  0.5053069  0.52984107 0.5408203  0.57119673\n",
      " 0.5177518  0.5417772  0.528128   0.5300055  0.4875018  0.47284642], shape=(24,), dtype=float32)\n",
      "LOSS tf.Tensor(0.07592407, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got scalar array instead:\narray=0.380443811416626.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 51\u001b[0m\n\u001b[1;32m     47\u001b[0m     loss \u001b[38;5;241m=\u001b[39m compute_loss(max_bound, min_bound, pred_tf)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Record the current value of x\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m mean_smape, mean_rmse \u001b[38;5;241m=\u001b[39m \u001b[43mforecast_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_tf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel trained, with test sMAPE score \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_smape\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; test RMSE score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m0.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Grad_X_e: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrad_X_e\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36mforecast_metrics\u001b[0;34m(dataset, Y_pred, inverse_transform)\u001b[0m\n\u001b[1;32m      8\u001b[0m     scaler \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mscaler[idx]\n\u001b[1;32m     10\u001b[0m     Y_test_original\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     11\u001b[0m         scaler[dataset\u001b[38;5;241m.\u001b[39mtarget_col]\u001b[38;5;241m.\u001b[39minverse_transform(dataset\u001b[38;5;241m.\u001b[39mX_test_target[i])\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m     Y_pred_original\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 14\u001b[0m         \u001b[43mscaler\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     17\u001b[0m Y_test_original \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Y_test_original)\n\u001b[1;32m     18\u001b[0m Y_pred_original \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(Y_pred_original)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:544\u001b[0m, in \u001b[0;36mMinMaxScaler.inverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Undo the scaling of X according to feature_range.\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;124;03m    Transformed data.\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    542\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 544\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m X \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n\u001b[1;32m    549\u001b[0m X \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:930\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_2d:\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;66;03m# If input is scalar raise error\u001b[39;00m\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 930\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    931\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got scalar array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    932\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    934\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[1;32m    935\u001b[0m         )\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=0.380443811416626.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_loss(max_bound, min_bound, pred):\n",
    "    mse_loss_ = tf.keras.losses.MeanSquaredError(\n",
    "        reduction=tf.keras.losses.Reduction.SUM\n",
    "    )\n",
    "    print(max_bound, \"A:SNV:ANDB\", pred)\n",
    "    dist_max = mse_loss_(max_bound, pred)\n",
    "    dist_min = mse_loss_(min_bound, pred)\n",
    "    loss = dist_max + dist_min\n",
    "    print(\"LOSS\", loss)\n",
    "    return loss\n",
    "max_bound = np.transpose(max_bound)\n",
    "min_bound = np.transpose(min_bound)\n",
    "X_test_exog = random.choice(dataset.X_test_exog)\n",
    "X = dataset.X_train_exog\n",
    "X = random.choice(X)\n",
    "X = tf.Variable(tf.convert_to_tensor(X, dtype=tf.float32), dtype=tf.float32)\n",
    "grad_X_e = tf.convert_to_tensor(X_test_exog, dtype=tf.float32)\n",
    "grad_X_e = tf.Variable(grad_X_e, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(grad_X_e)\n",
    "    # Calculate the value of the function and record the gradient\n",
    "    pred = tf_model(tf.expand_dims(grad_X_e, axis=0))\n",
    "    pred = tf.squeeze(pred)\n",
    "    loss = compute_loss(max_bound, min_bound, pred)\n",
    "    print(loss)\n",
    "#pred = tf_model(grad_X_e)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05,  epsilon=1e-07,)\n",
    "max_iter = 100\n",
    "it = 0\n",
    "while (tf.reduce_any(pred_tf>max_bound) or tf.reduce_any(pred_tf<min_bound)) and (it<max_iter):\n",
    "    #change (X_e)\n",
    "    gradient = tape.gradient(loss, grad_X_e)\n",
    "    print(gradient)\n",
    "    if gradient is None:\n",
    "        print(\"no gradient\")\n",
    "        #break\n",
    "\n",
    "    # Use the Adam optimizer to update the value of x\n",
    "    optimizer.apply_gradients([(gradient, grad_X_e)])\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(grad_X_e)\n",
    "        # Calculate the value of the function and record the gradient\n",
    "        pred_tf = tf_model(tf.expand_dims(grad_X_e, axis=0))\n",
    "        pred_tf = tf.squeeze(pred_tf)\n",
    "        loss = compute_loss(max_bound, min_bound, pred_tf)\n",
    "    # Record the current value of x\n",
    "    \n",
    "\n",
    "    mean_smape, mean_rmse = forecast_metrics(dataset, pred_tf)\n",
    "    print(\n",
    "        f\"model trained, with test sMAPE score {mean_smape:0.4f}; test RMSE score: {mean_rmse:0.4f}.\"\n",
    "    )\n",
    "    print(f\"Iteration {it}, Loss: {loss.numpy()}, Grad_X_e: {grad_X_e.numpy()}\")\n",
    "    it += 1\n",
    "    \n",
    "print(\"Optimized value of x:\", grad_X_e.numpy())\n",
    "final_pred = tf_model(tf.expand_dims(grad_X_e, axis=0))\n",
    "print(\"Value of the function at the optimized point:\", final_pred.numpy())\n",
    "print(\"ADGAB\", tf.squeeze(final_pred, axis=-1).numpy()[:,:][0])\n",
    "#exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "698a62ad-0f7c-46c2-bec2-8dbcddb20dd6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[271.81447083]\n",
      " [265.30454338]\n",
      " [283.24752957]\n",
      " [281.23268789]\n",
      " [283.56020987]\n",
      " [248.46681195]\n",
      " [250.65565252]\n",
      " [204.38113442]\n",
      " [216.64419317]\n",
      " [202.79713267]\n",
      " [219.86688876]\n",
      " [184.10921434]\n",
      " [207.86753452]\n",
      " [212.56277883]\n",
      " [189.2408776 ]\n",
      " [217.81378812]\n",
      " [173.93123022]\n",
      " [201.95641583]\n",
      " [162.55615804]\n",
      " [170.92149845]\n",
      " [121.62773776]\n",
      " [187.70519471]\n",
      " [117.77584782]\n",
      " [ 92.01780032]] [[291.4876     283.89165072 276.29570145 268.69975217 261.1038029\n",
      "  253.50785362 245.91190435 238.31595507 230.7200058  223.12405652\n",
      "  215.52810725 207.93215797 200.3362087  192.74025942 185.14431014\n",
      "  177.54836087 169.95241159 162.35646232 154.76051304 147.16456377\n",
      "  139.56861449 131.97266522 124.37671594 116.78076667]] [[304.3844     296.78845072 289.19250145 281.59655217 274.0006029\n",
      "  266.40465362 258.80870435 251.21275507 243.6168058  236.02085652\n",
      "  228.42490725 220.82895797 213.2330087  205.63705942 198.04111014\n",
      "  190.44516087 182.84921159 175.25326232 167.65731304 160.06136377\n",
      "  152.46541449 144.86946522 137.27351594 129.67756667]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(final_pred)):\n",
    "    scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "    Y_preds_original_gru = scaler.inverse_transform(final_pred[i])\n",
    "    min_bound_true = scaler.inverse_transform(min_bound)\n",
    "    max_bound_true = scaler.inverse_transform(max_bound)\n",
    "\n",
    "print(Y_preds_original_gru, min_bound_true, max_bound_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b5530ae-2df2-432a-bf3c-53bfaeea4293",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        patient_id  glucose  basal  bolus  carbs  exercise_intensity  \\\n",
      "0              540      NaN   0.80    0.0      0                 0.0   \n",
      "2              540      NaN   1.05    0.8      0                 0.0   \n",
      "4              540      NaN   0.95    0.0      0                 0.0   \n",
      "5              540      NaN   0.95    0.0      0                 0.0   \n",
      "6              540      NaN   0.95    0.0      0                 0.0   \n",
      "...            ...      ...    ...    ...    ...                 ...   \n",
      "148702         591    266.0   0.98    4.1      0                 0.0   \n",
      "148703         591    275.0   0.98    0.0      0                 0.0   \n",
      "148704         591    268.0   0.98    0.0      0                 0.0   \n",
      "148705         591    301.0   0.98    0.0      0                 0.0   \n",
      "148706         591    290.0   0.98    0.0      0                 0.0   \n",
      "\n",
      "                      time  \n",
      "0      2027-05-19 00:00:00  \n",
      "2      2027-05-19 07:15:00  \n",
      "4      2027-05-19 09:55:00  \n",
      "5      2027-05-19 10:00:00  \n",
      "6      2027-05-19 10:05:00  \n",
      "...                    ...  \n",
      "148702 2022-01-13 23:35:00  \n",
      "148703 2022-01-13 23:40:00  \n",
      "148704 2022-01-13 23:45:00  \n",
      "148705 2022-01-13 23:50:00  \n",
      "148706 2022-01-13 23:55:00  \n",
      "\n",
      "[148284 rows x 7 columns]         patient_id  glucose  basal  bolus  carbs  exercise_intensity  \\\n",
      "0              540      NaN   0.80    0.0      0                 0.0   \n",
      "2              540      NaN   1.05    0.8      0                 0.0   \n",
      "4              540      NaN   0.95    0.0      0                 0.0   \n",
      "5              540      NaN   0.95    0.0      0                 0.0   \n",
      "6              540      NaN   0.95    0.0      0                 0.0   \n",
      "...            ...      ...    ...    ...    ...                 ...   \n",
      "151582         591      NaN   0.98    0.0      0                 0.0   \n",
      "151583         591      NaN   0.98    0.0      0                 0.0   \n",
      "151584         591      NaN   0.98    0.0      0                 0.0   \n",
      "151585         591      NaN   0.98    0.0      0                 0.0   \n",
      "151586         591      NaN   0.98    0.0      0                 0.0   \n",
      "\n",
      "                      time  \n",
      "0      2027-05-19 00:00:00  \n",
      "2      2027-05-19 07:15:00  \n",
      "4      2027-05-19 09:55:00  \n",
      "5      2027-05-19 10:00:00  \n",
      "6      2027-05-19 10:05:00  \n",
      "...                    ...  \n",
      "151582 2022-01-23 23:35:00  \n",
      "151583 2022-01-23 23:40:00  \n",
      "151584 2022-01-23 23:45:00  \n",
      "151585 2022-01-23 23:50:00  \n",
      "151586 2022-01-23 23:55:00  \n",
      "\n",
      "[151164 rows x 7 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT\n",
    "print(orig_train, orig_test)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "X_test_exog = random.choice(dataset.X_test_exog)\n",
    "for i in range(len(X_test_exog)):\n",
    "    scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "    X_test_exog_true = scaler.inverse_transform(X_test_exog)\n",
    "plt.plot(X_test_exog_true, label='exog1')#data[look_back+1:][0]\n",
    "plt.plot(Y_preds_original_gru, label='target', color='r')\n",
    "plt.plot(min_bound_true, label='min_bound', color='black')\n",
    "plt.plot(max_bound_true, label='max_bound', color='black')\n",
    "#plt.plot(test, label='actual', color='g')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Time-Series Forecasting')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#Y_preds_original_gru, min_bound_true, max_bound_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "226a4ce7-ac4c-4e12-97e9-7fb926b3368d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "fig.suptitle('Data')\n",
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "plot_data = orig_train[orig_train.patient_id == 540]\n",
    "ax1.plot(plot_data.glucose.tail(40), label='target', color='b')\n",
    "ax2.plot(plot_data.bolus.tail(40), label='bolus', color='r')\n",
    "ax2.plot(plot_data.basal.tail(40), label='basal', color='b')\n",
    "ax2.plot(plot_data.exercise_intensity.tail(24), label='exercise', color='g')\n",
    "ax2.plot(plot_data.carbs.tail(40), label='carbs', color='y')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig('test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3b591-a811-48d3-9a74-2a8fcba1e0d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cfc6af6-e1f6-45f1-88e7-7b04fd35e280",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07598784]\n",
      " [0.06990881]\n",
      " [0.07902736]\n",
      " [0.09422492]\n",
      " [0.11550152]\n",
      " [0.15197568]\n",
      " [0.17933131]\n",
      " [0.21276596]\n",
      " [0.24620061]\n",
      " [0.27659574]\n",
      " [0.29483283]\n",
      " [0.30395137]\n",
      " [0.30395137]\n",
      " [0.31306991]\n",
      " [0.3100304 ]\n",
      " [0.29179331]\n",
      " [0.25835866]\n",
      " [0.2674772 ]\n",
      " [0.26443769]\n",
      " [0.2674772 ]\n",
      " [0.27051672]\n",
      " [0.27051672]\n",
      " [0.26443769]\n",
      " [0.25531915]] [[0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.48245614 0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.9047619  0.         0.         0.        ]\n",
      " [0.9047619  0.         0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/statsmodels/tsa/statespace/mlemodel.py:3014: RuntimeWarning: invalid value encountered in divide\n",
      "  return self.params / self.bse\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/statsmodels/tsa/statespace/representation.py:374: FutureWarning: Unknown keyword arguments: dict_keys(['start_params']).Passing unknown keyword arguments will raise a TypeError beginning in version 0.15.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   24\n",
      "Model:               SARIMAX(1, 0, 0)   Log Likelihood                  59.406\n",
      "Date:                Fri, 13 Dec 2024   AIC                           -106.811\n",
      "Time:                        11:49:21   BIC                            -99.743\n",
      "Sample:                             0   HQIC                          -104.936\n",
      "                                 - 24                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.0320      0.028      1.146      0.252      -0.023       0.087\n",
      "x2            -0.0058      0.019     -0.312      0.755      -0.043       0.031\n",
      "const               0         -0        nan        nan           0           0\n",
      "x3                  0         -0        nan        nan           0           0\n",
      "ar.L1          0.9948      0.011     94.469      0.000       0.974       1.015\n",
      "sigma2         0.0003      0.000      2.845      0.004       0.000       0.001\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   7.42   Jarque-Bera (JB):                 0.38\n",
      "Prob(Q):                              0.01   Prob(JB):                         0.83\n",
      "Heteroskedasticity (H):               0.39   Skew:                            -0.30\n",
      "Prob(H) (two-sided):                  0.20   Kurtosis:                         2.92\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "[2] Covariance matrix is singular or near-singular, with condition number 7.02e+35. Standard errors may be unstable.\n",
      "[0.25406326 0.25281391 0.25157108 0.24751305 0.24910483 0.24788134\n",
      " 0.24666423 0.24545347 0.25872994 0.25753178 0.25346687 0.25228117\n",
      " 0.25110165 0.24992828 0.24876103 0.24759987 0.24644476 0.23368774\n",
      " 0.23254465 0.23140752 0.23027631 0.22915101 0.22803157 0.22691798]\n",
      "max_min [[0.8036    ]\n",
      " [0.78051201]\n",
      " [0.75742402]\n",
      " [0.73433602]\n",
      " [0.71124803]\n",
      " [0.68816004]\n",
      " [0.66507205]\n",
      " [0.64198406]\n",
      " [0.61889607]\n",
      " [0.59580807]\n",
      " [0.57272008]\n",
      " [0.54963209]\n",
      " [0.5265441 ]\n",
      " [0.50345611]\n",
      " [0.48036812]\n",
      " [0.45728012]\n",
      " [0.43419213]\n",
      " [0.41110414]\n",
      " [0.38801615]\n",
      " [0.36492816]\n",
      " [0.34184017]\n",
      " [0.31875217]\n",
      " [0.29566418]\n",
      " [0.27257619]] [[0.7644    ]\n",
      " [0.74131201]\n",
      " [0.71822402]\n",
      " [0.69513602]\n",
      " [0.67204803]\n",
      " [0.64896004]\n",
      " [0.62587205]\n",
      " [0.60278406]\n",
      " [0.57969607]\n",
      " [0.55660807]\n",
      " [0.53352008]\n",
      " [0.51043209]\n",
      " [0.4873441 ]\n",
      " [0.46425611]\n",
      " [0.44116812]\n",
      " [0.41808012]\n",
      " [0.39499213]\n",
      " [0.37190414]\n",
      " [0.34881615]\n",
      " [0.32572816]\n",
      " [0.30264017]\n",
      " [0.27955217]\n",
      " [0.25646418]\n",
      " [0.23337619]]\n",
      "0\n",
      "[0.2540844  0.25283505 0.25159222 0.24753164 0.24912597 0.24790249\n",
      " 0.24668538 0.24547462 0.25875727 0.2575591  0.25349161 0.2523059\n",
      " 0.25112638 0.24995301 0.24878576 0.2476246  0.24646949 0.23370888\n",
      " 0.23256579 0.23142866 0.23029746 0.22917215 0.22805272 0.22693912]\n",
      "1\n",
      "[0.25410556 0.25285621 0.25161338 0.24755024 0.24914713 0.24792365\n",
      " 0.24670654 0.24549578 0.2587846  0.25758643 0.25351634 0.25233063\n",
      " 0.25115111 0.24997774 0.24881049 0.24764933 0.24649422 0.23373004\n",
      " 0.23258695 0.23144982 0.23031862 0.22919331 0.22807388 0.22696028]\n",
      "2\n",
      "[0.25412673 0.25287739 0.25163456 0.24756885 0.24916831 0.24794482\n",
      " 0.24672771 0.24551695 0.25881192 0.25761375 0.25354106 0.25235536\n",
      " 0.25117584 0.25000247 0.24883522 0.24767406 0.24651895 0.23375122\n",
      " 0.23260813 0.23147099 0.23033979 0.22921449 0.22809505 0.22698146]\n",
      "3\n",
      "[0.25414792 0.25289858 0.25165575 0.24758748 0.2491895  0.24796601\n",
      " 0.2467489  0.24553814 0.25883924 0.25764108 0.25356579 0.25238008\n",
      " 0.25120056 0.25002719 0.24885994 0.24769878 0.24654367 0.23377241\n",
      " 0.23262932 0.23149219 0.23036098 0.22923568 0.22811624 0.22700265]\n",
      "4\n",
      "[0.25416913 0.25291979 0.25167696 0.24760612 0.24921071 0.24798722\n",
      " 0.24677011 0.24555935 0.25886657 0.2576684  0.25359051 0.2524048\n",
      " 0.25122528 0.25005191 0.24888466 0.2477235  0.24656839 0.23379362\n",
      " 0.23265052 0.23151339 0.23038219 0.22925689 0.22813745 0.22702385]\n",
      "5\n",
      "[0.25419035 0.25294101 0.25169818 0.24762478 0.24923193 0.24800844\n",
      " 0.24679133 0.24558057 0.25889389 0.25769572 0.25361522 0.25242952\n",
      " 0.25125    0.25007663 0.24890938 0.24774822 0.24659311 0.23381484\n",
      " 0.23267174 0.23153461 0.23040341 0.22927811 0.22815867 0.22704507]\n",
      "6\n",
      "[0.25421159 0.25296224 0.25171941 0.24764345 0.24925316 0.24802968\n",
      " 0.24681257 0.24560181 0.2589212  0.25772304 0.25363994 0.25245424\n",
      " 0.25127472 0.25010135 0.2489341  0.24777293 0.24661782 0.23383607\n",
      " 0.23269298 0.23155585 0.23042465 0.22929934 0.22817991 0.22706631]\n",
      "7\n",
      "[0.25423284 0.2529835  0.25174067 0.24766213 0.24927442 0.24805093\n",
      " 0.24683382 0.24562306 0.25894852 0.25775035 0.25366465 0.25247895\n",
      " 0.25129943 0.25012606 0.24895881 0.24779764 0.24664254 0.23385733\n",
      " 0.23271423 0.2315771  0.2304459  0.2293206  0.22820116 0.22708756]\n",
      "8\n",
      "[0.25425411 0.25300477 0.25176194 0.24768083 0.24929569 0.2480722\n",
      " 0.24685509 0.24564433 0.25897584 0.25777767 0.25368936 0.25250366\n",
      " 0.25132414 0.25015077 0.24898352 0.24782235 0.24666725 0.2338786\n",
      " 0.2327355  0.23159837 0.23046717 0.22934186 0.22822243 0.22710883]\n",
      "9\n",
      "[0.25427539 0.25302605 0.25178322 0.24769954 0.24931697 0.24809348\n",
      " 0.24687637 0.24566561 0.25900315 0.25780498 0.25371407 0.25252836\n",
      " 0.25134884 0.25017547 0.24900822 0.24784706 0.24669195 0.23389988\n",
      " 0.23275679 0.23161965 0.23048845 0.22936315 0.22824371 0.22713011]\n",
      "10\n",
      "[0.25429669 0.25304735 0.25180452 0.24771827 0.24933827 0.24811478\n",
      " 0.24689767 0.24568691 0.25903046 0.2578323  0.25373877 0.25255307\n",
      " 0.25137355 0.25020018 0.24903293 0.24787176 0.24671665 0.23392118\n",
      " 0.23277808 0.23164095 0.23050975 0.22938445 0.22826501 0.22715141]\n",
      "11\n",
      "[0.25431801 0.25306866 0.25182583 0.24773701 0.24935958 0.24813609\n",
      " 0.24691899 0.24570822 0.25905777 0.25785961 0.25376347 0.25257777\n",
      " 0.25139825 0.25022488 0.24905763 0.24789646 0.24674135 0.23394249\n",
      " 0.2327994  0.23166227 0.23053106 0.22940576 0.22828633 0.22717273]\n",
      "12\n",
      "[0.25433934 0.25308999 0.25184716 0.24775576 0.24938091 0.24815742\n",
      " 0.24694032 0.24572955 0.25908508 0.25788691 0.25378817 0.25260246\n",
      " 0.25142294 0.25024957 0.24908232 0.24792116 0.24676605 0.23396382\n",
      " 0.23282073 0.2316836  0.23055239 0.22942709 0.22830766 0.22719406]\n",
      "13\n",
      "[0.25436068 0.25311134 0.25186851 0.24777453 0.24940226 0.24817877\n",
      " 0.24696166 0.2457509  0.25911239 0.25791422 0.25381286 0.25262716\n",
      " 0.25144764 0.25027427 0.24910702 0.24794585 0.24679074 0.23398517\n",
      " 0.23284207 0.23170494 0.23057374 0.22944844 0.228329   0.2272154 ]\n",
      "14\n",
      "[0.25438204 0.2531327  0.25188987 0.24779331 0.24942362 0.24820013\n",
      " 0.24698302 0.24577226 0.25913969 0.25794152 0.25383755 0.25265185\n",
      " 0.25147233 0.25029896 0.24913171 0.24797054 0.24681543 0.23400653\n",
      " 0.23286343 0.2317263  0.2305951  0.2294698  0.22835036 0.22723676]\n",
      "15\n",
      "[0.25440342 0.25315407 0.25191124 0.24781211 0.24944499 0.24822151\n",
      " 0.2470044  0.24579364 0.25916699 0.25796883 0.25386224 0.25267653\n",
      " 0.25149701 0.25032364 0.24915639 0.24799523 0.24684012 0.2340279\n",
      " 0.23288481 0.23174768 0.23061648 0.22949117 0.22837174 0.22725814]\n",
      "16\n",
      "[0.25442481 0.25317546 0.25193264 0.24783092 0.24946638 0.2482429\n",
      " 0.24702579 0.24581503 0.25919429 0.25799613 0.25388692 0.25270122\n",
      " 0.2515217  0.25034833 0.24918108 0.24801991 0.2468648  0.2340493\n",
      " 0.2329062  0.23176907 0.23063787 0.22951256 0.22839313 0.22727953]\n",
      "17\n",
      "[0.25444622 0.25319687 0.25195404 0.24784975 0.24948779 0.2482643\n",
      " 0.2470472  0.24583643 0.25922159 0.25802342 0.2539116  0.2527259\n",
      " 0.25154638 0.25037301 0.24920576 0.24804459 0.24688948 0.2340707\n",
      " 0.23292761 0.23179048 0.23065927 0.22953397 0.22841454 0.22730094]\n",
      "18\n",
      "[0.25446764 0.25321829 0.25197546 0.24786859 0.24950921 0.24828573\n",
      " 0.24706862 0.24585786 0.25924888 0.25805072 0.25393627 0.25275057\n",
      " 0.25157105 0.25039768 0.24923043 0.24806927 0.24691416 0.23409212\n",
      " 0.23294903 0.2318119  0.2306807  0.22955539 0.22843596 0.22732236]\n",
      "19\n",
      "[0.25448908 0.25323973 0.2519969  0.24788744 0.24953065 0.24830716\n",
      " 0.24709006 0.24587929 0.25927618 0.25807801 0.25396095 0.25277524\n",
      " 0.25159572 0.25042235 0.2492551  0.24809394 0.24693883 0.23411356\n",
      " 0.23297047 0.23183334 0.23070213 0.22957683 0.2284574  0.2273438 ]\n",
      "20\n",
      "[0.25451053 0.25326118 0.25201835 0.24790631 0.2495521  0.24832862\n",
      " 0.24711151 0.24590075 0.25930347 0.2581053  0.25398562 0.25279991\n",
      " 0.25162039 0.25044702 0.24927977 0.24811861 0.2469635  0.23413501\n",
      " 0.23299192 0.23185479 0.23072359 0.22959828 0.22847885 0.22736525]\n",
      "21\n",
      "[0.254532   0.25328265 0.25203982 0.24792519 0.24957357 0.24835008\n",
      " 0.24713298 0.24592222 0.25933076 0.25813259 0.25401028 0.25282458\n",
      " 0.25164506 0.25047169 0.24930444 0.24814327 0.24698817 0.23415648\n",
      " 0.23301339 0.23187626 0.23074506 0.22961975 0.22850032 0.22738672]\n",
      "22\n",
      "[0.25455348 0.25330414 0.25206131 0.24794409 0.24959506 0.24837157\n",
      " 0.24715446 0.2459437  0.25935804 0.25815988 0.25403494 0.25284924\n",
      " 0.25166972 0.25049635 0.2493291  0.24816794 0.24701283 0.23417797\n",
      " 0.23303487 0.23189774 0.23076654 0.22964124 0.2285218  0.2274082 ]\n",
      "23\n",
      "[0.25457498 0.25332564 0.25208281 0.247963   0.24961656 0.24839307\n",
      " 0.24717596 0.2459652  0.25938532 0.25818716 0.2540596  0.2528739\n",
      " 0.25169438 0.25052101 0.24935376 0.24819259 0.24703748 0.23419947\n",
      " 0.23305637 0.23191924 0.23078804 0.22966274 0.2285433  0.2274297 ]\n",
      "24\n",
      "[0.25459649 0.25334715 0.25210432 0.24798192 0.24963807 0.24841458\n",
      " 0.24719747 0.24598671 0.25941261 0.25821444 0.25408425 0.25289855\n",
      " 0.25171903 0.25054566 0.24937841 0.24821725 0.24706214 0.23422098\n",
      " 0.23307789 0.23194076 0.23080955 0.22968425 0.22856481 0.22745122]\n",
      "25\n",
      "[0.25461802 0.25336868 0.25212585 0.24800086 0.2496596  0.24843611\n",
      " 0.247219   0.24600824 0.25943988 0.25824172 0.2541089  0.2529232\n",
      " 0.25174368 0.25057031 0.24940306 0.2482419  0.24708679 0.23424251\n",
      " 0.23309942 0.23196228 0.23083108 0.22970578 0.22858634 0.22747275]\n",
      "26\n",
      "[0.25463957 0.25339022 0.2521474  0.24801981 0.24968114 0.24845766\n",
      " 0.24724055 0.24602979 0.25946716 0.25826899 0.25413355 0.25294785\n",
      " 0.25176833 0.25059496 0.24942771 0.24826654 0.24711143 0.23426406\n",
      " 0.23312096 0.23198383 0.23085263 0.22972732 0.22860789 0.22749429]\n",
      "27\n",
      "[0.25466113 0.25341179 0.25216896 0.24803878 0.24970271 0.24847922\n",
      " 0.24726211 0.24605135 0.25949443 0.25829626 0.25415819 0.25297249\n",
      " 0.25179297 0.2506196  0.24945235 0.24829118 0.24713608 0.23428562\n",
      " 0.23314252 0.23200539 0.23087419 0.22974888 0.22862945 0.22751585]\n",
      "28\n",
      "[0.25468271 0.25343336 0.25219053 0.24805776 0.24972428 0.24850079\n",
      " 0.24728368 0.24607292 0.2595217  0.25832353 0.25418283 0.25299712\n",
      " 0.2518176  0.25064424 0.24947698 0.24831582 0.24716071 0.23430719\n",
      " 0.2331641  0.23202697 0.23089576 0.22977046 0.22865103 0.22753743]\n",
      "29\n",
      "[0.2547043  0.25345495 0.25221212 0.24807675 0.24974587 0.24852238\n",
      " 0.24730528 0.24609451 0.25954896 0.2583508  0.25420746 0.25302176\n",
      " 0.25184224 0.25066887 0.24950162 0.24834045 0.24718535 0.23432878\n",
      " 0.23318569 0.23204856 0.23091735 0.22979205 0.22867262 0.22755902]\n",
      "30\n",
      "[0.2547259  0.25347656 0.25223373 0.24809576 0.24976748 0.24854399\n",
      " 0.24732688 0.24611612 0.25957623 0.25837806 0.25423209 0.25304639\n",
      " 0.25186687 0.2506935  0.24952625 0.24836508 0.24720997 0.23435039\n",
      " 0.2332073  0.23207016 0.23093896 0.22981366 0.22869422 0.22758062]\n",
      "31\n",
      "[0.25474752 0.25349818 0.25225535 0.24811478 0.2497891  0.24856561\n",
      " 0.2473485  0.24613774 0.25960349 0.25840532 0.25425671 0.25307101\n",
      " 0.25189149 0.25071812 0.24955087 0.24838971 0.2472346  0.23437201\n",
      " 0.23322892 0.23209178 0.23096058 0.22983528 0.22871584 0.22760225]\n",
      "32\n",
      "[0.25476916 0.25351982 0.25227699 0.24813382 0.24981074 0.24858725\n",
      " 0.24737014 0.24615938 0.25963074 0.25843258 0.25428133 0.25309563\n",
      " 0.25191611 0.25074274 0.24957549 0.24841433 0.24725922 0.23439365\n",
      " 0.23325055 0.23211342 0.23098222 0.22985692 0.22873748 0.22762388]\n",
      "33\n",
      "[0.25479081 0.25354147 0.25229864 0.24815287 0.24983239 0.2486089\n",
      " 0.24739179 0.24618103 0.259658   0.25845983 0.25430595 0.25312025\n",
      " 0.25194073 0.25076736 0.24960011 0.24843894 0.24728384 0.2344153\n",
      " 0.23327221 0.23213507 0.23100387 0.22987857 0.22875913 0.22764553]\n",
      "34\n",
      "[0.25481248 0.25356314 0.25232031 0.24817193 0.24985406 0.24863057\n",
      " 0.24741346 0.2462027  0.25968525 0.25848708 0.25433056 0.25314486\n",
      " 0.25196534 0.25079197 0.24962472 0.24846356 0.24730845 0.23443697\n",
      " 0.23329387 0.23215674 0.23102554 0.22990024 0.2287808  0.2276672 ]\n",
      "35\n",
      "[0.25483416 0.25358482 0.25234199 0.24819101 0.24987574 0.24865225\n",
      " 0.24743514 0.24622438 0.2597125  0.25851433 0.25435517 0.25316947\n",
      " 0.25198995 0.25081658 0.24964933 0.24848816 0.24733305 0.23445865\n",
      " 0.23331556 0.23217842 0.23104722 0.22992192 0.22880248 0.22768888]\n",
      "36\n",
      "[0.25485586 0.25360652 0.25236369 0.2482101  0.24989744 0.24867395\n",
      " 0.24745684 0.24624608 0.25973974 0.25854157 0.25437977 0.25319407\n",
      " 0.25201455 0.25084118 0.24967393 0.24851277 0.24735766 0.23448035\n",
      " 0.23333725 0.23220012 0.23106892 0.22994362 0.22882418 0.22771058]\n",
      "37\n",
      "[0.25487757 0.25362823 0.2523854  0.24822921 0.24991915 0.24869566\n",
      " 0.24747855 0.24626779 0.25976698 0.25856881 0.25440437 0.25321867\n",
      " 0.25203915 0.25086578 0.24969853 0.24853736 0.24738225 0.23450206\n",
      " 0.23335897 0.23222183 0.23109063 0.22996533 0.22884589 0.2277323 ]\n",
      "38\n",
      "[0.2548993  0.25364996 0.25240713 0.24824833 0.24994088 0.24871739\n",
      " 0.24750028 0.24628952 0.25979422 0.25859605 0.25442896 0.25324326\n",
      " 0.25206374 0.25089037 0.24972312 0.24856196 0.24740685 0.23452379\n",
      " 0.23338069 0.23224356 0.23111236 0.22998706 0.22886762 0.22775402]\n",
      "39\n",
      "[0.25492105 0.2536717  0.25242887 0.24826746 0.24996262 0.24873913\n",
      " 0.24752203 0.24631126 0.25982145 0.25862328 0.25445355 0.25326785\n",
      " 0.25208833 0.25091496 0.24974771 0.24858654 0.24743144 0.23454553\n",
      " 0.23340244 0.23226531 0.2311341  0.2300088  0.22888937 0.22777577]\n",
      "40\n",
      "[0.2549428  0.25369346 0.25245063 0.24828661 0.24998438 0.24876089\n",
      " 0.24754378 0.24633302 0.25984868 0.25865051 0.25447813 0.25329243\n",
      " 0.25211291 0.25093954 0.24977229 0.24861113 0.24745602 0.23456729\n",
      " 0.2334242  0.23228707 0.23115586 0.23003056 0.22891112 0.22779753]\n",
      "41\n",
      "[0.25496458 0.25371523 0.2524724  0.24830577 0.25000615 0.24878267\n",
      " 0.24756556 0.2463548  0.25987591 0.25867774 0.25450271 0.25331701\n",
      " 0.25213749 0.25096412 0.24979687 0.24863571 0.2474806  0.23458906\n",
      " 0.23344597 0.23230884 0.23117764 0.23005233 0.2289329  0.2278193 ]\n",
      "42\n",
      "[0.25498637 0.25373702 0.25249419 0.24832495 0.25002794 0.24880445\n",
      " 0.24758735 0.24637659 0.25990313 0.25870496 0.25452729 0.25334158\n",
      " 0.25216206 0.25098869 0.24982144 0.24866028 0.24750517 0.23461085\n",
      " 0.23346776 0.23233063 0.23119943 0.23007412 0.22895469 0.22784109]\n",
      "43\n",
      "[0.25500817 0.25375883 0.252516   0.24834414 0.25004975 0.24882626\n",
      " 0.24760915 0.24639839 0.25993035 0.25873218 0.25455186 0.25336615\n",
      " 0.25218663 0.25101326 0.24984601 0.24868485 0.24752974 0.23463266\n",
      " 0.23348956 0.23235243 0.23122123 0.23009593 0.22897649 0.22786289]\n",
      "44\n",
      "[0.25502999 0.25378065 0.25253782 0.24836334 0.25007157 0.24884808\n",
      " 0.24763097 0.24642021 0.25995756 0.2587594  0.25457642 0.25339072\n",
      " 0.2522112  0.25103783 0.24987058 0.24870941 0.2475543  0.23465448\n",
      " 0.23351138 0.23237425 0.23124305 0.23011775 0.22899831 0.22788471]\n",
      "45\n",
      "[0.25505183 0.25380248 0.25255965 0.24838256 0.2500934  0.24886991\n",
      " 0.2476528  0.24644204 0.25998477 0.25878661 0.25460098 0.25341527\n",
      " 0.25223575 0.25106238 0.24989513 0.24873397 0.24757886 0.23467631\n",
      " 0.23353322 0.23239609 0.23126488 0.23013958 0.22902015 0.22790655]\n",
      "46\n",
      "[0.25507368 0.25382433 0.2525815  0.24840179 0.25011525 0.24889176\n",
      " 0.24767465 0.24646389 0.26001198 0.25881381 0.25462553 0.25343983\n",
      " 0.25226031 0.25108694 0.24991969 0.24875852 0.24760342 0.23469816\n",
      " 0.23355507 0.23241794 0.23128673 0.23016143 0.22904199 0.2279284 ]\n",
      "47\n",
      "[0.25509554 0.2538462  0.25260337 0.24842104 0.25013712 0.24891363\n",
      " 0.24769652 0.24648576 0.26003918 0.25884102 0.25465008 0.25346438\n",
      " 0.25228486 0.25111149 0.24994424 0.24878307 0.24762796 0.23472003\n",
      " 0.23357693 0.2324398  0.2313086  0.23018329 0.22906386 0.22795026]\n",
      "48\n",
      "[0.25511742 0.25386808 0.25262525 0.2484403  0.250159   0.24893551\n",
      " 0.2477184  0.24650764 0.26006638 0.25886821 0.25467462 0.25348892\n",
      " 0.2523094  0.25113603 0.24996878 0.24880762 0.24765251 0.23474191\n",
      " 0.23359881 0.23246168 0.23133048 0.23020517 0.22908574 0.22797214]\n",
      "49\n",
      "[0.25513932 0.25388997 0.25264714 0.24845957 0.25018089 0.2489574\n",
      " 0.24774029 0.24652953 0.26009357 0.25889541 0.25469916 0.25351346\n",
      " 0.25233394 0.25116057 0.24999332 0.24883215 0.24767704 0.2347638\n",
      " 0.23362071 0.23248358 0.23135237 0.23022707 0.22910763 0.22799404]\n",
      "50\n",
      "[0.25516123 0.25391188 0.25266905 0.24847886 0.2502028  0.24897931\n",
      " 0.2477622  0.24655144 0.26012077 0.2589226  0.25472369 0.25353799\n",
      " 0.25235847 0.2511851  0.25001785 0.24885669 0.24770158 0.23478571\n",
      " 0.23364262 0.23250549 0.23137428 0.23024898 0.22912954 0.22801595]\n",
      "51\n",
      "[0.25518315 0.25393381 0.25269098 0.24849816 0.25022473 0.24900124\n",
      " 0.24778413 0.24657337 0.26014795 0.25894978 0.25474822 0.25356252\n",
      " 0.252383   0.25120963 0.25004238 0.24888121 0.2477261  0.23480764\n",
      " 0.23366454 0.23252741 0.23139621 0.23027091 0.22915147 0.22803787]\n",
      "52\n",
      "[0.25520509 0.25395575 0.25271292 0.24851748 0.25024667 0.24902318\n",
      " 0.24780607 0.24659531 0.26017513 0.25897697 0.25477274 0.25358704\n",
      " 0.25240752 0.25123415 0.2500669  0.24890573 0.24775062 0.23482958\n",
      " 0.23368648 0.23254935 0.23141815 0.23029285 0.22917341 0.22805981]\n",
      "53\n",
      "[0.25522705 0.2539777  0.25273487 0.24853681 0.25026862 0.24904513\n",
      " 0.24782803 0.24661726 0.26020231 0.25900414 0.25479726 0.25361155\n",
      " 0.25243203 0.25125866 0.25009141 0.24893025 0.24777514 0.23485153\n",
      " 0.23370844 0.23257131 0.2314401  0.2303148  0.22919537 0.22808177]\n",
      "54\n",
      "[0.25524902 0.25399967 0.25275684 0.24855615 0.25029059 0.2490671\n",
      " 0.24785    0.24663923 0.26022948 0.25903132 0.25482177 0.25363606\n",
      " 0.25245654 0.25128317 0.25011592 0.24895476 0.24779965 0.2348735\n",
      " 0.23373041 0.23259328 0.23146207 0.23033677 0.22921734 0.22810374]\n",
      "55\n",
      "[0.255271   0.25402166 0.25277883 0.24857551 0.25031258 0.24908909\n",
      " 0.24787198 0.24666122 0.26025665 0.25905848 0.25484627 0.25366057\n",
      " 0.25248105 0.25130768 0.25014043 0.24897926 0.24782416 0.23489549\n",
      " 0.23375239 0.23261526 0.23148406 0.23035876 0.22923932 0.22812572]\n",
      "56\n",
      "[0.255293   0.25404366 0.25280083 0.24859488 0.25033458 0.24911109\n",
      " 0.24789398 0.24668322 0.26028381 0.25908565 0.25487077 0.25368507\n",
      " 0.25250555 0.25133218 0.25016493 0.24900376 0.24784865 0.23491749\n",
      " 0.2337744  0.23263726 0.23150606 0.23038076 0.22926132 0.22814772]\n",
      "57\n",
      "[0.25531502 0.25406567 0.25282284 0.24861427 0.25035659 0.24913311\n",
      " 0.247916   0.24670524 0.26031097 0.25911281 0.25489526 0.25370956\n",
      " 0.25253004 0.25135667 0.25018942 0.24902826 0.24787315 0.2349395\n",
      " 0.23379641 0.23265928 0.23152808 0.23040277 0.22928334 0.22816974]\n",
      "58\n",
      "[0.25533705 0.2540877  0.25284487 0.24863367 0.25037862 0.24915514\n",
      " 0.24793803 0.24672727 0.26033813 0.25913996 0.25491975 0.25373405\n",
      " 0.25255453 0.25138116 0.25021391 0.24905274 0.24789763 0.23496153\n",
      " 0.23381844 0.23268131 0.23155011 0.2304248  0.22930537 0.22819177]\n",
      "59\n",
      "[0.25535909 0.25410975 0.25286692 0.24865308 0.25040067 0.24917718\n",
      " 0.24796007 0.24674931 0.26036528 0.25916711 0.25494423 0.25375853\n",
      " 0.25257901 0.25140564 0.25023839 0.24907722 0.24792212 0.23498358\n",
      " 0.23384049 0.23270335 0.23157215 0.23044685 0.22932741 0.22821382]\n",
      "60\n",
      "[0.25538115 0.25413181 0.25288898 0.2486725  0.25042273 0.24919924\n",
      " 0.24798213 0.24677137 0.26039242 0.25919425 0.25496871 0.253783\n",
      " 0.25260348 0.25143011 0.25026286 0.2491017  0.24794659 0.23500564\n",
      " 0.23386255 0.23272542 0.23159421 0.23046891 0.22934947 0.22823588]\n",
      "61\n",
      "[0.25540323 0.25415389 0.25291106 0.24869195 0.25044481 0.24922132\n",
      " 0.24800421 0.24679345 0.26041956 0.25922139 0.25499317 0.25380747\n",
      " 0.25262795 0.25145458 0.25028733 0.24912617 0.24797106 0.23502772\n",
      " 0.23388462 0.23274749 0.23161629 0.23049099 0.22937155 0.22825795]\n",
      "62\n",
      "[0.25542532 0.25417598 0.25293315 0.2487114  0.2504669  0.24924341\n",
      " 0.2480263  0.24681554 0.26044669 0.25924853 0.25501764 0.25383193\n",
      " 0.25265241 0.25147905 0.2503118  0.24915063 0.24799552 0.23504981\n",
      " 0.23390671 0.23276958 0.23163838 0.23051308 0.22939364 0.22828004]\n",
      "63\n",
      "[0.25544743 0.25419808 0.25295525 0.24873087 0.250489   0.24926551\n",
      " 0.24804841 0.24683764 0.26047382 0.25927566 0.2550421  0.25385639\n",
      " 0.25267687 0.2515035  0.25033625 0.24917509 0.24801998 0.23507191\n",
      " 0.23392882 0.23279169 0.23166048 0.23053518 0.22941575 0.22830215]\n",
      "64\n",
      "[0.25546955 0.2542202  0.25297737 0.24875035 0.25051112 0.24928763\n",
      " 0.24807053 0.24685976 0.26050095 0.25930278 0.25506655 0.25388084\n",
      " 0.25270132 0.25152795 0.2503607  0.24919954 0.24804443 0.23509403\n",
      " 0.23395094 0.23281381 0.2316826  0.2305573  0.22943787 0.22832427]\n",
      "65\n",
      "[0.25549168 0.25424234 0.25299951 0.24876985 0.25053326 0.24930977\n",
      " 0.24809266 0.2468819  0.26052807 0.2593299  0.25509099 0.25390529\n",
      " 0.25272577 0.2515524  0.25038515 0.24922398 0.24806888 0.23511617\n",
      " 0.23397307 0.23283594 0.23170474 0.23057944 0.22946    0.2283464 ]\n",
      "66\n",
      "[0.25551383 0.25426449 0.25302166 0.24878936 0.25055541 0.24933192\n",
      " 0.24811481 0.24690405 0.26055518 0.25935701 0.25511543 0.25392973\n",
      " 0.25275021 0.25157684 0.25040959 0.24924842 0.24809331 0.23513832\n",
      " 0.23399522 0.23285809 0.23172689 0.23060159 0.22948215 0.22836855]\n",
      "67\n",
      "[0.255536   0.25428665 0.25304382 0.24880888 0.25057757 0.24935408\n",
      " 0.24813698 0.24692622 0.26058229 0.25938412 0.25513986 0.25395416\n",
      " 0.25277464 0.25160127 0.25043402 0.24927285 0.24811775 0.23516048\n",
      " 0.23401739 0.23288026 0.23174906 0.23062375 0.22950432 0.22839072]\n",
      "68\n",
      "[0.25555818 0.25430883 0.253066   0.24882842 0.25059975 0.24937626\n",
      " 0.24815916 0.2469484  0.26060939 0.25941123 0.25516429 0.25397858\n",
      " 0.25279906 0.25162569 0.25045844 0.24929728 0.24814217 0.23518266\n",
      " 0.23403957 0.23290244 0.23177124 0.23064593 0.2295265  0.2284129 ]\n",
      "69\n",
      "[0.25558037 0.25433103 0.2530882  0.24884797 0.25062195 0.24939846\n",
      " 0.24818135 0.24697059 0.26063649 0.25943832 0.25518871 0.254003\n",
      " 0.25282348 0.25165011 0.25048286 0.2493217  0.24816659 0.23520486\n",
      " 0.23406177 0.23292463 0.23179343 0.23066813 0.22954869 0.22843509]\n",
      "70\n",
      "[0.25560258 0.25435324 0.25311041 0.24886753 0.25064416 0.24942067\n",
      " 0.24820356 0.2469928  0.26066358 0.25946542 0.25521312 0.25402741\n",
      " 0.25284789 0.25167453 0.25050728 0.24934611 0.248191   0.23522707\n",
      " 0.23408397 0.23294684 0.23181564 0.23069034 0.2295709  0.2284573 ]\n",
      "71\n",
      "[0.25562481 0.25437546 0.25313263 0.24888711 0.25066638 0.24944289\n",
      " 0.24822579 0.24701503 0.26069067 0.2594925  0.25523752 0.25405182\n",
      " 0.2528723  0.25169893 0.25053168 0.24937052 0.24821541 0.23524929\n",
      " 0.2341062  0.23296907 0.23183787 0.23071256 0.22959313 0.22847953]\n",
      "72\n",
      "[0.25564705 0.2543977  0.25315487 0.2489067  0.25068862 0.24946513\n",
      " 0.24824803 0.24703726 0.26071775 0.25951959 0.25526192 0.25407622\n",
      " 0.2528967  0.25172333 0.25055608 0.24939492 0.24823981 0.23527153\n",
      " 0.23412844 0.23299131 0.2318601  0.2307348  0.22961537 0.22850177]\n",
      "73\n",
      "[0.2556693  0.25441996 0.25317713 0.24892631 0.25071088 0.24948739\n",
      " 0.24827028 0.24705952 0.26074483 0.25954666 0.25528632 0.25410061\n",
      " 0.25292109 0.25174772 0.25058047 0.24941931 0.2482642  0.23529379\n",
      " 0.23415069 0.23301356 0.23188236 0.23075706 0.22963762 0.22852402]\n",
      "74\n",
      "[0.25569157 0.25444223 0.2531994  0.24894593 0.25073315 0.24950966\n",
      " 0.24829255 0.24708179 0.2607719  0.25957373 0.2553107  0.254125\n",
      " 0.25294548 0.25177211 0.25060486 0.2494437  0.24828859 0.23531606\n",
      " 0.23417296 0.23303583 0.23190463 0.23077933 0.22965989 0.22854629]\n",
      "75\n",
      "[0.25571385 0.25446451 0.25322168 0.24896556 0.25075543 0.24953194\n",
      " 0.24831483 0.24710407 0.26079896 0.2596008  0.25533508 0.25414938\n",
      " 0.25296986 0.25179649 0.25062924 0.24946807 0.24831297 0.23533834\n",
      " 0.23419525 0.23305811 0.23192691 0.23080161 0.22968217 0.22856858]\n",
      "76\n",
      "[0.25573615 0.25448681 0.25324398 0.24898521 0.25077773 0.24955424\n",
      " 0.24833713 0.24712637 0.26082602 0.25962785 0.25535945 0.25417375\n",
      " 0.25299423 0.25182086 0.25065361 0.24949245 0.24833734 0.23536064\n",
      " 0.23421755 0.23308041 0.23194921 0.23082391 0.22970447 0.22859087]\n",
      "77\n",
      "[0.25575847 0.25450912 0.25326629 0.24900487 0.25080004 0.24957655\n",
      " 0.24835945 0.24714868 0.26085307 0.25965491 0.25538382 0.25419811\n",
      " 0.25301859 0.25184523 0.25067797 0.24951681 0.2483617  0.23538295\n",
      " 0.23423986 0.23310273 0.23197152 0.23084622 0.22972679 0.22861319]\n",
      "78\n",
      "[0.2557808  0.25453145 0.25328862 0.24902455 0.25082237 0.24959888\n",
      " 0.24838177 0.24717101 0.26088012 0.25968195 0.25540818 0.25422247\n",
      " 0.25304295 0.25186958 0.25070233 0.24954117 0.24838606 0.23540528\n",
      " 0.23426219 0.23312506 0.23199385 0.23086855 0.22974911 0.22863552]\n",
      "79\n",
      "[0.25580314 0.25455379 0.25331096 0.24904424 0.25084471 0.24962123\n",
      " 0.24840412 0.24719336 0.26090716 0.25970899 0.25543253 0.25424682\n",
      " 0.2530673  0.25189393 0.25072668 0.24956552 0.24841041 0.23542762\n",
      " 0.23428453 0.2331474  0.2320162  0.23089089 0.22977146 0.22865786]\n",
      "80\n",
      "[0.2558255  0.25457615 0.25333332 0.24906394 0.25086707 0.24964358\n",
      " 0.24842648 0.24721571 0.26093419 0.25973603 0.25545687 0.25427117\n",
      " 0.25309165 0.25191828 0.25075103 0.24958986 0.24843476 0.23544998\n",
      " 0.23430689 0.23316976 0.23203855 0.23091325 0.22979382 0.22868022]\n",
      "81\n",
      "[0.25584787 0.25459852 0.25335569 0.24908365 0.25088944 0.24966596\n",
      " 0.24844885 0.24723809 0.26096122 0.25976306 0.25548121 0.2542955\n",
      " 0.25311598 0.25194262 0.25077537 0.2496142  0.24845909 0.23547235\n",
      " 0.23432926 0.23319213 0.23206093 0.23093562 0.22981619 0.22870259]\n",
      "82\n",
      "[0.25587026 0.25462091 0.25337808 0.24910338 0.25091183 0.24968834\n",
      " 0.24847124 0.24726047 0.26098824 0.25979008 0.25550554 0.25431983\n",
      " 0.25314031 0.25196695 0.25079969 0.24963853 0.24848342 0.23549474\n",
      " 0.23435165 0.23321452 0.23208331 0.23095801 0.22983858 0.22872498]\n",
      "83\n",
      "[0.25589266 0.25464331 0.25340048 0.24912313 0.25093423 0.24971075\n",
      " 0.24849364 0.24728288 0.26101526 0.25981709 0.25552986 0.25434416\n",
      " 0.25316464 0.25199127 0.25082402 0.24966285 0.24850774 0.23551714\n",
      " 0.23437405 0.23323692 0.23210572 0.23098041 0.22986098 0.22874738]\n",
      "84\n",
      "[0.25591508 0.25466573 0.2534229  0.24914288 0.25095665 0.24973316\n",
      " 0.24851605 0.24730529 0.26104227 0.2598441  0.25555417 0.25436847\n",
      " 0.25318895 0.25201558 0.25084833 0.24968717 0.24853206 0.23553956\n",
      " 0.23439647 0.23325934 0.23212813 0.23100283 0.22988339 0.2287698 ]\n",
      "85\n",
      "[0.25593751 0.25468816 0.25344533 0.24916265 0.25097908 0.24975559\n",
      " 0.24853849 0.24732772 0.26106927 0.25987111 0.25557848 0.25439278\n",
      " 0.25321326 0.25203989 0.25087264 0.24971148 0.24855637 0.23556199\n",
      " 0.2344189  0.23328177 0.23215056 0.23102526 0.22990583 0.22879223]\n",
      "86\n",
      "[0.25595995 0.25471061 0.25346778 0.24918244 0.25100153 0.24977804\n",
      " 0.24856093 0.24735017 0.26109627 0.2598981  0.25560278 0.25441708\n",
      " 0.25323756 0.25206419 0.25089694 0.24973578 0.24858067 0.23558444\n",
      " 0.23444135 0.23330421 0.23217301 0.23104771 0.22992827 0.22881467]\n",
      "87\n",
      "[0.25598241 0.25473307 0.25349024 0.24920224 0.25102399 0.2498005\n",
      " 0.24858339 0.24737263 0.26112326 0.25992509 0.25562708 0.25444137\n",
      " 0.25326185 0.25208848 0.25092123 0.24976007 0.24860496 0.2356069\n",
      " 0.23446381 0.23332667 0.23219547 0.23107017 0.22995073 0.22883714]\n",
      "88\n",
      "[0.25600489 0.25475554 0.25351271 0.24922205 0.25104646 0.24982298\n",
      " 0.24860587 0.24739511 0.26115024 0.25995207 0.25565136 0.25446566\n",
      " 0.25328614 0.25211277 0.25094552 0.24978435 0.24862925 0.23562937\n",
      " 0.23448628 0.23334915 0.23221795 0.23109264 0.22997321 0.22885961]\n",
      "89\n",
      "[0.25602738 0.25477803 0.2535352  0.24924187 0.25106895 0.24984547\n",
      " 0.24862836 0.2474176  0.26117722 0.25997905 0.25567564 0.25448993\n",
      " 0.25331041 0.25213705 0.25096979 0.24980863 0.24865352 0.23565186\n",
      " 0.23450877 0.23337164 0.23224044 0.23111513 0.2299957  0.2288821 ]\n",
      "90\n",
      "[0.25604988 0.25480054 0.25355771 0.24926171 0.25109146 0.24986797\n",
      " 0.24865086 0.2474401  0.26120419 0.26000602 0.25569991 0.2545142\n",
      " 0.25333468 0.25216131 0.25099406 0.2498329  0.24867779 0.23567437\n",
      " 0.23453128 0.23339414 0.23226294 0.23113764 0.2300182  0.22890461]\n",
      "91\n",
      "[0.2560724  0.25482306 0.25358023 0.24928156 0.25111398 0.24989049\n",
      " 0.24867338 0.24746262 0.26123115 0.26003298 0.25572417 0.25453847\n",
      " 0.25335895 0.25218558 0.25101833 0.24985716 0.24870205 0.23569689\n",
      " 0.2345538  0.23341666 0.23228546 0.23116016 0.23004072 0.22892712]\n",
      "92\n",
      "[0.25609494 0.25484559 0.25360276 0.24930143 0.25113651 0.24991302\n",
      " 0.24869592 0.24748515 0.26125811 0.26005994 0.25574842 0.25456272\n",
      " 0.2533832  0.25220983 0.25104258 0.24988142 0.24872631 0.23571942\n",
      " 0.23457633 0.2334392  0.232308   0.23118269 0.23006326 0.22894966]\n",
      "93\n",
      "[0.25611749 0.25486814 0.25362531 0.24932131 0.25115906 0.24993557\n",
      " 0.24871846 0.2475077  0.26128506 0.26008689 0.25577267 0.25458697\n",
      " 0.25340745 0.25223408 0.25106683 0.24990566 0.24875055 0.23574197\n",
      " 0.23459888 0.23346175 0.23233054 0.23120524 0.2300858  0.22897221]\n",
      "94\n",
      "[0.25614005 0.2548907  0.25364787 0.2493412  0.25118162 0.24995814\n",
      " 0.24874103 0.24753027 0.261312   0.26011383 0.25579691 0.2546112\n",
      " 0.25343168 0.25225832 0.25109107 0.2499299  0.24877479 0.23576453\n",
      " 0.23462144 0.23348431 0.23235311 0.2312278  0.23010837 0.22899477]\n",
      "95\n",
      "[0.25616263 0.25491328 0.25367045 0.24936111 0.2512042  0.24998071\n",
      " 0.24876361 0.24755284 0.26133893 0.26014077 0.25582114 0.25463544\n",
      " 0.25345592 0.25228255 0.2511153  0.24995413 0.24879902 0.23578711\n",
      " 0.23464402 0.23350689 0.23237568 0.23125038 0.23013095 0.22901735]\n",
      "96\n",
      "[0.25618522 0.25493587 0.25369304 0.24938103 0.25122679 0.25000331\n",
      " 0.2487862  0.24757544 0.26136586 0.2601677  0.25584536 0.25465966\n",
      " 0.25348014 0.25230677 0.25113952 0.24997835 0.24882325 0.2358097\n",
      " 0.23466661 0.23352948 0.23239828 0.23127297 0.23015354 0.22903994]\n",
      "97\n",
      "[0.25620782 0.25495848 0.25371565 0.24940096 0.2512494  0.25002591\n",
      " 0.2488088  0.24759804 0.26139278 0.26019462 0.25586958 0.25468387\n",
      " 0.25350435 0.25233098 0.25116373 0.25000257 0.24884746 0.23583231\n",
      " 0.23468922 0.23355209 0.23242088 0.23129558 0.23017614 0.22906255]\n",
      "98\n",
      "[0.25623045 0.2549811  0.25373827 0.24942091 0.25127202 0.25004853\n",
      " 0.24883143 0.24762066 0.2614197  0.26022153 0.25589378 0.25470808\n",
      " 0.25352856 0.25235519 0.25118794 0.25002677 0.24887167 0.23585493\n",
      " 0.23471184 0.23357471 0.2324435  0.2313182  0.23019877 0.22908517]\n",
      "99\n",
      "[0.25625308 0.25500374 0.25376091 0.24944087 0.25129466 0.25007117\n",
      " 0.24885406 0.2476433  0.2614466  0.26024844 0.25591798 0.25473228\n",
      " 0.25355276 0.25237939 0.25121214 0.25005097 0.24889586 0.23587757\n",
      " 0.23473447 0.23359734 0.23246614 0.23134084 0.2302214  0.2291078 ]\n",
      "[0.25625308 0.25500374 0.25376091 0.24944087 0.25129466 0.25007117\n",
      " 0.24885406 0.2476433  0.2614466  0.26024844 0.25591798 0.25473228\n",
      " 0.25355276 0.25237939 0.25121214 0.25005097 0.24889586 0.23587757\n",
      " 0.23473447 0.23359734 0.23246614 0.23134084 0.2302214  0.2291078 ]\n"
     ]
    }
   ],
   "source": [
    "#statistical\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "#orig_train = orig_train.dropna()\n",
    "#orig_test = orig_test.dropna()\n",
    "#endog = orig_train[orig_train.patient_id==544].glucose\n",
    "#exog = orig_train[orig_train.patient_id==544].drop(['glucose', 'time', 'patient_id'], axis=1)\n",
    "#exog_pred = orig_test[orig_test.patient_id==544].drop(['glucose', 'time', 'patient_id'], axis=1)\n",
    "#endog_pred = orig_test[orig_test.patient_id==544].glucose\n",
    "#print(orig_test, endog_pred)\n",
    "#print(np.asarray(endog), np.asarray(exog))\n",
    "print(y[0],X[0])\n",
    "mod = sm.tsa.SARIMAX(endog=np.asarray(y[0]), exog=np.asarray(X[0]), order=(1,0,0))\n",
    "#res = mod.fit(disp=False)\n",
    "mod = mod.fit(disp=False)\n",
    "start_params = mod.params\n",
    "print(mod.summary())\n",
    "pred = mod.forecast(horizon, start_params=start_params, exog=np.asarray(dataset.X_test_exog[0])[-horizon:])\n",
    "print(pred)\n",
    "max_iter = 100\n",
    "it = 0\n",
    "learning_rate = 0.0001\n",
    "gradient=lambda v: 4 * v**3 - 10 * v - 3\n",
    "exog_pred_change = np.asarray(dataset.X_test_exog[0])[-horizon:]\n",
    "print(\"max_min\", max_bound, min_bound)\n",
    "while ((pred>max_bound).any() or (pred<min_bound).any()) and (it<max_iter):\n",
    "    #change (X_e)\n",
    "    print(it)\n",
    "    diff = -learning_rate * gradient(exog_pred_change)\n",
    "    exog_pred_change += diff\n",
    "    it += 1\n",
    "    pred = mod.forecast(horizon, start_params=start_params, exog=exog_pred_change)\n",
    "    print(pred)\n",
    "    \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bef633-abdf-41a0-a202-25e97b09522a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#regression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def mean_squared_error(y_true, y_predicted):\n",
    "    # Calculating the loss or cost\n",
    "    cost = np.sum((y_true-y_predicted)**2) / len(y_true)\n",
    "    return cost\n",
    "    \n",
    "orig_train = orig_train.dropna()\n",
    "orig_test = orig_test.dropna()\n",
    "endog = orig_train[orig_train.patient_id==591].glucose\n",
    "exog = orig_train[orig_train.patient_id==591].drop(['glucose', 'time', 'patient_id'], axis=1)\n",
    "exog_pred = orig_test[orig_test.patient_id==591].drop(['glucose', 'time', 'patient_id'], axis=1)\n",
    "endog_pred = orig_test[orig_test.patient_id==591].glucose\n",
    "print(orig_test, endog_pred)\n",
    "print(np.asarray(endog), np.asarray(exog))\n",
    "forest = sm.OLS(np.asarray(endog), np.asarray(exog)).fit()\n",
    "pred = forest.predict(np.asarray(exog_pred))\n",
    "print(pred)\n",
    "max_iter = 100\n",
    "it = 0\n",
    "learning_rate = 0.0001\n",
    "gradient=lambda v: 4 * v**3 - 10 * v - 3\n",
    "exog_pred_change = np.asarray(exog_pred)\n",
    "print(\"max_min\", max_bound, min_bound)\n",
    "while ((pred>max_bound).any() or (pred<min_bound).any()) and (it<max_iter):\n",
    "    #change (X_e)\n",
    "    print(it)\n",
    "    diff = -learning_rate * gradient(exog_pred_change)\n",
    "    exog_pred_change += diff\n",
    "    it += 1\n",
    "    pred = forest.predict(exog_pred_change)\n",
    "    print(pred)\n",
    "    \n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
