{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e1bcfa9-6360-4e7d-9a95-ddd188011c84",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 (main, Jul  5 2023, 13:45:01) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import os\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6986b7f6-f470-48a2-b51e-a96708c58240",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 13:29:53.518769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734438593.671186    6941 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734438593.713582    6941 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-17 13:29:54.001260: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import copy\n",
    "import torch\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as python_random\n",
    "import tensorflow as tf\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as Xet\n",
    "from argparse import ArgumentParser\n",
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATSx\n",
    "from neuralforecast.losses.pytorch import MQLoss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nbeats_pytorch.model import NBeatsNet\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import statsmodels.api as sm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4fd5bd7-a819-4640-84b7-8408a49c8100",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Load data into desired formats for training/validation/testing, including preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, horizon, back_horizon):\n",
    "        self.horizon = horizon\n",
    "        self.back_horizon = back_horizon\n",
    "        self.scaler = list()\n",
    "        self.historical_values = list()  # first by patient idx, then by col_idx\n",
    "\n",
    "    def preprocessing(\n",
    "        self,\n",
    "        lst_train_arrays,\n",
    "        lst_test_arrays,\n",
    "        # train_mode=True, # flag for train_mode (split into train/val), test_mode (no split)\n",
    "        train_size=0.8,\n",
    "        normalize=False,\n",
    "        sequence_stride=6,\n",
    "        target_col=0,\n",
    "        horizon=12\n",
    "    ):\n",
    "        self.normalize = normalize\n",
    "        self.sequence_stride = sequence_stride\n",
    "        self.target_col = target_col\n",
    "        train_arrays = copy.deepcopy(lst_train_arrays)\n",
    "        test_arrays = copy.deepcopy(lst_test_arrays)\n",
    "        # count valid timesteps for each individual series\n",
    "        # train_array.shape = n_timesteps x n_features\n",
    "        self.valid_steps_train = [train_array.shape[0] for train_array in train_arrays]\n",
    "        train_lst, val_lst, test_lst = list(), list(), list()\n",
    "        for idx in range(len(train_arrays)):\n",
    "            print(idx, \"index\")\n",
    "            bg_sample_train = train_arrays[idx]\n",
    "            #bg_sample_train_exog = np.delete(train_arrays[idx], 0, 1)\n",
    "            bg_sample_test = test_arrays[idx]#[:, target_col]\n",
    "            #bg_sample_test_exog = np.delete(test_arrays[idx], 0, 1)\n",
    "            valid_steps_sample = self.valid_steps_train[idx]\n",
    "            #train_target = bg_sample_train_target[: int(train_size * valid_steps_sample)].copy()\n",
    "            train = bg_sample_train[: int(train_size * valid_steps_sample), :].copy()\n",
    "            #val_target = bg_sample_train_target[int(train_size * valid_steps_sample) :].copy()\n",
    "            val = bg_sample_train[int(train_size * valid_steps_sample) :, :].copy()\n",
    "            #test_target = bg_sample_test_target[:].copy()\n",
    "            test = bg_sample_test[:, :].copy()\n",
    "            if self.normalize:\n",
    "                scaler_cols = list()\n",
    "                # train.shape = n_train_timesteps x n_features\n",
    "                for col_idx in range(train.shape[1]):\n",
    "                    scaler = MinMaxScaler(feature_range=(0, 1), clip=False)\n",
    "                    train[:, col_idx] = remove_extra_dim(\n",
    "                        scaler.fit_transform((add_extra_dim(train[:, col_idx])))\n",
    "                    )\n",
    "                    val[:, col_idx] = remove_extra_dim(\n",
    "                        scaler.transform(add_extra_dim(val[:, col_idx]))\n",
    "                    )\n",
    "                    test[:, col_idx] = remove_extra_dim(\n",
    "                        scaler.transform(add_extra_dim(test[:, col_idx]))\n",
    "                    )\n",
    "                    scaler_cols.append(scaler)  # by col_idx, each feature\n",
    "                self.scaler.append(scaler_cols)  # by pat_idx, each patient\n",
    "                \n",
    "            lst_hist_values = list()\n",
    "            for col_idx in range(train.shape[1]):\n",
    "                all_train_col = np.concatenate((train[:, col_idx], val[:, col_idx]))\n",
    "                # decimals = 1, 2 OR 3?\n",
    "                unique_values = np.unique(np.round(all_train_col, decimals=2))\n",
    "                lst_hist_values.append(unique_values)\n",
    "            self.historical_values.append(lst_hist_values)\n",
    "\n",
    "            train_lst.append(train)\n",
    "            #train_lst_exog.append(train_exog)\n",
    "            val_lst.append(val)\n",
    "            #val_lst_exog.append(val_exog)\n",
    "            test_lst.append(test)\n",
    "            #test_lst_exog.append(test_exog)\n",
    "        \n",
    "\n",
    "        (\n",
    "            self.X_train_exog,\n",
    "            self.X_train_target,\n",
    "            self.Y_train,\n",
    "            self.train_idxs,\n",
    "        ) = self.create_sequences(\n",
    "            train_lst,\n",
    "            self.horizon,\n",
    "            self.back_horizon,\n",
    "            self.sequence_stride,\n",
    "            self.target_col,\n",
    "        )\n",
    "        (\n",
    "            self.X_val_exog,\n",
    "            self.X_val_target,\n",
    "            self.Y_val,\n",
    "            self.val_idxs,\n",
    "        ) = self.create_sequences(\n",
    "            val_lst,\n",
    "            self.horizon,\n",
    "            self.back_horizon,\n",
    "            self.sequence_stride,\n",
    "            self.target_col,\n",
    "        )\n",
    "        (\n",
    "            self.X_test_exog,\n",
    "            self.X_test_target,\n",
    "            self.Y_test,\n",
    "            self.test_idxs,\n",
    "        ) = self.create_sequences(\n",
    "            test_lst,\n",
    "            self.horizon,\n",
    "            self.back_horizon,\n",
    "            self.sequence_stride,\n",
    "            self.target_col,\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_sequences(\n",
    "        series_lst, horizon, back_horizon, sequence_stride, target_col=0, exog=False\n",
    "    ):\n",
    "        Xs_exog, Xs_target, Ys, sample_idxs = list(), list(), list(), list()\n",
    "        \n",
    "        cnt_nans = 0\n",
    "        for idx, series in enumerate(series_lst):\n",
    "            len_series = series.shape[0]\n",
    "            if len_series < (horizon + back_horizon):\n",
    "                print(\n",
    "                    f\"Warning: not enough timesteps to split for sample {idx}, len: {len_series}, horizon: {horizon}, back: {back_horizon}.\"\n",
    "                )\n",
    "            for i in range(0, len_series - back_horizon - horizon, sequence_stride):\n",
    "                input_series_exog = series[i : (i + back_horizon)]\n",
    "                input_series_exog = np.delete(input_series_exog, [target_col], axis=1)\n",
    "                input_series_target = series[i : (i + back_horizon), [target_col]]\n",
    "                output_series = series[\n",
    "                    (i + back_horizon) : (i + back_horizon + horizon), [target_col]\n",
    "                ]\n",
    "                # TODO: add future plans as additional variables (?)\n",
    "                if np.isfinite(input_series_exog).all() and np.isfinite(input_series_target).all() and np.isfinite(output_series).all():\n",
    "                    Xs_exog.append(input_series_exog)\n",
    "                    Xs_target.append(input_series_target)\n",
    "                    Ys.append(output_series)\n",
    "                    # record the sample index when splitting\n",
    "                    sample_idxs.append(idx)\n",
    "                else:\n",
    "                    cnt_nans += 1\n",
    "                    if cnt_nans % 100 == 0:\n",
    "                        print(f\"{cnt_nans} strides skipped due to NaN values.\")\n",
    "        #print(\"train\", np.array(Xs), \"test\", np.array(Ys), \"val\", np.array(sample_idxs))\n",
    "        return np.array(Xs_exog), np.array(Xs_target), np.array(Ys), np.array(sample_idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a6fdba9-a79d-4357-8557-3223aee456bb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove an extra dimension\n",
    "def remove_extra_dim(input_array):\n",
    "    # 2d to 1d\n",
    "    if len(input_array.shape) == 2:\n",
    "        return np.reshape(input_array, (-1))\n",
    "    # 3d to 2d (remove the last empty dim)\n",
    "    elif len(input_array.shape) == 3:\n",
    "        return np.squeeze(np.asarray(input_array), axis=-1)\n",
    "    else:\n",
    "        print(\"Not implemented.\")\n",
    "        #print(input_array, \"JLNA;iknb\")\n",
    "\n",
    "# add an extra dimension\n",
    "def add_extra_dim(input_array):\n",
    "    # 1d to 2d\n",
    "    if len(input_array.shape) == 1:\n",
    "        return np.reshape(input_array, (-1, 1))\n",
    "    # 2d to 3d\n",
    "    elif len(input_array.shape) == 2:\n",
    "        return np.asarray(input_array)[:, :, np.newaxis]\n",
    "    else:\n",
    "        print(\"Not implemented.\")\n",
    "        #print(input_array, \"ALVNAPNV\")\n",
    "\n",
    "# Method: Fix the random seeds to get consistent models\n",
    "def reset_seeds(seed_value=39):\n",
    "    # ref: https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\n",
    "    # necessary for starting Numpy generated random numbers in a well-defined initial state.\n",
    "    np.random.seed(seed_value)\n",
    "    # necessary for starting core Python generated random numbers in a well-defined state.\n",
    "    python_random.seed(seed_value)\n",
    "    # set_seed() will make random number generation\n",
    "    tf.random.set_seed(seed_value)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83c0945e-74f5-410f-9b29-acfe552ffba4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(dataset, data_path):\n",
    "    df = []\n",
    "    df = pd.DataFrame(df)\n",
    "    if dataset == \"simulated\":\n",
    "        for i,j in zip([\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\"],[1,2,3,4,5,6,7,8,9,10]):\n",
    "            a = pd.read_csv(f\"../results/simulation_4/adult#0{i}.csv\")\n",
    "            a[\"Time\"] = a[[\"Time\"]].apply(\n",
    "                lambda x: pd.to_datetime(x, errors=\"coerce\", format=\"%Y-%m-%d %H:%M:%S\")\n",
    "            )\n",
    "            #a['Time'] = pd.to_datetime(a['Time'])\n",
    "            #a.rename(columns={\"Time\":\"ds\", \"BG\":\"y\"}, inplace=True)\n",
    "            a = a.dropna()\n",
    "            #date_index = pd.date_range(a.Time[0], periods=len(a),freq='3min')\n",
    "            #a.index = date_index\n",
    "            a['patient_id'] = pd.Series([f\"{j}\" for x in range(len(a.index))])\n",
    "            df = pd.concat([df,a], ignore_index=True)\n",
    "        \n",
    "        #df.drop(['Time','BG','LBGI','HBGI','Risk'], axis=1, inplace=True)\n",
    "        print(\"aldingvapnb[\", df)\n",
    "        idx = int( df.shape[0] * 0.8)#TEST_SIZE)\n",
    "        cut = int((df.shape[0]-idx)/10)\n",
    "        Y_train_df = df[df.CGM<df['CGM'].values[-cut]] # 132 train\n",
    "        Y_test_df = df[df.CGM>=df['CGM'].values[-cut]].reset_index(drop=True) # 12 test  \n",
    "        Y_train_df.to_csv(\"data/data_simulation/all_train.csv\")\n",
    "        Y_test_df.to_csv(\"data/data_simulation/all_test.csv\")\n",
    "        df.drop(['Time','BG','LBGI','HBGI','Risk'], axis=1, inplace=True)\n",
    "        #return df\n",
    "        \n",
    "    elif dataset == \"ohiot1dm\":\n",
    "        train = []\n",
    "        test = []\n",
    "        train = pd.DataFrame(train)\n",
    "        test = pd.DataFrame(test)\n",
    "        for i in [540, 544, 552, 567, 584, 596, 559, 563, 570, 575, 588, 591]:\n",
    "            file_train = pd.read_csv(data_path + \"data_OhioT1DM/\" + f\"{i}_train.csv\")\n",
    "            file_test = pd.read_csv(data_path + \"data_OhioT1DM/\" + f\"{i}_test.csv\")\n",
    "            \n",
    "            file_train['patient_id'] = pd.Series([f\"{i}\" for x in range(len(file_train.index))])\n",
    "            file_test['patient_id'] = pd.Series([f\"{i}\" for x in range(len(file_test.index))])\n",
    "            \n",
    "            train = pd.concat([train, file_train], ignore_index=True)\n",
    "            test = pd.concat([train, file_test], ignore_index=True)\n",
    "            \n",
    "        train.to_csv(data_path + \"data_OhioT1DM/all_train.csv\")\n",
    "        test.to_csv(data_path + \"data_OhioT1DM/all_test.csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def1e674-4b21-49dc-b23f-00d8621b8753",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_data(dataset, data_path):\n",
    "    prepare_data(dataset, data_path)\n",
    "    if dataset == \"ohiot1dm\":\n",
    "        train, orig_train = load_ohio_data(data_path, \"all_train.csv\")\n",
    "        test, orig_test = load_ohio_data(data_path, \"all_test.csv\")\n",
    "    elif dataset == \"simulated\":\n",
    "        #idx = int( df.shape[0] * 1-TEST_SIZE )\n",
    "        #cut = int((df.shape[0]-idx)/10)\n",
    "        #train = df[df.CGM<df['CGM'].values[-cut]] # 132 train\n",
    "        #test = df[df.CGM>=df['CGM'].values[-cut]].reset_index(drop=True) # 12 test  \n",
    "        train, orig_train = load_sim_data(data_path, \"all_train.csv\")\n",
    "        test, orig_test = load_sim_data(data_path, \"all_test.csv\")\n",
    "    else:\n",
    "        print(\"No dataset chosen\")\n",
    "    return train, test, orig_train, orig_test\n",
    "\n",
    "def load_ohio_data(data_path, file_name=\"all_train.csv\"):\n",
    "    # load all the patients, combined\n",
    "    data = pd.read_csv(data_path + \"data_OhioT1DM/\" + file_name)\n",
    "\n",
    "    from functools import reduce\n",
    "    from operator import or_ as union\n",
    "\n",
    "    def idx_union(mylist):\n",
    "        idx = reduce(union, (index for index in mylist))\n",
    "        return idx\n",
    "\n",
    "    idx_missing = data.loc[data[\"missing\"] != -1].index\n",
    "    idx_missing_union = idx_union([idx_missing - 1, idx_missing])\n",
    "\n",
    "    data = data.drop(idx_missing_union)\n",
    "    data_bg = data[\n",
    "        [\n",
    "            \"index_new\",\n",
    "            \"patient_id\",\n",
    "            \"glucose\",\n",
    "            \"basal\",\n",
    "            \"bolus\",\n",
    "            \"carbs\",\n",
    "            \"exercise_intensity\",\n",
    "        ]\n",
    "    ]\n",
    "    data_bg[\"time\"] = data_bg[[\"index_new\"]].apply(\n",
    "        lambda x: pd.to_datetime(x, errors=\"coerce\", format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    )\n",
    "    data_bg = data_bg.drop(\"index_new\", axis=1)\n",
    "\n",
    "    data_bg[\"bolus\"][data_bg[\"bolus\"] == -1] = 0\n",
    "    data_bg[\"carbs\"][data_bg[\"carbs\"] == -1] = 0\n",
    "    data_bg[\"exercise_intensity\"][data_bg[\"exercise_intensity\"] == -1] = 0\n",
    "    data_bg[\"glucose\"][data_bg[\"glucose\"] == -1] = np.NaN\n",
    "\n",
    "    lst_patient_id = [\n",
    "        540,\n",
    "        544,\n",
    "        552,\n",
    "        567,\n",
    "        584,\n",
    "        596,\n",
    "        559,\n",
    "        563,\n",
    "        570,\n",
    "        575,\n",
    "        588,\n",
    "        591,\n",
    "    ]\n",
    "    lst_arrays = list()\n",
    "    for pat_id in lst_patient_id:\n",
    "        lst_arrays.append(\n",
    "            np.asarray(\n",
    "                data_bg[data_bg[\"patient_id\"] == pat_id][\n",
    "                    [\n",
    "                        \"glucose\",\n",
    "                        \"basal\",\n",
    "                        \"bolus\",\n",
    "                        \"carbs\",\n",
    "                        \"exercise_intensity\",\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    return lst_arrays, data_bg\n",
    "\n",
    "\n",
    "def load_sim_data(data_path, file_name=\"all_train.csv\"):\n",
    "    data = pd.read_csv(data_path + \"data_simulation/\" + file_name)\n",
    "    data_bg = data[[\"patient_id\", \"Time\", \"CGM\", \"CHO\", \"insulin\"]]\n",
    "    #print(data_bg)\n",
    "    data_bg[\"time\"] = data_bg[[\"Time\"]].apply(\n",
    "        lambda x: pd.to_datetime(x, errors=\"coerce\", format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    )\n",
    "    data_bg = data_bg.drop(\"Time\", axis=1)\n",
    "    lst_patient_id = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    lst_arrays = list()\n",
    "    for pat_id in lst_patient_id:\n",
    "        lst_arrays.append(\n",
    "            np.asarray(\n",
    "                data_bg[data_bg[\"patient_id\"] == pat_id][[\"CGM\", \"CHO\", \"insulin\"]]\n",
    "            )\n",
    "        )\n",
    "    return lst_arrays, data_bg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1b1a137-f7a4-4042-b85c-0235d9ec6570",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def forecast_metrics(dataset, Y_pred, inverse_transform=True):\n",
    "    Y_test_original, Y_pred_original = list(), list()\n",
    "    #Y_pred = np.squeeze(Y_pred, axis=-1)\n",
    "    if inverse_transform:\n",
    "        for i in range(dataset.X_test_exog.shape[0]):\n",
    "            #print(\"Y_test\", dataset.Y_test[i], \"Y_pred\", Y_pred[i])\n",
    "            idx = dataset.test_idxs[i]\n",
    "            scaler = dataset.scaler[idx]\n",
    "\n",
    "            Y_test_original.append(\n",
    "                scaler[dataset.target_col].inverse_transform(dataset.X_test_target[i])\n",
    "            )\n",
    "            Y_pred_original.append(\n",
    "                scaler[dataset.target_col].inverse_transform(Y_pred[i])\n",
    "            )\n",
    "\n",
    "        Y_test_original = np.array(Y_test_original)\n",
    "        Y_pred_original = np.array(Y_pred_original)\n",
    "    else:\n",
    "        Y_test_original = dataset.X_test_target\n",
    "        Y_pred_original = Y_pred\n",
    "\n",
    "    def smape(Y_test, Y_pred):\n",
    "        # src: https://github.com/ServiceNow/N-BEATS/blob/c746a4f13ffc957487e0c3279b182c3030836053/common/metrics.py\n",
    "        def smape_sample(actual, forecast):\n",
    "            return 200 * np.mean(\n",
    "                np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast))\n",
    "            )\n",
    "\n",
    "        return np.mean([smape_sample(Y_test[i], Y_pred[i]) for i in range(len(Y_pred))])\n",
    "\n",
    "    def rmse(Y_test, Y_pred):\n",
    "        return np.sqrt(np.mean((Y_pred - Y_test) ** 2))\n",
    "\n",
    "    #print(\"test\", Y_test_original, \"pred\", Y_pred_original)\n",
    "    mean_smape = smape(Y_test_original, Y_pred_original)\n",
    "    mean_rmse = rmse(Y_test_original, Y_pred_original)\n",
    "\n",
    "    return mean_smape, mean_rmse\n",
    "\n",
    "def forecast_metrics_single(Y_orig, Y_pred, inverse_transform=True):\n",
    "    Y_test_original, Y_pred_original = list(), list()\n",
    "    #Y_pred = np.squeeze(Y_pred, axis=-1)\n",
    "    if inverse_transform:\n",
    "        #for i in range(dataset.X_test_exog.shape[0]):\n",
    "            #print(\"Y_test\", dataset.Y_test[i], \"Y_pred\", Y_pred[i])\n",
    "        #    idx = dataset.test_idxs[i]\n",
    "        scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "        \n",
    "        Y_test_original = scaler.inverse_transform(Y_orig)  # Ensure correct shape\n",
    "        Y_pred_original = scaler.inverse_transform(Y_pred.numpy().reshape(-1, 1))  # Ensure correct shape\n",
    "\n",
    "\n",
    "        Y_test_original = np.array(Y_test_original)\n",
    "        Y_pred_original = np.array(Y_pred_original)\n",
    "    else:\n",
    "        Y_test_original = dataset.X_test_target\n",
    "        Y_pred_original = Y_pred\n",
    "\n",
    "    def smape(Y_test, Y_pred):\n",
    "        # src: https://github.com/ServiceNow/N-BEATS/blob/c746a4f13ffc957487e0c3279b182c3030836053/common/metrics.py\n",
    "        def smape_sample(actual, forecast):\n",
    "            return 200 * np.mean(\n",
    "                np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast))\n",
    "            )\n",
    "\n",
    "        return np.mean([smape_sample(Y_test[i], Y_pred[i]) for i in range(len(Y_pred))])\n",
    "\n",
    "    def rmse(Y_test, Y_pred):\n",
    "        return np.sqrt(np.mean((Y_pred - Y_test) ** 2))\n",
    "    #print(\"test\", Y_test_original, \"pred\", Y_pred_original)\n",
    "    mean_smape = smape(Y_test_original, Y_pred_original)\n",
    "    mean_rmse = rmse(Y_test_original, Y_pred_original)\n",
    "\n",
    "    return mean_smape, mean_rmse\n",
    "\n",
    "\n",
    "def polynomial_values(shift, change_percent, poly_order, horizon, desired_steps=None):\n",
    "    \"\"\"\n",
    "    shift: e.g., +0.1 (110% of the start value)\n",
    "    change_percent: e.g., 0.1 (10% increase)\n",
    "    poly_order: e.g., order 1, or 2, ...\n",
    "    horizon: the forecasting horizon\n",
    "    desired_steps: the desired timesteps for the change_percent to finally happen (can be larger than horizon)\n",
    "    \"\"\"\n",
    "    if horizon == 1:\n",
    "        return np.asarray([shift + change_percent])\n",
    "    desired_steps = desired_steps if desired_steps else horizon\n",
    "\n",
    "    p_orders = [shift]  # intercept\n",
    "    p_orders.extend([0 for i in range(poly_order)])\n",
    "    p_orders[-1] = change_percent / ((desired_steps - 1) ** poly_order)\n",
    "\n",
    "    p = np.polynomial.Polynomial(p_orders)\n",
    "    p_coefs = list(reversed(p.coef))\n",
    "    value_lst = np.asarray([np.polyval(p_coefs, i) for i in range(desired_steps)])\n",
    "\n",
    "    return value_lst[:horizon]\n",
    "\n",
    "\n",
    "def generate_bounds(\n",
    "    center,\n",
    "    shift,\n",
    "    desired_center,\n",
    "    poly_order,\n",
    "    horizon,\n",
    "    fraction_std,\n",
    "    input_series,\n",
    "    desired_steps,\n",
    "):\n",
    "    if input_series[-1] == 0:\n",
    "        center = \"mean\"\n",
    "    if center == \"last\":\n",
    "        start_value = input_series[-1]\n",
    "    elif center == \"median\":\n",
    "        start_value = np.median(input_series)\n",
    "    elif center == \"mean\":\n",
    "        start_value = np.mean(input_series)\n",
    "    elif center == \"min\":\n",
    "        start_value = np.min(input_series)\n",
    "    elif center == \"max\":\n",
    "        start_value = np.max(input_series)\n",
    "    else:\n",
    "        print(\"Center: not implemented.\")\n",
    "\n",
    "    std = np.std(input_series)\n",
    "    # Calculate the change_percent based on the desired center (in 2 hours)\n",
    "    change_percent = (desired_center - start_value) / start_value\n",
    "    # Create a default fluctuating range for the upper and lower bound if std is too small\n",
    "    fluct_range = fraction_std * std if fraction_std * std >= 0.025 else 0.025\n",
    "    upper = add_extra_dim(\n",
    "        start_value\n",
    "        * (\n",
    "            1\n",
    "            + polynomial_values(\n",
    "                shift, change_percent, poly_order, horizon, desired_steps\n",
    "            )\n",
    "            + fluct_range\n",
    "        )\n",
    "    )\n",
    "    lower = add_extra_dim(\n",
    "        start_value\n",
    "        * (\n",
    "            1\n",
    "            + polynomial_values(\n",
    "                shift, change_percent, poly_order, horizon, desired_steps\n",
    "            )\n",
    "            - fluct_range\n",
    "        )\n",
    "    )\n",
    "    return upper, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a96c350c-50ef-452b-abee-19cde6412da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6941/3259339262.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"time\"] = data_bg[[\"index_new\"]].apply(\n",
      "/tmp/ipykernel_6941/3259339262.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"bolus\"][data_bg[\"bolus\"] == -1] = 0\n",
      "/tmp/ipykernel_6941/3259339262.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"carbs\"][data_bg[\"carbs\"] == -1] = 0\n",
      "/tmp/ipykernel_6941/3259339262.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"exercise_intensity\"][data_bg[\"exercise_intensity\"] == -1] = 0\n",
      "/tmp/ipykernel_6941/3259339262.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"glucose\"][data_bg[\"glucose\"] == -1] = np.NaN\n",
      "/tmp/ipykernel_6941/3259339262.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"time\"] = data_bg[[\"index_new\"]].apply(\n",
      "/tmp/ipykernel_6941/3259339262.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"bolus\"][data_bg[\"bolus\"] == -1] = 0\n",
      "/tmp/ipykernel_6941/3259339262.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"carbs\"][data_bg[\"carbs\"] == -1] = 0\n",
      "/tmp/ipykernel_6941/3259339262.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"exercise_intensity\"][data_bg[\"exercise_intensity\"] == -1] = 0\n",
      "/tmp/ipykernel_6941/3259339262.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_bg[\"glucose\"][data_bg[\"glucose\"] == -1] = np.NaN\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of loaded train: 12*(12236, 5)\n",
      "The shape of test: 12*(12236, 5)\n",
      "===========Desired trend parameters=============\n",
      "center: last, desired_shift: 0;\n",
      "fraction_std:1;\n",
      "desired_change:'sample_based', poly_order:1.\n",
      "0 index\n",
      "1 index\n",
      "2 index\n",
      "3 index\n",
      "4 index\n",
      "5 index\n",
      "6 index\n",
      "7 index\n",
      "8 index\n",
      "9 index\n",
      "10 index\n",
      "11 index\n",
      "100 strides skipped due to NaN values.\n",
      "200 strides skipped due to NaN values.\n",
      "300 strides skipped due to NaN values.\n",
      "400 strides skipped due to NaN values.\n",
      "500 strides skipped due to NaN values.\n",
      "600 strides skipped due to NaN values.\n",
      "700 strides skipped due to NaN values.\n",
      "800 strides skipped due to NaN values.\n",
      "900 strides skipped due to NaN values.\n",
      "1000 strides skipped due to NaN values.\n",
      "1100 strides skipped due to NaN values.\n",
      "1200 strides skipped due to NaN values.\n",
      "1300 strides skipped due to NaN values.\n",
      "1400 strides skipped due to NaN values.\n",
      "1500 strides skipped due to NaN values.\n",
      "100 strides skipped due to NaN values.\n",
      "200 strides skipped due to NaN values.\n",
      "300 strides skipped due to NaN values.\n",
      "100 strides skipped due to NaN values.\n",
      "200 strides skipped due to NaN values.\n",
      "300 strides skipped due to NaN values.\n",
      "400 strides skipped due to NaN values.\n",
      "500 strides skipped due to NaN values.\n",
      "600 strides skipped due to NaN values.\n",
      "700 strides skipped due to NaN values.\n",
      "800 strides skipped due to NaN values.\n",
      "900 strides skipped due to NaN values.\n",
      "1000 strides skipped due to NaN values.\n",
      "1100 strides skipped due to NaN values.\n",
      "1200 strides skipped due to NaN values.\n",
      "1300 strides skipped due to NaN values.\n",
      "1400 strides skipped due to NaN values.\n",
      "1500 strides skipped due to NaN values.\n",
      "1600 strides skipped due to NaN values.\n",
      "1700 strides skipped due to NaN values.\n",
      "1800 strides skipped due to NaN values.\n",
      "[[[0.45238095 0.         0.         0.        ]\n",
      "  [0.45238095 0.         0.         0.        ]\n",
      "  [0.45238095 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.9047619  0.         0.         0.        ]\n",
      "  [0.9047619  0.49122807 0.         0.        ]\n",
      "  [0.9047619  0.49122807 0.         0.        ]]\n",
      "\n",
      " [[0.9047619  0.49122807 0.         0.        ]\n",
      "  [0.9047619  0.49122807 0.         0.        ]\n",
      "  [0.9047619  0.49122807 0.         0.        ]\n",
      "  ...\n",
      "  [0.45238095 0.         0.         0.        ]\n",
      "  [0.45238095 0.         0.         0.        ]\n",
      "  [0.45238095 0.         0.         0.        ]]\n",
      "\n",
      " [[0.45238095 0.         0.         0.        ]\n",
      "  [0.45238095 0.         0.         0.        ]\n",
      "  [0.19047619 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.28571429 0.         0.         0.        ]\n",
      "  [0.28571429 0.         0.         0.        ]\n",
      "  [0.19047619 0.         0.         0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.8        0.         0.         0.        ]\n",
      "  [0.8        0.         0.         0.        ]\n",
      "  [0.8        0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.8        0.         0.         0.        ]\n",
      "  [0.8        0.         0.         0.        ]\n",
      "  [0.8        0.         0.         0.        ]]\n",
      "\n",
      " [[0.8        0.         0.         0.        ]\n",
      "  [0.8        0.         0.         0.        ]\n",
      "  [0.8        0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.8        0.         0.         0.        ]\n",
      "  [0.8        0.         0.         0.        ]\n",
      "  [0.8        0.         0.         0.        ]]\n",
      "\n",
      " [[0.8        0.         0.         0.        ]\n",
      "  [0.8        0.         0.         0.        ]\n",
      "  [0.8        0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.96       0.         0.         0.        ]\n",
      "  [0.96       0.         0.         0.        ]\n",
      "  [0.96       0.         0.         0.        ]]] [[[0.07598784]\n",
      "  [0.06990881]\n",
      "  [0.07902736]\n",
      "  ...\n",
      "  [0.27659574]\n",
      "  [0.29483283]\n",
      "  [0.30395137]]\n",
      "\n",
      " [[0.30395137]\n",
      "  [0.31306991]\n",
      "  [0.3100304 ]\n",
      "  ...\n",
      "  [0.27051672]\n",
      "  [0.26443769]\n",
      "  [0.25531915]]\n",
      "\n",
      " [[0.25227964]\n",
      "  [0.25531915]\n",
      "  [0.26139818]\n",
      "  ...\n",
      "  [0.25531915]\n",
      "  [0.24924012]\n",
      "  [0.25227964]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.3125    ]\n",
      "  [0.30357143]\n",
      "  [0.29464286]\n",
      "  ...\n",
      "  [0.23809524]\n",
      "  [0.23214286]\n",
      "  [0.22916667]]\n",
      "\n",
      " [[0.22619048]\n",
      "  [0.2202381 ]\n",
      "  [0.2172619 ]\n",
      "  ...\n",
      "  [0.2172619 ]\n",
      "  [0.22916667]\n",
      "  [0.22619048]]\n",
      "\n",
      " [[0.23214286]\n",
      "  [0.22321429]\n",
      "  [0.20535714]\n",
      "  ...\n",
      "  [0.19642857]\n",
      "  [0.19047619]\n",
      "  [0.1875    ]]] (8313, 12, 4) (8313, 12, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-17 15:54:59.498498: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model trained, with test sMAPE score 28.9353; test RMSE score: 57.9041.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 128292 into shape (24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 109\u001b[0m\n\u001b[1;32m    102\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Assuming you have your predictions and actual values\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# pred_tf is the output from your model\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# dataset.Y_test is the actual target values\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Reshape predictions if necessary\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m pred_tf_reshaped \u001b[38;5;241m=\u001b[39m \u001b[43mpred_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust based on your output shape\u001b[39;00m\n\u001b[1;32m    110\u001b[0m y_test_reshaped \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mY_test\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m24\u001b[39m)  \u001b[38;5;66;03m# Adjust based on your target shape\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Call the plotting function\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 128292 into shape (24)"
     ]
    }
   ],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument( \"--dataset\", type=str, help=\"Choose dataset.\")\n",
    "parser.add_argument( \"--horizon\", type=int, help=\"Horizon of forecasting task.\")\n",
    "parser.add_argument( \"--back-horizon\", type=int, help=\"Back horizon of forecasting task.\")\n",
    "parser.add_argument( \"--random-seed\", type=int, default=39, help=\"Random seed parameter, default 39.\")\n",
    "parser.add_argument( \"--train-size\", type=float, default=0.8, help=\"Proportional size of the training set.\")\n",
    "parser.add_argument( \"--test-group\", type=str, default=None, help=\"Extract random 100 samples from test group, i.e., 'hyper'/'hypo'; default None.\")\n",
    "# Parse the arguments from a string\n",
    "args = parser.parse_args(\"--dataset ohiot1dm --horizon 12 --back-horizon 12 --random-seed 35 --train-size 0.8 --test-group hypo\".split())\n",
    "#args = parser.parse_args()\n",
    "data_path = \"./data/\"\n",
    "lst_arrays, lst_arrays_test, orig_train, orig_test = load_data(args.dataset, data_path) #misschien toch load_data gebruiken?\n",
    "print(f\"The shape of loaded train: {len(lst_arrays)}*{lst_arrays[0].shape}\")\n",
    "print(f\"The shape of test: {len(lst_arrays_test)}*{lst_arrays_test[0].shape}\")\n",
    "\n",
    "print(f\"===========Desired trend parameters=============\")\n",
    "center = \"last\"\n",
    "desired_shift, poly_order = 0, 1\n",
    "fraction_std = 1#args.fraction_std\n",
    "print(f\"center: {center}, desired_shift: {desired_shift};\")\n",
    "print(f\"fraction_std:{fraction_std};\")\n",
    "print(f\"desired_change:'sample_based', poly_order:{poly_order}.\")\n",
    "\n",
    "TARGET_COL = 0\n",
    "if args.dataset == \"ohiot1dm\":\n",
    "    CHANGE_COLS = [1, 2, 3, 4]\n",
    "elif args.dataset == \"simulated\":\n",
    "    CHANGE_COLS = [1, 2]\n",
    "else:\n",
    "    CHANGE_COLS = None\n",
    "\n",
    "RANDOM_STATE = args.random_seed\n",
    "TRAIN_SIZE = args.train_size\n",
    "horizon, back_horizon = args.horizon, args.back_horizon\n",
    "dataset = DataLoader(horizon, back_horizon)\n",
    "dataset.preprocessing(#???\n",
    "    lst_train_arrays=lst_arrays,\n",
    "    lst_test_arrays=lst_arrays_test,\n",
    "    train_size=TRAIN_SIZE,\n",
    "    normalize=True,\n",
    "    sequence_stride= horizon,\n",
    "    target_col=TARGET_COL,\n",
    "    horizon = horizon\n",
    ")\n",
    "\n",
    "#print(dataset.X_train.shape, dataset.Y_train.shape)\n",
    "#print(dataset.X_val.shape, dataset.Y_val.shape)\n",
    "#print(dataset.X_test.shape, dataset.Y_test.shape)\n",
    "\n",
    "\n",
    "print(dataset.X_train_exog, dataset.X_train_target, dataset.X_train_exog.shape, dataset.X_train_target.shape)\n",
    "X = dataset.X_train_exog\n",
    "y = dataset.X_train_target\n",
    "tf.random.set_seed(args.random_seed)\n",
    "\n",
    "n_in_features = dataset.X_train_exog.shape[2]\n",
    "n_out_features = 1\n",
    "tf_model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input(shape=(back_horizon, n_in_features)),\n",
    "        # Shape [batch, time, features] => [batch, time, gru_units]\n",
    "        tf.keras.layers.GRU(100, activation=\"tanh\", return_sequences=True),\n",
    "        tf.keras.layers.GRU(100, activation=\"tanh\", return_sequences=False),\n",
    "        # Shape => [batch, time, features]\n",
    "        tf.keras.layers.Dense(horizon, activation=\"linear\"),\n",
    "        tf.keras.layers.Reshape((horizon, n_out_features)),\n",
    "    ]\n",
    ")\n",
    "#orig_test_metric = np.asarray(orig_test.drop(['time'], axis=1))#[orig_test.patient_id==544], 'patient_id'\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "tf_model.compile(optimizer=optimizer, loss=\"mae\")\n",
    "#tf_model.compile(optimizer='adam', loss='mse')\n",
    "tf_model.fit(X, y, epochs=100, verbose=0)\n",
    "pred_tf = tf_model(dataset.X_test_exog)#[-horizon:])\n",
    "mean_smape, mean_rmse = forecast_metrics(dataset, pred_tf)\n",
    "print(\n",
    "    f\"model trained, with test sMAPE score {mean_smape:0.4f}; test RMSE score: {mean_rmse:0.4f}.\"\n",
    ")\n",
    "def plot_predictions(y_true, y_pred, title='Model Predictions vs Actual', num_samples=5):\n",
    "\n",
    "    # Ensure y_true and y_pred are numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Select a random sample of indices to plot\n",
    "    indices = np.random.choice(y_true.shape[0], num_samples, replace=False)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(num_samples, 1, i + 1)\n",
    "        plt.plot(y_true[idx], label='Actual', color='blue')\n",
    "        plt.plot(y_pred[idx], label='Predicted', color='orange')\n",
    "        plt.title(f'Sample {idx + 1}')\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.subplots_adjust(top=0.9)  # Adjust the top to make room for the title\n",
    "    plt.show()\n",
    "\n",
    "# Assuming you have your predictions and actual values\n",
    "# pred_tf is the output from your model\n",
    "# dataset.Y_test is the actual target values\n",
    "\n",
    "# Reshape predictions if necessary\n",
    "pred_tf_reshaped = pred_tf.numpy().reshape(-1, 24)  # Adjust based on your output shape\n",
    "y_test_reshaped = dataset.Y_test.reshape(-1, 24)  # Adjust based on your target shape\n",
    "\n",
    "# Call the plotting function\n",
    "plot_predictions(y_test_reshaped, pred_tf_reshaped, title='Model Predictions vs Actual')\n",
    "\n",
    "\n",
    "\n",
    "#Welke vorm moet pred hebben???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789e29d-2942-45ab-97ed-f5333c3d6a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_bound, hypo_bound = 180, 70\n",
    "print(f\"===========CF generation setup=============\")\n",
    "print(f\"hyper bound value: {hyper_bound}, hypo bound: {hypo_bound}.\")\n",
    "\n",
    "event_labels = list()\n",
    "for i in range(len(pred_tf)):\n",
    "    scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "    Y_preds_original = scaler.inverse_transform(pred_tf[i].numpy())\n",
    "    if np.any(Y_preds_original >= hyper_bound):\n",
    "        event_labels.append(\"hyper\")\n",
    "    elif np.any(Y_preds_original <= hypo_bound):\n",
    "        event_labels.append(\"hypo\")\n",
    "    else:\n",
    "        event_labels.append(\"normal\")\n",
    "hyper_indices = np.argwhere(np.array(event_labels) == \"hyper\").reshape(-1)\n",
    "hypo_indices = np.argwhere(np.array(event_labels) == \"hypo\").reshape(-1)\n",
    "\n",
    "print(f\"hyper_indices shape: {hyper_indices.shape}\")\n",
    "print(f\"hypo_indices shape: {hypo_indices.shape}\")\n",
    "\n",
    "print(\"LSASLSLKDGNS\", Y_preds_original)\n",
    "#plot(orig_train, orig_test, Y_preds_original)\n",
    "\n",
    "# use a subset of the test\n",
    "rand_test_size = 100\n",
    "print(args.test_group)\n",
    "if args.test_group == \"hyper\":\n",
    "    if len(hyper_indices) >= rand_test_size:\n",
    "        print(\"if\", hyper_indices)\n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        rand_test_idx = np.random.choice(\n",
    "            hyper_indices, rand_test_size, replace=False\n",
    "        )\n",
    "    else:\n",
    "        print(\"else\", hyper_indices)\n",
    "        rand_test_idx = hyper_indices\n",
    "elif args.test_group == \"hypo\":\n",
    "    if len(hypo_indices) >= rand_test_size:\n",
    "        np.random.seed(RANDOM_STATE)\n",
    "        rand_test_idx = np.random.choice(\n",
    "            hypo_indices, rand_test_size, replace=False\n",
    "        )\n",
    "    else:\n",
    "        rand_test_idx = hypo_indices\n",
    "else:\n",
    "    rand_test_idx = np.arange(dataset.X_test_exog.shape[0])\n",
    "\n",
    "X_test = dataset.X_test_exog[rand_test_idx]\n",
    "Y_test = dataset.X_test_target[rand_test_idx]\n",
    "\n",
    "print(\n",
    "    f\"Generating CFs for {len(rand_test_idx)} samples in total, for {args.test_group} test group...\"\n",
    ")\n",
    "\n",
    "# loss calculation ==> min/max bounds\n",
    "desired_max_lst, desired_min_lst = list(), list()\n",
    "hist_inputs = list()\n",
    "\n",
    "# define the desired center to reach in two hours (24 timesteps for OhioT1DM)\n",
    "# then we need to cut the first 6 steps to generate the desired bounds\n",
    "desired_steps = 24 if args.dataset == \"ohiot1dm\" else 20\n",
    "if args.test_group == \"hyper\":\n",
    "    desired_center_2h = hyper_bound - 10  # -10 for a fluctuating bound\n",
    "elif args.test_group == \"hypo\":\n",
    "    desired_center_2h = hypo_bound + 10  # +10 for a fluctuating bound\n",
    "else:\n",
    "    print(\n",
    "        f\"Group not identified: {args.test_group}, use a default center\"\n",
    "    )\n",
    "    desired_center_2h = (hyper_bound + hypo_bound) / 2\n",
    "#print(f\"desired center {desired_center_2h} in {desired_steps} timesteps.\")\n",
    "\n",
    "for i in range(len(X_test)): #???Maybe join exog, target\n",
    "    idx = dataset.test_idxs[rand_test_idx[i]]\n",
    "    scaler = dataset.scaler[idx]\n",
    "\n",
    "    desired_center_scaled = scaler[TARGET_COL].transform(\n",
    "        np.array(desired_center_2h).reshape(-1, 1)\n",
    "    )[0][0]\n",
    "    #print(\n",
    "    #    f\"desired_center: {desired_center_2h}; after scaling: {desired_center_scaled:0.4f}\"\n",
    "    #)\n",
    "    # desired trend bounds: use the `center` parameter from the input sequence as the starting point\n",
    "    desired_max_scaled, desired_min_scaled = generate_bounds(\n",
    "        center=center,  # Use the parameters defined at the beginning of the script\n",
    "        shift=desired_shift,\n",
    "        desired_center=desired_center_scaled,\n",
    "        poly_order=poly_order,\n",
    "        horizon=horizon,\n",
    "        fraction_std=fraction_std,\n",
    "        input_series=np.transpose(Y_test[i,0]),\n",
    "        desired_steps=desired_steps,\n",
    "    )\n",
    "    print(\"max\", desired_max_scaled)\n",
    "    # TODO: remove the ones that already satisfy the bounds here, OR afterwards?\n",
    "    desired_max_lst.append(desired_max_scaled)\n",
    "    desired_min_lst.append(desired_min_scaled)\n",
    "    hist_inputs.append(dataset.historical_values[idx])\n",
    "    \n",
    "#for i in range(X_test.shape[0]):\n",
    "#    max_bound = (\n",
    "#        desired_max_lst[i] if desired_max_lst != None else self.MISSING_MAX_BOUND\n",
    "#    )\n",
    "#    min_bound = (\n",
    "#        desired_min_lst[i] if desired_min_lst != None else self.MISSING_MIN_BOUND\n",
    "#    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f236e-95b7-4f45-99a2-fe305c721507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical new, TEST!!! CHECK!!!\n",
    "\n",
    "def compute_loss(max_bound, min_bound, pred):\n",
    "    mse_loss_ = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "    dist_max = mse_loss_(max_bound, pred)\n",
    "    dist_min = mse_loss_(min_bound, pred)\n",
    "    loss = dist_max + dist_min\n",
    "    return loss\n",
    "\n",
    "def compute_gradient_finite_difference(model, X_e, max_bound, min_bound, epsilon=1e-4):\n",
    "    # Convert to numpy for SARIMAX\n",
    "    temp = X_e.numpy()\n",
    "    \n",
    "    # Get the original prediction\n",
    "    pred = model.forecast(horizon, start_params=start_params, exog=temp)\n",
    "    pred = tf.convert_to_tensor(pred, dtype=tf.float32)\n",
    "    \n",
    "    # Original loss\n",
    "    original_loss = compute_loss(max_bound, min_bound, pred)\n",
    "\n",
    "    # Approximate the gradient using finite differences\n",
    "    gradients = np.zeros_like(X_e.numpy())\n",
    "    for i in range(X_e.shape[0]):\n",
    "        X_e_perturbed = X_e.numpy()\n",
    "        X_e_perturbed[i] += epsilon\n",
    "        pred_perturbed = model.forecast(horizon, start_params=start_params, exog=X_e_perturbed)\n",
    "        pred_perturbed = tf.convert_to_tensor(pred_perturbed, dtype=tf.float32)\n",
    "        \n",
    "        # Compute the perturbed loss\n",
    "        perturbed_loss = compute_loss(max_bound, min_bound, pred_perturbed)\n",
    "        \n",
    "        # Finite difference (gradient approximation)\n",
    "        gradients[i] = (perturbed_loss.numpy() - original_loss.numpy()) / epsilon\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def euclidean_distance(arr1, arr2):\n",
    "    sum_sq = np.sum(np.square(arr1 - arr2))\n",
    "    return np.sqrt(sum_sq)\n",
    "    \n",
    "#all samples\n",
    "targets_sarimax = np.empty(X_test.shape)\n",
    "exogs_sarimax = np.empty(X_test.shape)\n",
    "losses_sarimax = np.empty(X_test.shape[0])\n",
    "max_bound_sarimax = np.empty(X_test.shape)\n",
    "min_bound_sarimax = np.empty(X_test.shape)\n",
    "euclidean_min_sarimax = np.empty(X_test.shape[0])\n",
    "euclidean_max_sarimax = np.empty(X_test.shape[0])\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    print(f\"{i} samples been transformed.\")\n",
    "    max_bound = (\n",
    "        desired_max_lst[i] if desired_max_lst != None else self.MISSING_MAX_BOUND\n",
    "    )\n",
    "    min_bound = (\n",
    "        desired_min_lst[i] if desired_min_lst != None else self.MISSING_MIN_BOUND\n",
    "    )\n",
    "    min_bound_sarimax[i] = min_bound\n",
    "    max_bound_sarimax[i] = max_bound\n",
    "    X = X_test[i]\n",
    "    y = Y_test[i]\n",
    "    X_test_exog = dataset.X_test_exog[i]\n",
    "#X = random.choice(dataset.X_train_exog)\n",
    "#y = random.choice(dataset.X_train_target)\n",
    "#X_test_exog = random.choice(dataset.X_test_exog)\n",
    "    \n",
    "    \n",
    "    mod = sm.tsa.SARIMAX(endog=np.asarray(y), exog=np.asarray(X), order=(1,0,0))\n",
    "    #res = mod.fit(disp=False)\n",
    "    mod = mod.fit(disp=False)\n",
    "    start_params = mod.params\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "    \n",
    "    \n",
    "    # Initialize the input tensor\n",
    "    grad_X_e = tf.convert_to_tensor(X_test_exog, dtype=tf.float32)\n",
    "    grad_X_e = tf.Variable(grad_X_e, dtype=tf.float32)\n",
    "    \n",
    "    # Initialize prediction and loss\n",
    "    \n",
    "    temp = grad_X_e.numpy()\n",
    "    pred_sarimax = mod.forecast(horizon, start_params=start_params, exog=temp)\n",
    "    pred_sarimax = tf.convert_to_tensor(pred_sarimax, dtype=tf.float32)\n",
    "    if i == 0:\n",
    "        mean_smape, mean_rmse = forecast_metrics_single(dataset.X_test_target[i], pred_sarimax)\n",
    "        print(\n",
    "            f\"model trained, with test sMAPE score {mean_smape:0.4f}; test RMSE score: {mean_rmse:0.4f}.\"\n",
    "        )\n",
    "    # Perform optimization\n",
    "    max_iter = 100\n",
    "    it = 0\n",
    "    while (tf.reduce_any(pred_sarimax > max_bound) or tf.reduce_any(pred_sarimax < min_bound)) and (it < max_iter):\n",
    "        #print(\"Iteration:\", it)\n",
    "        \n",
    "        # Compute approximate gradients using finite differences\n",
    "        gradients = compute_gradient_finite_difference(mod, grad_X_e, max_bound, min_bound)\n",
    "        \n",
    "        # Apply gradients using optimizer\n",
    "        optimizer.apply_gradients([(tf.convert_to_tensor(gradients, dtype=tf.float32), grad_X_e)])\n",
    "    \n",
    "        # Update predictions and loss\n",
    "        temp = grad_X_e.numpy()\n",
    "        pred_sarimax = mod.forecast(horizon, start_params=start_params, exog=temp)\n",
    "        pred_sarimax = tf.convert_to_tensor(pred_sarimax, dtype=tf.float32)\n",
    "        loss = compute_loss(max_bound, min_bound, pred_sarimax)\n",
    "        \n",
    "        #print(f\"Iteration {it}, Loss: {loss.numpy()}, Grad_X_e: {grad_X_e.numpy()}\")\n",
    "        it += 1\n",
    "    \n",
    "    #print(\"Optimized value of X:\", grad_X_e.numpy(), \"Best prediction:\", pred_sarimax.numpy())\n",
    "    \n",
    "    # Final inversion step if needed\n",
    "    scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "    Y_preds_original_sarimax = scaler.inverse_transform(pred_sarimax.numpy().reshape(-1, 1))  # Ensure correct shape\n",
    "    print(\"Original Prediction after scaling inversion:\", Y_preds_original_sarimax)\n",
    "    dist_min = euclidean_distance(min_bound, pred_sarimax)\n",
    "    dist_max = euclidean_distance(pred_sarimax, max_bound)\n",
    "\n",
    "    targets_sarimax[i] = Y_preds_original_sarimax\n",
    "    exogs_sarimax[i] = grad_X_e.numpy()\n",
    "    losses_sarimax[i] = loss\n",
    "    euclidean_min_sarimax[i] = dist_min\n",
    "    euclidean_max_sarimax[i] = dist_max\n",
    "\n",
    "\n",
    "print(\"results\", targets_sarimax, exogs_sarimax, losses_sarimax)\n",
    "#print(\"distance\", euclidean_min_sarimax, euclidean_max_sarimax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed84ae-223f-4e71-a4ff-03e7cfbee570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST REGRESSION CHECK!!!\n",
    "\n",
    "# Define the loss function\n",
    "def compute_loss(max_bound, min_bound, pred):\n",
    "    mse_loss_ = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
    "    dist_max = mse_loss_(max_bound, pred)\n",
    "    dist_min = mse_loss_(min_bound, pred)\n",
    "    loss = dist_max + dist_min\n",
    "    return loss\n",
    "\n",
    "# Gradient computation using finite difference method\n",
    "def compute_gradient_finite_difference(model, X_e, max_bound, min_bound, epsilon=1e-4):\n",
    "    # Convert tensor to numpy for compatibility\n",
    "    temp = X_e.numpy()\n",
    "\n",
    "    # Get the original prediction\n",
    "    pred = model.predict(temp)\n",
    "    pred = tf.convert_to_tensor(pred, dtype=tf.float32)\n",
    "\n",
    "    # Original loss\n",
    "    original_loss = compute_loss(max_bound, min_bound, pred)\n",
    "\n",
    "    # Approximate the gradient using finite differences\n",
    "    gradients = np.zeros_like(X_e.numpy())\n",
    "    for i in range(X_e.shape[0]):\n",
    "        X_e_perturbed = X_e.numpy()\n",
    "        X_e_perturbed[i] += epsilon\n",
    "        pred_perturbed = model.predict(X_e_perturbed)\n",
    "        pred_perturbed = tf.convert_to_tensor(pred_perturbed, dtype=tf.float32)\n",
    "\n",
    "        # Compute the perturbed loss\n",
    "        perturbed_loss = compute_loss(max_bound, min_bound, pred_perturbed)\n",
    "\n",
    "        # Finite difference (gradient approximation)\n",
    "        gradients[i] = (perturbed_loss.numpy() - original_loss.numpy()) / epsilon\n",
    "\n",
    "    return gradients\n",
    "\n",
    "\n",
    "targets_ols = np.empty(X_test.shape)\n",
    "exogs_ols = np.empty(X_test.shape)\n",
    "losses_ols = np.empty(X_test.shape[0])\n",
    "max_bound_ols = np.empty(X_test.shape)\n",
    "min_bound_ols = np.empty(X_test.shape)\n",
    "euclidean_min_ols = np.empty(X_test.shape[0])\n",
    "euclidean_max_ols = np.empty(X_test.shape[0])\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    print(f\"{i} samples been transformed.\")\n",
    "    max_bound = (\n",
    "        desired_max_lst[i] if desired_max_lst != None else self.MISSING_MAX_BOUND\n",
    "    )\n",
    "    min_bound = (\n",
    "        desired_min_lst[i] if desired_min_lst != None else self.MISSING_MIN_BOUND\n",
    "    )\n",
    "    min_bound_ols[i] = min_bound\n",
    "    max_bound_ols[i] = max_bound\n",
    "    X = X_test[i]\n",
    "    y = Y_test[i]\n",
    "    X_test_exog = dataset.X_test_exog[i]\n",
    "\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # Optimizer setup\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "    \n",
    "    # Initialize the input tensor\n",
    "    grad_X_e = tf.convert_to_tensor(X_test_exog, dtype=tf.float32)\n",
    "    grad_X_e = tf.Variable(grad_X_e, dtype=tf.float32)\n",
    "    \n",
    "    \n",
    "    pred_ols = model.predict(grad_X_e.numpy())\n",
    "    pred_ols = tf.convert_to_tensor(pred_ols, dtype=tf.float32)\n",
    "    \n",
    "    if i == 0:\n",
    "        mean_smape, mean_rmse = forecast_metrics_single(dataset.X_test_target[i], pred_ols)\n",
    "        print(\n",
    "            f\"model trained, with test sMAPE score {mean_smape:0.4f}; test RMSE score: {mean_rmse:0.4f}.\"\n",
    "        )\n",
    "        \n",
    "    # Perform optimization\n",
    "    max_iter = 100\n",
    "    it = 0\n",
    "    while (tf.reduce_any(pred_ols > max_bound) or tf.reduce_any(pred_ols < min_bound)) and (it < max_iter):\n",
    "        # Add intercept to test data\n",
    "        #grad_X_e_with_intercept = sm.add_constant(grad_X_e.numpy())\n",
    "    \n",
    "        # Get predictions\n",
    "        pred_ols = model.predict(grad_X_e.numpy())\n",
    "        pred_ols = tf.convert_to_tensor(pred_ols, dtype=tf.float32)\n",
    "    \n",
    "        # Check bounds\n",
    "        #print(f\"Iteration: {it}\")\n",
    "    \n",
    "        # Compute approximate gradients using finite differences\n",
    "        gradients = compute_gradient_finite_difference(model, grad_X_e, max_bound, min_bound)\n",
    "    \n",
    "        # Apply gradients using optimizer\n",
    "        gradients_tensor = tf.convert_to_tensor(gradients, dtype=tf.float32)\n",
    "        optimizer.apply_gradients([(gradients_tensor, grad_X_e)])\n",
    "    \n",
    "        # Update predictions\n",
    "        grad_X_e_with_intercept = sm.add_constant(grad_X_e.numpy())\n",
    "        pred_ols = model.predict(grad_X_e.numpy())\n",
    "        pred_ols = tf.convert_to_tensor(pred_ols, dtype=tf.float32)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = compute_loss(max_bound, min_bound, pred_ols)\n",
    "        #print(f\"Iteration {it}, Loss: {loss.numpy()}, Grad_X_e: {grad_X_e.numpy()}\")\n",
    "        it += 1\n",
    "\n",
    "    # Output results\n",
    "    print(f\"Optimized value of X: {grad_X_e.numpy()}, Best prediction: {pred_ols.numpy()}\")\n",
    "    \n",
    "    # Final inversion step if needed\n",
    "    scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "    Y_preds_original_reg = scaler.inverse_transform(pred_ols.numpy().reshape(-1, 1))  # Ensure correct shape\n",
    "    print(\"Original Prediction after scaling inversion:\", Y_preds_original_reg)\n",
    "    dist_min = euclidean_distance(min_bound, pred_ols)\n",
    "    dist_max = euclidean_distance(pred_ols, max_bound)\n",
    "\n",
    "\n",
    "    targets_ols[i] = Y_preds_original_reg\n",
    "    exogs_ols[i] = grad_X_e.numpy()\n",
    "    losses_ols[i] = loss\n",
    "    euclidean_min_ols[i] = dist_min\n",
    "    euclidean_max_ols[i] = dist_max\n",
    "\n",
    "\n",
    "print(\"results\", targets_ols, exogs_ols, losses_ols, euclidean_min_ols, euclidean_max_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0dfd3f-540a-4424-b22a-23c3c5a42bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU\n",
    "def compute_loss(max_bound, min_bound, pred):\n",
    "    mse_loss_ = tf.keras.losses.MeanSquaredError(\n",
    "        reduction=tf.keras.losses.Reduction.SUM\n",
    "    )\n",
    "    dist_max = mse_loss_(max_bound, pred)\n",
    "    dist_min = mse_loss_(min_bound, pred)\n",
    "    loss = dist_max + dist_min\n",
    "    return loss\n",
    "\n",
    "\n",
    "targets_gru = np.empty(X_test.shape)\n",
    "exogs_gru = np.empty(X_test.shape)\n",
    "losses_gru = np.empty(X_test.shape[0])\n",
    "max_bound_gru = np.empty(X_test.shape)\n",
    "min_bound_gru = np.empty(X_test.shape)\n",
    "\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    print(f\"{i} samples been transformed.\")\n",
    "    max_bound = (\n",
    "        desired_max_lst[i] if desired_max_lst != None else self.MISSING_MAX_BOUND\n",
    "    )\n",
    "    min_bound = (\n",
    "        desired_min_lst[i] if desired_min_lst != None else self.MISSING_MIN_BOUND\n",
    "    )\n",
    "    min_bound_gru[i] = min_bound\n",
    "    max_bound_gru[i] = max_bound\n",
    "    X = X_test[i]\n",
    "    y = Y_test[i]\n",
    "    X_test_exog = dataset.X_test_exog[i]\n",
    "#X = dataset.X_train_exog\n",
    "#X = random.choice(X)\n",
    "#X = tf.Variable(tf.convert_to_tensor(X, dtype=tf.float32), dtype=tf.float32)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.05,  epsilon=1e-07,)\n",
    "    \n",
    "    #tf_model.compile(optimizer=optimizer, loss=\"mae\")\n",
    "    #tf_model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "    grad_X_e = tf.convert_to_tensor(X_test_exog, dtype=tf.float32)\n",
    "    grad_X_e = tf.Variable(grad_X_e, dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(grad_X_e)\n",
    "        # Calculate the value of the function and record the gradient\n",
    "        pred_gru = tf_model(tf.expand_dims(grad_X_e, axis=0))\n",
    "        pred_gru = tf.squeeze(pred_gru)\n",
    "        loss = compute_loss(max_bound, min_bound, pred_gru)\n",
    "        print(loss)\n",
    "#pred = tf_model(grad_X_e)\n",
    "\n",
    "    #if i == 0:\n",
    "    #    mean_smape, mean_rmse = forecast_metrics(dataset, pred_gru)\n",
    "    #    print(\n",
    "    #        f\"model trained, with test sMAPE score {mean_smape:0.4f}; test RMSE score: {mean_rmse:0.4f}.\"\n",
    "    #    )\n",
    "        \n",
    "    max_iter = 100\n",
    "    it = 0\n",
    "    while (tf.reduce_any(pred_gru>max_bound) or tf.reduce_any(pred_gru<min_bound)) and (it<max_iter):\n",
    "        #change (X_e)\n",
    "        gradient = tape.gradient(loss, grad_X_e)\n",
    "        #print(gradient)\n",
    "        if gradient is None:\n",
    "            print(\"no gradient\")\n",
    "            #break\n",
    "    \n",
    "        # Use the Adam optimizer to update the value of x\n",
    "        optimizer.apply_gradients([(gradient, grad_X_e)])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(grad_X_e)\n",
    "            # Calculate the value of the function and record the gradient\n",
    "            pred_gru = tf_model(tf.expand_dims(grad_X_e, axis=0))\n",
    "            pred_gru = tf.squeeze(pred_gru)\n",
    "            loss = compute_loss(max_bound, min_bound, pred_gru)\n",
    "        # Record the current value of x\n",
    "        #print(f\"Iteration {it}, Loss: {loss.numpy()}, Grad_X_e: {grad_X_e.numpy()}\")\n",
    "        it += 1\n",
    "        \n",
    "    #print(\"Optimized value of x:\", grad_X_e.numpy())\n",
    "    final_pred = tf_model(tf.expand_dims(grad_X_e, axis=0))\n",
    "    #print(\"Value of the function at the optimized point:\", final_pred.numpy())\n",
    "    scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "    Y_preds_original_gru = scaler.inverse_transform(final_pred.numpy().reshape(-1, 1))  # Ensure correct shape\n",
    "    #print(\"Original Prediction after scaling inversion:\", Y_preds_original_gru)\n",
    "\n",
    "\n",
    "    targets_gru[i] = Y_preds_original_gru\n",
    "    exogs_gru[i] = grad_X_e.numpy()\n",
    "    losses_gru[i] = loss\n",
    "\n",
    "\n",
    "print(\"results\", targets_gru, exogs_gru, losses_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a62ad-0f7c-46c2-bec2-8dbcddb20dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(targets_gru[1])):\n",
    "    scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "    #scaler_exog = dataset.scaler[dataset.test_idxs[i]][CHANGE_COLS]\n",
    "    target = scaler.inverse_transform(targets_gru[1])\n",
    "    #exog = scaler.inverse_transform(exogs_gru[0])\n",
    "    min_bound_true = scaler.inverse_transform(min_bound_gru[1])\n",
    "    max_bound_true = scaler.inverse_transform(max_bound_gru[1])\n",
    "\n",
    "print(target[:,0], min_bound_true[:,0], max_bound_true[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "313abb76-3dbd-487d-a386-2ae1819d6581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101.1        100.11449275  99.12898551  98.14347826  97.15797101\n",
      "  96.17246377  95.18695652  94.20144928  93.21594203  92.23043478\n",
      "  91.24492754  90.25942029  89.27391304  88.2884058   87.30289855\n",
      "  86.3173913   85.33188406  84.34637681  83.36086957  82.37536232\n",
      "  81.38985507  80.40434783  79.41884058  78.43333333] [104.23333333 103.24782609 102.26231884 101.27681159 100.29130435\n",
      "  99.3057971   98.32028986  97.33478261  96.34927536  95.36376812\n",
      "  94.37826087  93.39275362  92.40724638  91.42173913  90.43623188\n",
      "  89.45072464  88.46521739  87.47971014  86.4942029   85.50869565\n",
      "  84.52318841  83.53768116  82.55217391  81.56666667]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(targets_sarimax[1])):\n",
    "    scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "    #scaler_exog = dataset.scaler[dataset.test_idxs[i]][CHANGE_COLS]\n",
    "    #exog = scaler.inverse_transform(exogs_gru[0])\n",
    "    min_bound_true_sarimax = scaler.inverse_transform(min_bound_sarimax[1])\n",
    "    max_bound_true_sarimax = scaler.inverse_transform(max_bound_sarimax[1])\n",
    "\n",
    "print(min_bound_true_sarimax[:,0], max_bound_true_sarimax[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a3b6f79f-e760-4919-90ff-bdc8c9fd081e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "test = dataset.X_test_target\n",
    "hypo = list()\n",
    "for i in range(len(test)):\n",
    "    scaler = dataset.scaler[dataset.test_idxs[i]][TARGET_COL]\n",
    "    Y_preds_original = scaler.inverse_transform(pred_tf[i].numpy())\n",
    "    test_true = scaler.inverse_transform(test[i])\n",
    "    if np.any(Y_preds_original <= hypo_bound):\n",
    "        hypo.append(test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b5530ae-2df2-432a-bf3c-53bfaeea4293",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#plt.plot(X_test_exog_true, label='exog1')#data[look_back+1:][0]\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43max\u001b[49m\u001b[38;5;241m.\u001b[39mplot(targets_sarimax[a][:,\u001b[38;5;241m0\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_sarimax\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(targets_gru[a][:,\u001b[38;5;241m0\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_gru\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(targets_ols[a][:,\u001b[38;5;241m0\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_ols\u001b[39m\u001b[38;5;124m'\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ax' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PLOT\n",
    "#print(orig_train, orig_test)\n",
    "plt.figure(figsize=(12,6))\n",
    "#fig, ax = plt.subplots()\n",
    "#fig.suptitle('Time-Series Forecasting')\n",
    "a=0\n",
    "#plt.plot(X_test_exog_true, label='exog1')#data[look_back+1:][0]\n",
    "ax.plot(targets_sarimax[a][:,0], label='target_sarimax', color='r')\n",
    "ax.plot(targets_gru[a][:,0], label='target_gru', color='g')\n",
    "ax.plot(targets_ols[a][:,0], label='target_ols', color='b')\n",
    "ax.plot(hypo[a], label='actual', color='y')\n",
    "ax.plot(min_bound_true_sarimax[a][:,0], label='min', color='black')\n",
    "ax.plot(max_bound_true_sarimax[a][:,0], label='max', color='black')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "#ax.title('Time-Series Forecasting')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#Y_preds_original_gru, min_bound_true, max_bound_true\n",
    "fig.savefig(\"forecasting.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3b591-a811-48d3-9a74-2a8fcba1e0d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7142232c-99e0-4318-a46d-8fe305b2df95",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 15:34:07.142579: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2024-12-16 15:36:44.814209: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 3s 8ms/step\n",
      "[[[0.30662084]\n",
      "  [0.31125593]\n",
      "  [0.31132373]\n",
      "  ...\n",
      "  [0.30645213]\n",
      "  [0.30144808]\n",
      "  [0.30556005]]\n",
      "\n",
      " [[0.30662084]\n",
      "  [0.31125593]\n",
      "  [0.31132373]\n",
      "  ...\n",
      "  [0.30645213]\n",
      "  [0.30144808]\n",
      "  [0.30556005]]\n",
      "\n",
      " [[0.30662084]\n",
      "  [0.31125593]\n",
      "  [0.31132373]\n",
      "  ...\n",
      "  [0.30645213]\n",
      "  [0.30144808]\n",
      "  [0.30556005]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.30662084]\n",
      "  [0.31125593]\n",
      "  [0.31132373]\n",
      "  ...\n",
      "  [0.30645213]\n",
      "  [0.30144808]\n",
      "  [0.30556005]]\n",
      "\n",
      " [[0.30662084]\n",
      "  [0.31125593]\n",
      "  [0.31132373]\n",
      "  ...\n",
      "  [0.30645213]\n",
      "  [0.30144808]\n",
      "  [0.30556005]]\n",
      "\n",
      " [[0.30662084]\n",
      "  [0.31125593]\n",
      "  [0.31132373]\n",
      "  ...\n",
      "  [0.30645213]\n",
      "  [0.30144808]\n",
      "  [0.30556005]]]\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "# Prepare the data\n",
    "X = dataset.X_train_exog  # Exogenous input: Shape (num_samples, back_horizon, n_in_features)\n",
    "y = dataset.X_train_target  # Target: Shape (num_samples, horizon)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(args.random_seed)\n",
    "\n",
    "# Define model hyperparameters\n",
    "horizon = args.horizon  # Forecast horizon\n",
    "back_horizon = args.back_horizon  # Backcast horizon\n",
    "n_in_features = X.shape[2]  # Number of input features\n",
    "n_hidden_units = 100  # Hidden layer size (equivalent to GRU units)\n",
    "\n",
    "# Create the NBeatsNet model\n",
    "nbeats_model = NBeatsNet(\n",
    "    input_dim=n_in_features,  # Number of input features\n",
    "    output_dim=1,  # Number of output features per time step (typically 1)\n",
    "    backcast_length=back_horizon,\n",
    "    forecast_length=horizon,\n",
    "    stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),  # Use generic blocks\n",
    "    nb_blocks_per_stack=2,  # Number of blocks per stack\n",
    "    hidden_layer_units=n_hidden_units,  # Equivalent to GRU units\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "nbeats_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=\"mae\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "nbeats_model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "predictions = nbeats_model.predict(dataset.X_test_exog)  # Test predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d85788b1-5cbf-4696-9350-da3954265e6d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 14:24:13.657645: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2024-12-16 14:30:11.007834: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156/156 [==============================] - 3s 15ms/step\n",
      "[[[0.17782977]\n",
      "  [0.17884499]\n",
      "  [0.18326782]\n",
      "  ...\n",
      "  [0.27964562]\n",
      "  [0.2694362 ]\n",
      "  [0.25506657]]\n",
      "\n",
      " [[0.2372778 ]\n",
      "  [0.2342448 ]\n",
      "  [0.23786803]\n",
      "  ...\n",
      "  [0.1380234 ]\n",
      "  [0.14417213]\n",
      "  [0.13886356]]\n",
      "\n",
      " [[0.10552319]\n",
      "  [0.08188455]\n",
      "  [0.07310452]\n",
      "  ...\n",
      "  [0.31579325]\n",
      "  [0.34583175]\n",
      "  [0.35718545]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.34906587]\n",
      "  [0.35519826]\n",
      "  [0.36571905]\n",
      "  ...\n",
      "  [0.3450673 ]\n",
      "  [0.34745586]\n",
      "  [0.34187752]]\n",
      "\n",
      " [[0.34906587]\n",
      "  [0.35519826]\n",
      "  [0.36571905]\n",
      "  ...\n",
      "  [0.3450673 ]\n",
      "  [0.34745586]\n",
      "  [0.34187752]]\n",
      "\n",
      " [[0.34906587]\n",
      "  [0.35519826]\n",
      "  [0.36571905]\n",
      "  ...\n",
      "  [0.3450673 ]\n",
      "  [0.34745586]\n",
      "  [0.34187752]]]\n"
     ]
    }
   ],
   "source": [
    "#NBEATS\n",
    "import tensorflow as tf\n",
    "from nbeats_keras.model import NBeatsNet\n",
    "\n",
    "# Prepare the data\n",
    "X = dataset.X_train_exog  # Shape: (num_samples, back_horizon, num_features)\n",
    "y = dataset.X_train_target  # Shape: (num_samples, horizon)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(args.random_seed)\n",
    "\n",
    "# Define model hyperparameters\n",
    "horizon, back_horizon = args.horizon, args.back_horizon\n",
    "\n",
    "# Create the N-BEATSx model\n",
    "nbeats_model = NBeatsNet(\n",
    "    stack_types=(NBeatsNet.GENERIC_BLOCK, NBeatsNet.GENERIC_BLOCK),\n",
    "    forecast_length=horizon,\n",
    "    backcast_length=back_horizon,\n",
    "    hidden_layer_units=256,\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "nbeats_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=\"mae\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "nbeats_model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "pred_nbeats = nbeats_model.predict(dataset.X_test_exog)  # Predictions on the test set\n",
    "print(pred_nbeats)\n",
    "\n",
    "mean_smape, mean_rmse = forecast_metrics(dataset, pred_nbeats)\n",
    "print(\n",
    "    f\"model trained, with test sMAPE score {mean_smape:0.4f}; test RMSE score: {mean_rmse:0.4f}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0cfc1728-3c67-40ee-bd4f-60c1caca4407",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 samples been transformed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 15:44:59.222752: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "No gradient defined for operation'IteratorGetNext' (op type: IteratorGetNext). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m tape\u001b[38;5;241m.\u001b[39mwatch(grad_X_e)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Calculate the value of the function and record the gradient\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m pred_nbeats \u001b[38;5;241m=\u001b[39m \u001b[43mnbeats_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_X_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m pred_nbeats \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(pred_nbeats)\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(max_bound, min_bound, pred_nbeats)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/nbeats_keras/model.py:203\u001b[0m, in \u001b[0;36mNBeatsNet.__getattr__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn_backcast\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    202\u001b[0m     cast_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_BACKCAST\n\u001b[0;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcast_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/tensorflow/python/ops/gradients_util.py:674\u001b[0m, in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\u001b[0m\n\u001b[1;32m    672\u001b[0m       grad_fn \u001b[38;5;241m=\u001b[39m func_call\u001b[38;5;241m.\u001b[39mpython_grad_func\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 674\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(\n\u001b[1;32m    675\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradient defined for operation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    676\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (op type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mop\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    677\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn general every operation must have an associated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    678\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`@tf.RegisterGradient` for correct autodiff, which this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    679\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mop is lacking. If you want to pretend this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    680\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperation is a constant in your program, you may insert \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    681\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.stop_gradient`. This can be useful to silence the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in cases where you know gradients are not needed, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me.g. the forward pass of tf.custom_gradient. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see more details in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    685\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/api_docs/python/tf/custom_gradient.\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=line-too-long\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop_state:\n\u001b[1;32m    687\u001b[0m   loop_state\u001b[38;5;241m.\u001b[39mEnterGradWhileContext(op, before\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mLookupError\u001b[0m: No gradient defined for operation'IteratorGetNext' (op type: IteratorGetNext). In general every operation must have an associated `@tf.RegisterGradient` for correct autodiff, which this op is lacking. If you want to pretend this operation is a constant in your program, you may insert `tf.stop_gradient`. This can be useful to silence the error in cases where you know gradients are not needed, e.g. the forward pass of tf.custom_gradient. Please see more details in https://www.tensorflow.org/api_docs/python/tf/custom_gradient."
     ]
    }
   ],
   "source": [
    "#nbeats\n",
    "def compute_loss(max_bound, min_bound, pred):\n",
    "    mse_loss_ = tf.keras.losses.MeanSquaredError(\n",
    "        reduction=tf.keras.losses.Reduction.SUM\n",
    "    )\n",
    "    dist_max = mse_loss_(max_bound, pred)\n",
    "    dist_min = mse_loss_(min_bound, pred)\n",
    "    loss = dist_max + dist_min\n",
    "    return loss\n",
    "\n",
    "\n",
    "targets_nbeats = np.empty(X_test.shape)\n",
    "exogs_nbeats = np.empty(X_test.shape)\n",
    "losses_nbeats = np.empty(X_test.shape[0])\n",
    "max_bound_nbeats = np.empty(X_test.shape)\n",
    "min_bound_nbeats = np.empty(X_test.shape)\n",
    "\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    print(f\"{i} samples been transformed.\")\n",
    "    max_bound = (\n",
    "        desired_max_lst[i] if desired_max_lst != None else self.MISSING_MAX_BOUND\n",
    "    )\n",
    "    min_bound = (\n",
    "        desired_min_lst[i] if desired_min_lst != None else self.MISSING_MIN_BOUND\n",
    "    )\n",
    "    min_bound_nbeats[i] = min_bound\n",
    "    max_bound_nbeats[i] = max_bound\n",
    "    X = X_test[i]\n",
    "    y = Y_test[i]\n",
    "    X_test_exog = dataset.X_test_exog[i]\n",
    "#X = dataset.X_train_exog\n",
    "#X = random.choice(X)\n",
    "#X = tf.Variable(tf.convert_to_tensor(X, dtype=tf.float32), dtype=tf.float32)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.05,  epsilon=1e-07,)\n",
    "    \n",
    "    #nbeatsx_model.compile(optimizer=optimizer, loss=\"mae\")\n",
    "    #nbeatsx_model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "    grad_X_e = tf.convert_to_tensor(X_test_exog, dtype=tf.float32)\n",
    "    grad_X_e = tf.Variable(grad_X_e, dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(grad_X_e)\n",
    "        # Calculate the value of the function and record the gradient\n",
    "        pred_nbeats = nbeats_model.predict(tf.expand_dims(grad_X_e, axis=0))\n",
    "        pred_nbeats = tf.squeeze(pred_nbeats)\n",
    "        loss = compute_loss(max_bound, min_bound, pred_nbeats)\n",
    "        print(loss)\n",
    "#pred = nbeatsx_model(grad_X_e)\n",
    "\n",
    "    #if i == 0:\n",
    "    #    mean_smape, mean_rmse = forecast_metrics(dataset, pred_nbeats)\n",
    "    #    print(\n",
    "    #        f\"model trained, with test sMAPE score {mean_smape:0.4f}; test RMSE score: {mean_rmse:0.4f}.\"\n",
    "    #    )\n",
    "        \n",
    "    max_iter = 100\n",
    "    it = 0\n",
    "    while (tf.reduce_any(pred_nbeats>max_bound) or tf.reduce_any(pred_nbeats<min_bound)) and (it<max_iter):\n",
    "        #change (X_e)\n",
    "        gradient = tape.gradient(loss, grad_X_e)\n",
    "        #print(gradient)\n",
    "        if gradient is None:\n",
    "            print(\"no gradient\")\n",
    "            #break\n",
    "    \n",
    "        # Use the Adam optimizer to update the value of x\n",
    "        optimizer.apply_gradients([(gradient, grad_X_e)])\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(grad_X_e)\n",
    "            # Calculate the value of the function and record the gradient\n",
    "            pred_nbeats = nbeats_model.predict(tf.expand_dims(grad_X_e, axis=0))\n",
    "            pred_nbeats = tf.squeeze(pred_nbeats)\n",
    "            loss = compute_loss(max_bound, min_bound, pred_nbeats)\n",
    "        # Record the current value of x\n",
    "        #print(f\"Iteration {it}, Loss: {loss.numpy()}, Grad_X_e: {grad_X_e.numpy()}\")\n",
    "        it += 1\n",
    "        \n",
    "    print(\"Optimized value of x:\", grad_X_e.numpy())\n",
    "    final_pred = nbeatsx_model.predict(tf.expand_dims(grad_X_e, axis=0))\n",
    "    print(\"Value of the function at the optimized point:\", final_pred.numpy())\n",
    "\n",
    "\n",
    "    targets_nbeats[i] = final_pred\n",
    "    exogs_nbeats[i] = grad_X_e.numpy()\n",
    "    losses_nbeats[i] = loss\n",
    "\n",
    "\n",
    "print(\"results\", targets_nbeats, exogs_nbeats, losses_nbeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cfc6af6-e1f6-45f1-88e7-7b04fd35e280",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07598784]\n",
      " [0.06990881]\n",
      " [0.07902736]\n",
      " [0.09422492]\n",
      " [0.11550152]\n",
      " [0.15197568]\n",
      " [0.17933131]\n",
      " [0.21276596]\n",
      " [0.24620061]\n",
      " [0.27659574]\n",
      " [0.29483283]\n",
      " [0.30395137]\n",
      " [0.30395137]\n",
      " [0.31306991]\n",
      " [0.3100304 ]\n",
      " [0.29179331]\n",
      " [0.25835866]\n",
      " [0.2674772 ]\n",
      " [0.26443769]\n",
      " [0.2674772 ]\n",
      " [0.27051672]\n",
      " [0.27051672]\n",
      " [0.26443769]\n",
      " [0.25531915]] [[0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.48245614 0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.9047619  0.         0.         0.        ]\n",
      " [0.9047619  0.         0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.9047619  0.49122807 0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]\n",
      " [0.45238095 0.         0.         0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/statsmodels/tsa/statespace/mlemodel.py:3014: RuntimeWarning: invalid value encountered in divide\n",
      "  return self.params / self.bse\n",
      "/home/tomkemeyer/miniconda3/lib/python3.11/site-packages/statsmodels/tsa/statespace/representation.py:374: FutureWarning: Unknown keyword arguments: dict_keys(['start_params']).Passing unknown keyword arguments will raise a TypeError beginning in version 0.15.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                   24\n",
      "Model:               SARIMAX(1, 0, 0)   Log Likelihood                  59.406\n",
      "Date:                Fri, 13 Dec 2024   AIC                           -106.811\n",
      "Time:                        11:49:21   BIC                            -99.743\n",
      "Sample:                             0   HQIC                          -104.936\n",
      "                                 - 24                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.0320      0.028      1.146      0.252      -0.023       0.087\n",
      "x2            -0.0058      0.019     -0.312      0.755      -0.043       0.031\n",
      "const               0         -0        nan        nan           0           0\n",
      "x3                  0         -0        nan        nan           0           0\n",
      "ar.L1          0.9948      0.011     94.469      0.000       0.974       1.015\n",
      "sigma2         0.0003      0.000      2.845      0.004       0.000       0.001\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   7.42   Jarque-Bera (JB):                 0.38\n",
      "Prob(Q):                              0.01   Prob(JB):                         0.83\n",
      "Heteroskedasticity (H):               0.39   Skew:                            -0.30\n",
      "Prob(H) (two-sided):                  0.20   Kurtosis:                         2.92\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
      "[2] Covariance matrix is singular or near-singular, with condition number 7.02e+35. Standard errors may be unstable.\n",
      "[0.25406326 0.25281391 0.25157108 0.24751305 0.24910483 0.24788134\n",
      " 0.24666423 0.24545347 0.25872994 0.25753178 0.25346687 0.25228117\n",
      " 0.25110165 0.24992828 0.24876103 0.24759987 0.24644476 0.23368774\n",
      " 0.23254465 0.23140752 0.23027631 0.22915101 0.22803157 0.22691798]\n",
      "max_min [[0.8036    ]\n",
      " [0.78051201]\n",
      " [0.75742402]\n",
      " [0.73433602]\n",
      " [0.71124803]\n",
      " [0.68816004]\n",
      " [0.66507205]\n",
      " [0.64198406]\n",
      " [0.61889607]\n",
      " [0.59580807]\n",
      " [0.57272008]\n",
      " [0.54963209]\n",
      " [0.5265441 ]\n",
      " [0.50345611]\n",
      " [0.48036812]\n",
      " [0.45728012]\n",
      " [0.43419213]\n",
      " [0.41110414]\n",
      " [0.38801615]\n",
      " [0.36492816]\n",
      " [0.34184017]\n",
      " [0.31875217]\n",
      " [0.29566418]\n",
      " [0.27257619]] [[0.7644    ]\n",
      " [0.74131201]\n",
      " [0.71822402]\n",
      " [0.69513602]\n",
      " [0.67204803]\n",
      " [0.64896004]\n",
      " [0.62587205]\n",
      " [0.60278406]\n",
      " [0.57969607]\n",
      " [0.55660807]\n",
      " [0.53352008]\n",
      " [0.51043209]\n",
      " [0.4873441 ]\n",
      " [0.46425611]\n",
      " [0.44116812]\n",
      " [0.41808012]\n",
      " [0.39499213]\n",
      " [0.37190414]\n",
      " [0.34881615]\n",
      " [0.32572816]\n",
      " [0.30264017]\n",
      " [0.27955217]\n",
      " [0.25646418]\n",
      " [0.23337619]]\n",
      "0\n",
      "[0.2540844  0.25283505 0.25159222 0.24753164 0.24912597 0.24790249\n",
      " 0.24668538 0.24547462 0.25875727 0.2575591  0.25349161 0.2523059\n",
      " 0.25112638 0.24995301 0.24878576 0.2476246  0.24646949 0.23370888\n",
      " 0.23256579 0.23142866 0.23029746 0.22917215 0.22805272 0.22693912]\n",
      "1\n",
      "[0.25410556 0.25285621 0.25161338 0.24755024 0.24914713 0.24792365\n",
      " 0.24670654 0.24549578 0.2587846  0.25758643 0.25351634 0.25233063\n",
      " 0.25115111 0.24997774 0.24881049 0.24764933 0.24649422 0.23373004\n",
      " 0.23258695 0.23144982 0.23031862 0.22919331 0.22807388 0.22696028]\n",
      "2\n",
      "[0.25412673 0.25287739 0.25163456 0.24756885 0.24916831 0.24794482\n",
      " 0.24672771 0.24551695 0.25881192 0.25761375 0.25354106 0.25235536\n",
      " 0.25117584 0.25000247 0.24883522 0.24767406 0.24651895 0.23375122\n",
      " 0.23260813 0.23147099 0.23033979 0.22921449 0.22809505 0.22698146]\n",
      "3\n",
      "[0.25414792 0.25289858 0.25165575 0.24758748 0.2491895  0.24796601\n",
      " 0.2467489  0.24553814 0.25883924 0.25764108 0.25356579 0.25238008\n",
      " 0.25120056 0.25002719 0.24885994 0.24769878 0.24654367 0.23377241\n",
      " 0.23262932 0.23149219 0.23036098 0.22923568 0.22811624 0.22700265]\n",
      "4\n",
      "[0.25416913 0.25291979 0.25167696 0.24760612 0.24921071 0.24798722\n",
      " 0.24677011 0.24555935 0.25886657 0.2576684  0.25359051 0.2524048\n",
      " 0.25122528 0.25005191 0.24888466 0.2477235  0.24656839 0.23379362\n",
      " 0.23265052 0.23151339 0.23038219 0.22925689 0.22813745 0.22702385]\n",
      "5\n",
      "[0.25419035 0.25294101 0.25169818 0.24762478 0.24923193 0.24800844\n",
      " 0.24679133 0.24558057 0.25889389 0.25769572 0.25361522 0.25242952\n",
      " 0.25125    0.25007663 0.24890938 0.24774822 0.24659311 0.23381484\n",
      " 0.23267174 0.23153461 0.23040341 0.22927811 0.22815867 0.22704507]\n",
      "6\n",
      "[0.25421159 0.25296224 0.25171941 0.24764345 0.24925316 0.24802968\n",
      " 0.24681257 0.24560181 0.2589212  0.25772304 0.25363994 0.25245424\n",
      " 0.25127472 0.25010135 0.2489341  0.24777293 0.24661782 0.23383607\n",
      " 0.23269298 0.23155585 0.23042465 0.22929934 0.22817991 0.22706631]\n",
      "7\n",
      "[0.25423284 0.2529835  0.25174067 0.24766213 0.24927442 0.24805093\n",
      " 0.24683382 0.24562306 0.25894852 0.25775035 0.25366465 0.25247895\n",
      " 0.25129943 0.25012606 0.24895881 0.24779764 0.24664254 0.23385733\n",
      " 0.23271423 0.2315771  0.2304459  0.2293206  0.22820116 0.22708756]\n",
      "8\n",
      "[0.25425411 0.25300477 0.25176194 0.24768083 0.24929569 0.2480722\n",
      " 0.24685509 0.24564433 0.25897584 0.25777767 0.25368936 0.25250366\n",
      " 0.25132414 0.25015077 0.24898352 0.24782235 0.24666725 0.2338786\n",
      " 0.2327355  0.23159837 0.23046717 0.22934186 0.22822243 0.22710883]\n",
      "9\n",
      "[0.25427539 0.25302605 0.25178322 0.24769954 0.24931697 0.24809348\n",
      " 0.24687637 0.24566561 0.25900315 0.25780498 0.25371407 0.25252836\n",
      " 0.25134884 0.25017547 0.24900822 0.24784706 0.24669195 0.23389988\n",
      " 0.23275679 0.23161965 0.23048845 0.22936315 0.22824371 0.22713011]\n",
      "10\n",
      "[0.25429669 0.25304735 0.25180452 0.24771827 0.24933827 0.24811478\n",
      " 0.24689767 0.24568691 0.25903046 0.2578323  0.25373877 0.25255307\n",
      " 0.25137355 0.25020018 0.24903293 0.24787176 0.24671665 0.23392118\n",
      " 0.23277808 0.23164095 0.23050975 0.22938445 0.22826501 0.22715141]\n",
      "11\n",
      "[0.25431801 0.25306866 0.25182583 0.24773701 0.24935958 0.24813609\n",
      " 0.24691899 0.24570822 0.25905777 0.25785961 0.25376347 0.25257777\n",
      " 0.25139825 0.25022488 0.24905763 0.24789646 0.24674135 0.23394249\n",
      " 0.2327994  0.23166227 0.23053106 0.22940576 0.22828633 0.22717273]\n",
      "12\n",
      "[0.25433934 0.25308999 0.25184716 0.24775576 0.24938091 0.24815742\n",
      " 0.24694032 0.24572955 0.25908508 0.25788691 0.25378817 0.25260246\n",
      " 0.25142294 0.25024957 0.24908232 0.24792116 0.24676605 0.23396382\n",
      " 0.23282073 0.2316836  0.23055239 0.22942709 0.22830766 0.22719406]\n",
      "13\n",
      "[0.25436068 0.25311134 0.25186851 0.24777453 0.24940226 0.24817877\n",
      " 0.24696166 0.2457509  0.25911239 0.25791422 0.25381286 0.25262716\n",
      " 0.25144764 0.25027427 0.24910702 0.24794585 0.24679074 0.23398517\n",
      " 0.23284207 0.23170494 0.23057374 0.22944844 0.228329   0.2272154 ]\n",
      "14\n",
      "[0.25438204 0.2531327  0.25188987 0.24779331 0.24942362 0.24820013\n",
      " 0.24698302 0.24577226 0.25913969 0.25794152 0.25383755 0.25265185\n",
      " 0.25147233 0.25029896 0.24913171 0.24797054 0.24681543 0.23400653\n",
      " 0.23286343 0.2317263  0.2305951  0.2294698  0.22835036 0.22723676]\n",
      "15\n",
      "[0.25440342 0.25315407 0.25191124 0.24781211 0.24944499 0.24822151\n",
      " 0.2470044  0.24579364 0.25916699 0.25796883 0.25386224 0.25267653\n",
      " 0.25149701 0.25032364 0.24915639 0.24799523 0.24684012 0.2340279\n",
      " 0.23288481 0.23174768 0.23061648 0.22949117 0.22837174 0.22725814]\n",
      "16\n",
      "[0.25442481 0.25317546 0.25193264 0.24783092 0.24946638 0.2482429\n",
      " 0.24702579 0.24581503 0.25919429 0.25799613 0.25388692 0.25270122\n",
      " 0.2515217  0.25034833 0.24918108 0.24801991 0.2468648  0.2340493\n",
      " 0.2329062  0.23176907 0.23063787 0.22951256 0.22839313 0.22727953]\n",
      "17\n",
      "[0.25444622 0.25319687 0.25195404 0.24784975 0.24948779 0.2482643\n",
      " 0.2470472  0.24583643 0.25922159 0.25802342 0.2539116  0.2527259\n",
      " 0.25154638 0.25037301 0.24920576 0.24804459 0.24688948 0.2340707\n",
      " 0.23292761 0.23179048 0.23065927 0.22953397 0.22841454 0.22730094]\n",
      "18\n",
      "[0.25446764 0.25321829 0.25197546 0.24786859 0.24950921 0.24828573\n",
      " 0.24706862 0.24585786 0.25924888 0.25805072 0.25393627 0.25275057\n",
      " 0.25157105 0.25039768 0.24923043 0.24806927 0.24691416 0.23409212\n",
      " 0.23294903 0.2318119  0.2306807  0.22955539 0.22843596 0.22732236]\n",
      "19\n",
      "[0.25448908 0.25323973 0.2519969  0.24788744 0.24953065 0.24830716\n",
      " 0.24709006 0.24587929 0.25927618 0.25807801 0.25396095 0.25277524\n",
      " 0.25159572 0.25042235 0.2492551  0.24809394 0.24693883 0.23411356\n",
      " 0.23297047 0.23183334 0.23070213 0.22957683 0.2284574  0.2273438 ]\n",
      "20\n",
      "[0.25451053 0.25326118 0.25201835 0.24790631 0.2495521  0.24832862\n",
      " 0.24711151 0.24590075 0.25930347 0.2581053  0.25398562 0.25279991\n",
      " 0.25162039 0.25044702 0.24927977 0.24811861 0.2469635  0.23413501\n",
      " 0.23299192 0.23185479 0.23072359 0.22959828 0.22847885 0.22736525]\n",
      "21\n",
      "[0.254532   0.25328265 0.25203982 0.24792519 0.24957357 0.24835008\n",
      " 0.24713298 0.24592222 0.25933076 0.25813259 0.25401028 0.25282458\n",
      " 0.25164506 0.25047169 0.24930444 0.24814327 0.24698817 0.23415648\n",
      " 0.23301339 0.23187626 0.23074506 0.22961975 0.22850032 0.22738672]\n",
      "22\n",
      "[0.25455348 0.25330414 0.25206131 0.24794409 0.24959506 0.24837157\n",
      " 0.24715446 0.2459437  0.25935804 0.25815988 0.25403494 0.25284924\n",
      " 0.25166972 0.25049635 0.2493291  0.24816794 0.24701283 0.23417797\n",
      " 0.23303487 0.23189774 0.23076654 0.22964124 0.2285218  0.2274082 ]\n",
      "23\n",
      "[0.25457498 0.25332564 0.25208281 0.247963   0.24961656 0.24839307\n",
      " 0.24717596 0.2459652  0.25938532 0.25818716 0.2540596  0.2528739\n",
      " 0.25169438 0.25052101 0.24935376 0.24819259 0.24703748 0.23419947\n",
      " 0.23305637 0.23191924 0.23078804 0.22966274 0.2285433  0.2274297 ]\n",
      "24\n",
      "[0.25459649 0.25334715 0.25210432 0.24798192 0.24963807 0.24841458\n",
      " 0.24719747 0.24598671 0.25941261 0.25821444 0.25408425 0.25289855\n",
      " 0.25171903 0.25054566 0.24937841 0.24821725 0.24706214 0.23422098\n",
      " 0.23307789 0.23194076 0.23080955 0.22968425 0.22856481 0.22745122]\n",
      "25\n",
      "[0.25461802 0.25336868 0.25212585 0.24800086 0.2496596  0.24843611\n",
      " 0.247219   0.24600824 0.25943988 0.25824172 0.2541089  0.2529232\n",
      " 0.25174368 0.25057031 0.24940306 0.2482419  0.24708679 0.23424251\n",
      " 0.23309942 0.23196228 0.23083108 0.22970578 0.22858634 0.22747275]\n",
      "26\n",
      "[0.25463957 0.25339022 0.2521474  0.24801981 0.24968114 0.24845766\n",
      " 0.24724055 0.24602979 0.25946716 0.25826899 0.25413355 0.25294785\n",
      " 0.25176833 0.25059496 0.24942771 0.24826654 0.24711143 0.23426406\n",
      " 0.23312096 0.23198383 0.23085263 0.22972732 0.22860789 0.22749429]\n",
      "27\n",
      "[0.25466113 0.25341179 0.25216896 0.24803878 0.24970271 0.24847922\n",
      " 0.24726211 0.24605135 0.25949443 0.25829626 0.25415819 0.25297249\n",
      " 0.25179297 0.2506196  0.24945235 0.24829118 0.24713608 0.23428562\n",
      " 0.23314252 0.23200539 0.23087419 0.22974888 0.22862945 0.22751585]\n",
      "28\n",
      "[0.25468271 0.25343336 0.25219053 0.24805776 0.24972428 0.24850079\n",
      " 0.24728368 0.24607292 0.2595217  0.25832353 0.25418283 0.25299712\n",
      " 0.2518176  0.25064424 0.24947698 0.24831582 0.24716071 0.23430719\n",
      " 0.2331641  0.23202697 0.23089576 0.22977046 0.22865103 0.22753743]\n",
      "29\n",
      "[0.2547043  0.25345495 0.25221212 0.24807675 0.24974587 0.24852238\n",
      " 0.24730528 0.24609451 0.25954896 0.2583508  0.25420746 0.25302176\n",
      " 0.25184224 0.25066887 0.24950162 0.24834045 0.24718535 0.23432878\n",
      " 0.23318569 0.23204856 0.23091735 0.22979205 0.22867262 0.22755902]\n",
      "30\n",
      "[0.2547259  0.25347656 0.25223373 0.24809576 0.24976748 0.24854399\n",
      " 0.24732688 0.24611612 0.25957623 0.25837806 0.25423209 0.25304639\n",
      " 0.25186687 0.2506935  0.24952625 0.24836508 0.24720997 0.23435039\n",
      " 0.2332073  0.23207016 0.23093896 0.22981366 0.22869422 0.22758062]\n",
      "31\n",
      "[0.25474752 0.25349818 0.25225535 0.24811478 0.2497891  0.24856561\n",
      " 0.2473485  0.24613774 0.25960349 0.25840532 0.25425671 0.25307101\n",
      " 0.25189149 0.25071812 0.24955087 0.24838971 0.2472346  0.23437201\n",
      " 0.23322892 0.23209178 0.23096058 0.22983528 0.22871584 0.22760225]\n",
      "32\n",
      "[0.25476916 0.25351982 0.25227699 0.24813382 0.24981074 0.24858725\n",
      " 0.24737014 0.24615938 0.25963074 0.25843258 0.25428133 0.25309563\n",
      " 0.25191611 0.25074274 0.24957549 0.24841433 0.24725922 0.23439365\n",
      " 0.23325055 0.23211342 0.23098222 0.22985692 0.22873748 0.22762388]\n",
      "33\n",
      "[0.25479081 0.25354147 0.25229864 0.24815287 0.24983239 0.2486089\n",
      " 0.24739179 0.24618103 0.259658   0.25845983 0.25430595 0.25312025\n",
      " 0.25194073 0.25076736 0.24960011 0.24843894 0.24728384 0.2344153\n",
      " 0.23327221 0.23213507 0.23100387 0.22987857 0.22875913 0.22764553]\n",
      "34\n",
      "[0.25481248 0.25356314 0.25232031 0.24817193 0.24985406 0.24863057\n",
      " 0.24741346 0.2462027  0.25968525 0.25848708 0.25433056 0.25314486\n",
      " 0.25196534 0.25079197 0.24962472 0.24846356 0.24730845 0.23443697\n",
      " 0.23329387 0.23215674 0.23102554 0.22990024 0.2287808  0.2276672 ]\n",
      "35\n",
      "[0.25483416 0.25358482 0.25234199 0.24819101 0.24987574 0.24865225\n",
      " 0.24743514 0.24622438 0.2597125  0.25851433 0.25435517 0.25316947\n",
      " 0.25198995 0.25081658 0.24964933 0.24848816 0.24733305 0.23445865\n",
      " 0.23331556 0.23217842 0.23104722 0.22992192 0.22880248 0.22768888]\n",
      "36\n",
      "[0.25485586 0.25360652 0.25236369 0.2482101  0.24989744 0.24867395\n",
      " 0.24745684 0.24624608 0.25973974 0.25854157 0.25437977 0.25319407\n",
      " 0.25201455 0.25084118 0.24967393 0.24851277 0.24735766 0.23448035\n",
      " 0.23333725 0.23220012 0.23106892 0.22994362 0.22882418 0.22771058]\n",
      "37\n",
      "[0.25487757 0.25362823 0.2523854  0.24822921 0.24991915 0.24869566\n",
      " 0.24747855 0.24626779 0.25976698 0.25856881 0.25440437 0.25321867\n",
      " 0.25203915 0.25086578 0.24969853 0.24853736 0.24738225 0.23450206\n",
      " 0.23335897 0.23222183 0.23109063 0.22996533 0.22884589 0.2277323 ]\n",
      "38\n",
      "[0.2548993  0.25364996 0.25240713 0.24824833 0.24994088 0.24871739\n",
      " 0.24750028 0.24628952 0.25979422 0.25859605 0.25442896 0.25324326\n",
      " 0.25206374 0.25089037 0.24972312 0.24856196 0.24740685 0.23452379\n",
      " 0.23338069 0.23224356 0.23111236 0.22998706 0.22886762 0.22775402]\n",
      "39\n",
      "[0.25492105 0.2536717  0.25242887 0.24826746 0.24996262 0.24873913\n",
      " 0.24752203 0.24631126 0.25982145 0.25862328 0.25445355 0.25326785\n",
      " 0.25208833 0.25091496 0.24974771 0.24858654 0.24743144 0.23454553\n",
      " 0.23340244 0.23226531 0.2311341  0.2300088  0.22888937 0.22777577]\n",
      "40\n",
      "[0.2549428  0.25369346 0.25245063 0.24828661 0.24998438 0.24876089\n",
      " 0.24754378 0.24633302 0.25984868 0.25865051 0.25447813 0.25329243\n",
      " 0.25211291 0.25093954 0.24977229 0.24861113 0.24745602 0.23456729\n",
      " 0.2334242  0.23228707 0.23115586 0.23003056 0.22891112 0.22779753]\n",
      "41\n",
      "[0.25496458 0.25371523 0.2524724  0.24830577 0.25000615 0.24878267\n",
      " 0.24756556 0.2463548  0.25987591 0.25867774 0.25450271 0.25331701\n",
      " 0.25213749 0.25096412 0.24979687 0.24863571 0.2474806  0.23458906\n",
      " 0.23344597 0.23230884 0.23117764 0.23005233 0.2289329  0.2278193 ]\n",
      "42\n",
      "[0.25498637 0.25373702 0.25249419 0.24832495 0.25002794 0.24880445\n",
      " 0.24758735 0.24637659 0.25990313 0.25870496 0.25452729 0.25334158\n",
      " 0.25216206 0.25098869 0.24982144 0.24866028 0.24750517 0.23461085\n",
      " 0.23346776 0.23233063 0.23119943 0.23007412 0.22895469 0.22784109]\n",
      "43\n",
      "[0.25500817 0.25375883 0.252516   0.24834414 0.25004975 0.24882626\n",
      " 0.24760915 0.24639839 0.25993035 0.25873218 0.25455186 0.25336615\n",
      " 0.25218663 0.25101326 0.24984601 0.24868485 0.24752974 0.23463266\n",
      " 0.23348956 0.23235243 0.23122123 0.23009593 0.22897649 0.22786289]\n",
      "44\n",
      "[0.25502999 0.25378065 0.25253782 0.24836334 0.25007157 0.24884808\n",
      " 0.24763097 0.24642021 0.25995756 0.2587594  0.25457642 0.25339072\n",
      " 0.2522112  0.25103783 0.24987058 0.24870941 0.2475543  0.23465448\n",
      " 0.23351138 0.23237425 0.23124305 0.23011775 0.22899831 0.22788471]\n",
      "45\n",
      "[0.25505183 0.25380248 0.25255965 0.24838256 0.2500934  0.24886991\n",
      " 0.2476528  0.24644204 0.25998477 0.25878661 0.25460098 0.25341527\n",
      " 0.25223575 0.25106238 0.24989513 0.24873397 0.24757886 0.23467631\n",
      " 0.23353322 0.23239609 0.23126488 0.23013958 0.22902015 0.22790655]\n",
      "46\n",
      "[0.25507368 0.25382433 0.2525815  0.24840179 0.25011525 0.24889176\n",
      " 0.24767465 0.24646389 0.26001198 0.25881381 0.25462553 0.25343983\n",
      " 0.25226031 0.25108694 0.24991969 0.24875852 0.24760342 0.23469816\n",
      " 0.23355507 0.23241794 0.23128673 0.23016143 0.22904199 0.2279284 ]\n",
      "47\n",
      "[0.25509554 0.2538462  0.25260337 0.24842104 0.25013712 0.24891363\n",
      " 0.24769652 0.24648576 0.26003918 0.25884102 0.25465008 0.25346438\n",
      " 0.25228486 0.25111149 0.24994424 0.24878307 0.24762796 0.23472003\n",
      " 0.23357693 0.2324398  0.2313086  0.23018329 0.22906386 0.22795026]\n",
      "48\n",
      "[0.25511742 0.25386808 0.25262525 0.2484403  0.250159   0.24893551\n",
      " 0.2477184  0.24650764 0.26006638 0.25886821 0.25467462 0.25348892\n",
      " 0.2523094  0.25113603 0.24996878 0.24880762 0.24765251 0.23474191\n",
      " 0.23359881 0.23246168 0.23133048 0.23020517 0.22908574 0.22797214]\n",
      "49\n",
      "[0.25513932 0.25388997 0.25264714 0.24845957 0.25018089 0.2489574\n",
      " 0.24774029 0.24652953 0.26009357 0.25889541 0.25469916 0.25351346\n",
      " 0.25233394 0.25116057 0.24999332 0.24883215 0.24767704 0.2347638\n",
      " 0.23362071 0.23248358 0.23135237 0.23022707 0.22910763 0.22799404]\n",
      "50\n",
      "[0.25516123 0.25391188 0.25266905 0.24847886 0.2502028  0.24897931\n",
      " 0.2477622  0.24655144 0.26012077 0.2589226  0.25472369 0.25353799\n",
      " 0.25235847 0.2511851  0.25001785 0.24885669 0.24770158 0.23478571\n",
      " 0.23364262 0.23250549 0.23137428 0.23024898 0.22912954 0.22801595]\n",
      "51\n",
      "[0.25518315 0.25393381 0.25269098 0.24849816 0.25022473 0.24900124\n",
      " 0.24778413 0.24657337 0.26014795 0.25894978 0.25474822 0.25356252\n",
      " 0.252383   0.25120963 0.25004238 0.24888121 0.2477261  0.23480764\n",
      " 0.23366454 0.23252741 0.23139621 0.23027091 0.22915147 0.22803787]\n",
      "52\n",
      "[0.25520509 0.25395575 0.25271292 0.24851748 0.25024667 0.24902318\n",
      " 0.24780607 0.24659531 0.26017513 0.25897697 0.25477274 0.25358704\n",
      " 0.25240752 0.25123415 0.2500669  0.24890573 0.24775062 0.23482958\n",
      " 0.23368648 0.23254935 0.23141815 0.23029285 0.22917341 0.22805981]\n",
      "53\n",
      "[0.25522705 0.2539777  0.25273487 0.24853681 0.25026862 0.24904513\n",
      " 0.24782803 0.24661726 0.26020231 0.25900414 0.25479726 0.25361155\n",
      " 0.25243203 0.25125866 0.25009141 0.24893025 0.24777514 0.23485153\n",
      " 0.23370844 0.23257131 0.2314401  0.2303148  0.22919537 0.22808177]\n",
      "54\n",
      "[0.25524902 0.25399967 0.25275684 0.24855615 0.25029059 0.2490671\n",
      " 0.24785    0.24663923 0.26022948 0.25903132 0.25482177 0.25363606\n",
      " 0.25245654 0.25128317 0.25011592 0.24895476 0.24779965 0.2348735\n",
      " 0.23373041 0.23259328 0.23146207 0.23033677 0.22921734 0.22810374]\n",
      "55\n",
      "[0.255271   0.25402166 0.25277883 0.24857551 0.25031258 0.24908909\n",
      " 0.24787198 0.24666122 0.26025665 0.25905848 0.25484627 0.25366057\n",
      " 0.25248105 0.25130768 0.25014043 0.24897926 0.24782416 0.23489549\n",
      " 0.23375239 0.23261526 0.23148406 0.23035876 0.22923932 0.22812572]\n",
      "56\n",
      "[0.255293   0.25404366 0.25280083 0.24859488 0.25033458 0.24911109\n",
      " 0.24789398 0.24668322 0.26028381 0.25908565 0.25487077 0.25368507\n",
      " 0.25250555 0.25133218 0.25016493 0.24900376 0.24784865 0.23491749\n",
      " 0.2337744  0.23263726 0.23150606 0.23038076 0.22926132 0.22814772]\n",
      "57\n",
      "[0.25531502 0.25406567 0.25282284 0.24861427 0.25035659 0.24913311\n",
      " 0.247916   0.24670524 0.26031097 0.25911281 0.25489526 0.25370956\n",
      " 0.25253004 0.25135667 0.25018942 0.24902826 0.24787315 0.2349395\n",
      " 0.23379641 0.23265928 0.23152808 0.23040277 0.22928334 0.22816974]\n",
      "58\n",
      "[0.25533705 0.2540877  0.25284487 0.24863367 0.25037862 0.24915514\n",
      " 0.24793803 0.24672727 0.26033813 0.25913996 0.25491975 0.25373405\n",
      " 0.25255453 0.25138116 0.25021391 0.24905274 0.24789763 0.23496153\n",
      " 0.23381844 0.23268131 0.23155011 0.2304248  0.22930537 0.22819177]\n",
      "59\n",
      "[0.25535909 0.25410975 0.25286692 0.24865308 0.25040067 0.24917718\n",
      " 0.24796007 0.24674931 0.26036528 0.25916711 0.25494423 0.25375853\n",
      " 0.25257901 0.25140564 0.25023839 0.24907722 0.24792212 0.23498358\n",
      " 0.23384049 0.23270335 0.23157215 0.23044685 0.22932741 0.22821382]\n",
      "60\n",
      "[0.25538115 0.25413181 0.25288898 0.2486725  0.25042273 0.24919924\n",
      " 0.24798213 0.24677137 0.26039242 0.25919425 0.25496871 0.253783\n",
      " 0.25260348 0.25143011 0.25026286 0.2491017  0.24794659 0.23500564\n",
      " 0.23386255 0.23272542 0.23159421 0.23046891 0.22934947 0.22823588]\n",
      "61\n",
      "[0.25540323 0.25415389 0.25291106 0.24869195 0.25044481 0.24922132\n",
      " 0.24800421 0.24679345 0.26041956 0.25922139 0.25499317 0.25380747\n",
      " 0.25262795 0.25145458 0.25028733 0.24912617 0.24797106 0.23502772\n",
      " 0.23388462 0.23274749 0.23161629 0.23049099 0.22937155 0.22825795]\n",
      "62\n",
      "[0.25542532 0.25417598 0.25293315 0.2487114  0.2504669  0.24924341\n",
      " 0.2480263  0.24681554 0.26044669 0.25924853 0.25501764 0.25383193\n",
      " 0.25265241 0.25147905 0.2503118  0.24915063 0.24799552 0.23504981\n",
      " 0.23390671 0.23276958 0.23163838 0.23051308 0.22939364 0.22828004]\n",
      "63\n",
      "[0.25544743 0.25419808 0.25295525 0.24873087 0.250489   0.24926551\n",
      " 0.24804841 0.24683764 0.26047382 0.25927566 0.2550421  0.25385639\n",
      " 0.25267687 0.2515035  0.25033625 0.24917509 0.24801998 0.23507191\n",
      " 0.23392882 0.23279169 0.23166048 0.23053518 0.22941575 0.22830215]\n",
      "64\n",
      "[0.25546955 0.2542202  0.25297737 0.24875035 0.25051112 0.24928763\n",
      " 0.24807053 0.24685976 0.26050095 0.25930278 0.25506655 0.25388084\n",
      " 0.25270132 0.25152795 0.2503607  0.24919954 0.24804443 0.23509403\n",
      " 0.23395094 0.23281381 0.2316826  0.2305573  0.22943787 0.22832427]\n",
      "65\n",
      "[0.25549168 0.25424234 0.25299951 0.24876985 0.25053326 0.24930977\n",
      " 0.24809266 0.2468819  0.26052807 0.2593299  0.25509099 0.25390529\n",
      " 0.25272577 0.2515524  0.25038515 0.24922398 0.24806888 0.23511617\n",
      " 0.23397307 0.23283594 0.23170474 0.23057944 0.22946    0.2283464 ]\n",
      "66\n",
      "[0.25551383 0.25426449 0.25302166 0.24878936 0.25055541 0.24933192\n",
      " 0.24811481 0.24690405 0.26055518 0.25935701 0.25511543 0.25392973\n",
      " 0.25275021 0.25157684 0.25040959 0.24924842 0.24809331 0.23513832\n",
      " 0.23399522 0.23285809 0.23172689 0.23060159 0.22948215 0.22836855]\n",
      "67\n",
      "[0.255536   0.25428665 0.25304382 0.24880888 0.25057757 0.24935408\n",
      " 0.24813698 0.24692622 0.26058229 0.25938412 0.25513986 0.25395416\n",
      " 0.25277464 0.25160127 0.25043402 0.24927285 0.24811775 0.23516048\n",
      " 0.23401739 0.23288026 0.23174906 0.23062375 0.22950432 0.22839072]\n",
      "68\n",
      "[0.25555818 0.25430883 0.253066   0.24882842 0.25059975 0.24937626\n",
      " 0.24815916 0.2469484  0.26060939 0.25941123 0.25516429 0.25397858\n",
      " 0.25279906 0.25162569 0.25045844 0.24929728 0.24814217 0.23518266\n",
      " 0.23403957 0.23290244 0.23177124 0.23064593 0.2295265  0.2284129 ]\n",
      "69\n",
      "[0.25558037 0.25433103 0.2530882  0.24884797 0.25062195 0.24939846\n",
      " 0.24818135 0.24697059 0.26063649 0.25943832 0.25518871 0.254003\n",
      " 0.25282348 0.25165011 0.25048286 0.2493217  0.24816659 0.23520486\n",
      " 0.23406177 0.23292463 0.23179343 0.23066813 0.22954869 0.22843509]\n",
      "70\n",
      "[0.25560258 0.25435324 0.25311041 0.24886753 0.25064416 0.24942067\n",
      " 0.24820356 0.2469928  0.26066358 0.25946542 0.25521312 0.25402741\n",
      " 0.25284789 0.25167453 0.25050728 0.24934611 0.248191   0.23522707\n",
      " 0.23408397 0.23294684 0.23181564 0.23069034 0.2295709  0.2284573 ]\n",
      "71\n",
      "[0.25562481 0.25437546 0.25313263 0.24888711 0.25066638 0.24944289\n",
      " 0.24822579 0.24701503 0.26069067 0.2594925  0.25523752 0.25405182\n",
      " 0.2528723  0.25169893 0.25053168 0.24937052 0.24821541 0.23524929\n",
      " 0.2341062  0.23296907 0.23183787 0.23071256 0.22959313 0.22847953]\n",
      "72\n",
      "[0.25564705 0.2543977  0.25315487 0.2489067  0.25068862 0.24946513\n",
      " 0.24824803 0.24703726 0.26071775 0.25951959 0.25526192 0.25407622\n",
      " 0.2528967  0.25172333 0.25055608 0.24939492 0.24823981 0.23527153\n",
      " 0.23412844 0.23299131 0.2318601  0.2307348  0.22961537 0.22850177]\n",
      "73\n",
      "[0.2556693  0.25441996 0.25317713 0.24892631 0.25071088 0.24948739\n",
      " 0.24827028 0.24705952 0.26074483 0.25954666 0.25528632 0.25410061\n",
      " 0.25292109 0.25174772 0.25058047 0.24941931 0.2482642  0.23529379\n",
      " 0.23415069 0.23301356 0.23188236 0.23075706 0.22963762 0.22852402]\n",
      "74\n",
      "[0.25569157 0.25444223 0.2531994  0.24894593 0.25073315 0.24950966\n",
      " 0.24829255 0.24708179 0.2607719  0.25957373 0.2553107  0.254125\n",
      " 0.25294548 0.25177211 0.25060486 0.2494437  0.24828859 0.23531606\n",
      " 0.23417296 0.23303583 0.23190463 0.23077933 0.22965989 0.22854629]\n",
      "75\n",
      "[0.25571385 0.25446451 0.25322168 0.24896556 0.25075543 0.24953194\n",
      " 0.24831483 0.24710407 0.26079896 0.2596008  0.25533508 0.25414938\n",
      " 0.25296986 0.25179649 0.25062924 0.24946807 0.24831297 0.23533834\n",
      " 0.23419525 0.23305811 0.23192691 0.23080161 0.22968217 0.22856858]\n",
      "76\n",
      "[0.25573615 0.25448681 0.25324398 0.24898521 0.25077773 0.24955424\n",
      " 0.24833713 0.24712637 0.26082602 0.25962785 0.25535945 0.25417375\n",
      " 0.25299423 0.25182086 0.25065361 0.24949245 0.24833734 0.23536064\n",
      " 0.23421755 0.23308041 0.23194921 0.23082391 0.22970447 0.22859087]\n",
      "77\n",
      "[0.25575847 0.25450912 0.25326629 0.24900487 0.25080004 0.24957655\n",
      " 0.24835945 0.24714868 0.26085307 0.25965491 0.25538382 0.25419811\n",
      " 0.25301859 0.25184523 0.25067797 0.24951681 0.2483617  0.23538295\n",
      " 0.23423986 0.23310273 0.23197152 0.23084622 0.22972679 0.22861319]\n",
      "78\n",
      "[0.2557808  0.25453145 0.25328862 0.24902455 0.25082237 0.24959888\n",
      " 0.24838177 0.24717101 0.26088012 0.25968195 0.25540818 0.25422247\n",
      " 0.25304295 0.25186958 0.25070233 0.24954117 0.24838606 0.23540528\n",
      " 0.23426219 0.23312506 0.23199385 0.23086855 0.22974911 0.22863552]\n",
      "79\n",
      "[0.25580314 0.25455379 0.25331096 0.24904424 0.25084471 0.24962123\n",
      " 0.24840412 0.24719336 0.26090716 0.25970899 0.25543253 0.25424682\n",
      " 0.2530673  0.25189393 0.25072668 0.24956552 0.24841041 0.23542762\n",
      " 0.23428453 0.2331474  0.2320162  0.23089089 0.22977146 0.22865786]\n",
      "80\n",
      "[0.2558255  0.25457615 0.25333332 0.24906394 0.25086707 0.24964358\n",
      " 0.24842648 0.24721571 0.26093419 0.25973603 0.25545687 0.25427117\n",
      " 0.25309165 0.25191828 0.25075103 0.24958986 0.24843476 0.23544998\n",
      " 0.23430689 0.23316976 0.23203855 0.23091325 0.22979382 0.22868022]\n",
      "81\n",
      "[0.25584787 0.25459852 0.25335569 0.24908365 0.25088944 0.24966596\n",
      " 0.24844885 0.24723809 0.26096122 0.25976306 0.25548121 0.2542955\n",
      " 0.25311598 0.25194262 0.25077537 0.2496142  0.24845909 0.23547235\n",
      " 0.23432926 0.23319213 0.23206093 0.23093562 0.22981619 0.22870259]\n",
      "82\n",
      "[0.25587026 0.25462091 0.25337808 0.24910338 0.25091183 0.24968834\n",
      " 0.24847124 0.24726047 0.26098824 0.25979008 0.25550554 0.25431983\n",
      " 0.25314031 0.25196695 0.25079969 0.24963853 0.24848342 0.23549474\n",
      " 0.23435165 0.23321452 0.23208331 0.23095801 0.22983858 0.22872498]\n",
      "83\n",
      "[0.25589266 0.25464331 0.25340048 0.24912313 0.25093423 0.24971075\n",
      " 0.24849364 0.24728288 0.26101526 0.25981709 0.25552986 0.25434416\n",
      " 0.25316464 0.25199127 0.25082402 0.24966285 0.24850774 0.23551714\n",
      " 0.23437405 0.23323692 0.23210572 0.23098041 0.22986098 0.22874738]\n",
      "84\n",
      "[0.25591508 0.25466573 0.2534229  0.24914288 0.25095665 0.24973316\n",
      " 0.24851605 0.24730529 0.26104227 0.2598441  0.25555417 0.25436847\n",
      " 0.25318895 0.25201558 0.25084833 0.24968717 0.24853206 0.23553956\n",
      " 0.23439647 0.23325934 0.23212813 0.23100283 0.22988339 0.2287698 ]\n",
      "85\n",
      "[0.25593751 0.25468816 0.25344533 0.24916265 0.25097908 0.24975559\n",
      " 0.24853849 0.24732772 0.26106927 0.25987111 0.25557848 0.25439278\n",
      " 0.25321326 0.25203989 0.25087264 0.24971148 0.24855637 0.23556199\n",
      " 0.2344189  0.23328177 0.23215056 0.23102526 0.22990583 0.22879223]\n",
      "86\n",
      "[0.25595995 0.25471061 0.25346778 0.24918244 0.25100153 0.24977804\n",
      " 0.24856093 0.24735017 0.26109627 0.2598981  0.25560278 0.25441708\n",
      " 0.25323756 0.25206419 0.25089694 0.24973578 0.24858067 0.23558444\n",
      " 0.23444135 0.23330421 0.23217301 0.23104771 0.22992827 0.22881467]\n",
      "87\n",
      "[0.25598241 0.25473307 0.25349024 0.24920224 0.25102399 0.2498005\n",
      " 0.24858339 0.24737263 0.26112326 0.25992509 0.25562708 0.25444137\n",
      " 0.25326185 0.25208848 0.25092123 0.24976007 0.24860496 0.2356069\n",
      " 0.23446381 0.23332667 0.23219547 0.23107017 0.22995073 0.22883714]\n",
      "88\n",
      "[0.25600489 0.25475554 0.25351271 0.24922205 0.25104646 0.24982298\n",
      " 0.24860587 0.24739511 0.26115024 0.25995207 0.25565136 0.25446566\n",
      " 0.25328614 0.25211277 0.25094552 0.24978435 0.24862925 0.23562937\n",
      " 0.23448628 0.23334915 0.23221795 0.23109264 0.22997321 0.22885961]\n",
      "89\n",
      "[0.25602738 0.25477803 0.2535352  0.24924187 0.25106895 0.24984547\n",
      " 0.24862836 0.2474176  0.26117722 0.25997905 0.25567564 0.25448993\n",
      " 0.25331041 0.25213705 0.25096979 0.24980863 0.24865352 0.23565186\n",
      " 0.23450877 0.23337164 0.23224044 0.23111513 0.2299957  0.2288821 ]\n",
      "90\n",
      "[0.25604988 0.25480054 0.25355771 0.24926171 0.25109146 0.24986797\n",
      " 0.24865086 0.2474401  0.26120419 0.26000602 0.25569991 0.2545142\n",
      " 0.25333468 0.25216131 0.25099406 0.2498329  0.24867779 0.23567437\n",
      " 0.23453128 0.23339414 0.23226294 0.23113764 0.2300182  0.22890461]\n",
      "91\n",
      "[0.2560724  0.25482306 0.25358023 0.24928156 0.25111398 0.24989049\n",
      " 0.24867338 0.24746262 0.26123115 0.26003298 0.25572417 0.25453847\n",
      " 0.25335895 0.25218558 0.25101833 0.24985716 0.24870205 0.23569689\n",
      " 0.2345538  0.23341666 0.23228546 0.23116016 0.23004072 0.22892712]\n",
      "92\n",
      "[0.25609494 0.25484559 0.25360276 0.24930143 0.25113651 0.24991302\n",
      " 0.24869592 0.24748515 0.26125811 0.26005994 0.25574842 0.25456272\n",
      " 0.2533832  0.25220983 0.25104258 0.24988142 0.24872631 0.23571942\n",
      " 0.23457633 0.2334392  0.232308   0.23118269 0.23006326 0.22894966]\n",
      "93\n",
      "[0.25611749 0.25486814 0.25362531 0.24932131 0.25115906 0.24993557\n",
      " 0.24871846 0.2475077  0.26128506 0.26008689 0.25577267 0.25458697\n",
      " 0.25340745 0.25223408 0.25106683 0.24990566 0.24875055 0.23574197\n",
      " 0.23459888 0.23346175 0.23233054 0.23120524 0.2300858  0.22897221]\n",
      "94\n",
      "[0.25614005 0.2548907  0.25364787 0.2493412  0.25118162 0.24995814\n",
      " 0.24874103 0.24753027 0.261312   0.26011383 0.25579691 0.2546112\n",
      " 0.25343168 0.25225832 0.25109107 0.2499299  0.24877479 0.23576453\n",
      " 0.23462144 0.23348431 0.23235311 0.2312278  0.23010837 0.22899477]\n",
      "95\n",
      "[0.25616263 0.25491328 0.25367045 0.24936111 0.2512042  0.24998071\n",
      " 0.24876361 0.24755284 0.26133893 0.26014077 0.25582114 0.25463544\n",
      " 0.25345592 0.25228255 0.2511153  0.24995413 0.24879902 0.23578711\n",
      " 0.23464402 0.23350689 0.23237568 0.23125038 0.23013095 0.22901735]\n",
      "96\n",
      "[0.25618522 0.25493587 0.25369304 0.24938103 0.25122679 0.25000331\n",
      " 0.2487862  0.24757544 0.26136586 0.2601677  0.25584536 0.25465966\n",
      " 0.25348014 0.25230677 0.25113952 0.24997835 0.24882325 0.2358097\n",
      " 0.23466661 0.23352948 0.23239828 0.23127297 0.23015354 0.22903994]\n",
      "97\n",
      "[0.25620782 0.25495848 0.25371565 0.24940096 0.2512494  0.25002591\n",
      " 0.2488088  0.24759804 0.26139278 0.26019462 0.25586958 0.25468387\n",
      " 0.25350435 0.25233098 0.25116373 0.25000257 0.24884746 0.23583231\n",
      " 0.23468922 0.23355209 0.23242088 0.23129558 0.23017614 0.22906255]\n",
      "98\n",
      "[0.25623045 0.2549811  0.25373827 0.24942091 0.25127202 0.25004853\n",
      " 0.24883143 0.24762066 0.2614197  0.26022153 0.25589378 0.25470808\n",
      " 0.25352856 0.25235519 0.25118794 0.25002677 0.24887167 0.23585493\n",
      " 0.23471184 0.23357471 0.2324435  0.2313182  0.23019877 0.22908517]\n",
      "99\n",
      "[0.25625308 0.25500374 0.25376091 0.24944087 0.25129466 0.25007117\n",
      " 0.24885406 0.2476433  0.2614466  0.26024844 0.25591798 0.25473228\n",
      " 0.25355276 0.25237939 0.25121214 0.25005097 0.24889586 0.23587757\n",
      " 0.23473447 0.23359734 0.23246614 0.23134084 0.2302214  0.2291078 ]\n",
      "[0.25625308 0.25500374 0.25376091 0.24944087 0.25129466 0.25007117\n",
      " 0.24885406 0.2476433  0.2614466  0.26024844 0.25591798 0.25473228\n",
      " 0.25355276 0.25237939 0.25121214 0.25005097 0.24889586 0.23587757\n",
      " 0.23473447 0.23359734 0.23246614 0.23134084 0.2302214  0.2291078 ]\n"
     ]
    }
   ],
   "source": [
    "#statistical\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "#orig_train = orig_train.dropna()\n",
    "#orig_test = orig_test.dropna()\n",
    "#endog = orig_train[orig_train.patient_id==544].glucose\n",
    "#exog = orig_train[orig_train.patient_id==544].drop(['glucose', 'time', 'patient_id'], axis=1)\n",
    "#exog_pred = orig_test[orig_test.patient_id==544].drop(['glucose', 'time', 'patient_id'], axis=1)\n",
    "#endog_pred = orig_test[orig_test.patient_id==544].glucose\n",
    "#print(orig_test, endog_pred)\n",
    "#print(np.asarray(endog), np.asarray(exog))\n",
    "print(y[0],X[0])\n",
    "mod = sm.tsa.SARIMAX(endog=np.asarray(y[0]), exog=np.asarray(X[0]), order=(1,0,0))\n",
    "#res = mod.fit(disp=False)\n",
    "mod = mod.fit(disp=False)\n",
    "start_params = mod.params\n",
    "print(mod.summary())\n",
    "pred = mod.forecast(horizon, start_params=start_params, exog=np.asarray(dataset.X_test_exog[0])[-horizon:])\n",
    "print(pred)\n",
    "max_iter = 100\n",
    "it = 0\n",
    "learning_rate = 0.0001\n",
    "gradient=lambda v: 4 * v**3 - 10 * v - 3\n",
    "exog_pred_change = np.asarray(dataset.X_test_exog[0])[-horizon:]\n",
    "print(\"max_min\", max_bound, min_bound)\n",
    "while ((pred>max_bound).any() or (pred<min_bound).any()) and (it<max_iter):\n",
    "    #change (X_e)\n",
    "    print(it)\n",
    "    diff = -learning_rate * gradient(exog_pred_change)\n",
    "    exog_pred_change += diff\n",
    "    it += 1\n",
    "    pred = mod.forecast(horizon, start_params=start_params, exog=exog_pred_change)\n",
    "    print(pred)\n",
    "    \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bef633-abdf-41a0-a202-25e97b09522a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#regression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def mean_squared_error(y_true, y_predicted):\n",
    "    # Calculating the loss or cost\n",
    "    cost = np.sum((y_true-y_predicted)**2) / len(y_true)\n",
    "    return cost\n",
    "    \n",
    "orig_train = orig_train.dropna()\n",
    "orig_test = orig_test.dropna()\n",
    "endog = orig_train[orig_train.patient_id==591].glucose\n",
    "exog = orig_train[orig_train.patient_id==591].drop(['glucose', 'time', 'patient_id'], axis=1)\n",
    "exog_pred = orig_test[orig_test.patient_id==591].drop(['glucose', 'time', 'patient_id'], axis=1)\n",
    "endog_pred = orig_test[orig_test.patient_id==591].glucose\n",
    "print(orig_test, endog_pred)\n",
    "print(np.asarray(endog), np.asarray(exog))\n",
    "forest = sm.OLS(np.asarray(endog), np.asarray(exog)).fit()\n",
    "pred = forest.predict(np.asarray(exog_pred))\n",
    "print(pred)\n",
    "max_iter = 100\n",
    "it = 0\n",
    "learning_rate = 0.0001\n",
    "gradient=lambda v: 4 * v**3 - 10 * v - 3\n",
    "exog_pred_change = np.asarray(exog_pred)\n",
    "print(\"max_min\", max_bound, min_bound)\n",
    "while ((pred>max_bound).any() or (pred<min_bound).any()) and (it<max_iter):\n",
    "    #change (X_e)\n",
    "    print(it)\n",
    "    diff = -learning_rate * gradient(exog_pred_change)\n",
    "    exog_pred_change += diff\n",
    "    it += 1\n",
    "    pred = forest.predict(exog_pred_change)\n",
    "    print(pred)\n",
    "    \n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
